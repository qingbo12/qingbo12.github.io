[{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/go/basic/_index.bn/","summary":"","tags":null,"title":"Go বেসিক"},{"categories":null,"contents":" Hello World A sample go program is show here.\npackage main import \u0026#34;fmt\u0026#34; func main() { message := greetMe(\u0026#34;world\u0026#34;) fmt.Println(message) } func greetMe(name string) string { return \u0026#34;Hello, \u0026#34; + name + \u0026#34;!\u0026#34; } Run the program as below:\n$ go run hello.go Variables Normal Declaration:\nvar msg string msg = \u0026#34;Hello\u0026#34; Shortcut:\nmsg := \u0026#34;Hello\u0026#34; Constants const Phi = 1.618 ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/go/basic/introduction/","summary":"\u003c!-- A Sample Program --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eHello World\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eA sample go program is show here.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003epackage\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;fmt\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003emessage\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egreetMe\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;world\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003emessage\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egreetMe\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e) \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello, \u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ename\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;!\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eRun the program as below:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$ go run hello.go\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Declaring Variables --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eVariables\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003e\u003cstrong\u003eNormal Declaration:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emsg\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003emsg\u003c/span\u003e = \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003e\u003cstrong\u003eShortcut:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003emsg\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Declaring Constants --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eConstants\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003econst\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ePhi\u003c/span\u003e = \u003cspan style=\"color:#ae81ff\"\u003e1.618\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e","tags":null,"title":"Introduction"},{"categories":null,"contents":" Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.Println(\u0026#34;Value is\u0026#34;, b) func getPointer () (myPointer *int) { a := 234 return \u0026amp;a a := new(int) *a = 234 Pointers point to a memory location of a variable. Go is fully garbage-collected.\nType Conversion i := 2 f := float64(i) u := uint(i) Slice slice := []int{2, 3, 4} slice := []byte(\u0026#34;Hello\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/go/basic/types/","summary":"\u003c!-- String Type --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eStrings\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003estr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eMultiline string\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003estr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e`Multiline\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#e6db74\"\u003estring`\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Number Types --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eNumbers\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cp\u003eTypical types\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enum\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e          \u003cspan style=\"color:#75715e\"\u003e// int\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enum\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3.\u003c/span\u003e         \u003cspan style=\"color:#75715e\"\u003e// float64\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enum\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e3\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e+\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e4i\u003c/span\u003e     \u003cspan style=\"color:#75715e\"\u003e// complex128\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enum\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e byte(\u003cspan style=\"color:#e6db74\"\u003e\u0026#39;a\u0026#39;\u003c/span\u003e)  \u003cspan style=\"color:#75715e\"\u003e// byte (alias for uint8)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eOther Types\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eu\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003euint\u003c/span\u003e = \u003cspan style=\"color:#ae81ff\"\u003e7\u003c/span\u003e        \u003cspan style=\"color:#75715e\"\u003e// uint (unsigned)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003evar\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ep\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003efloat32\u003c/span\u003e = \u003cspan style=\"color:#ae81ff\"\u003e22.7\u003c/span\u003e  \u003cspan style=\"color:#75715e\"\u003e// 32-bit float\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!----------- Arrays  ------\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eArrays\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// var numbers [5]int\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003enumbers\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e [\u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e]\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e{\u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e, \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Pointers --\u003e\n\u003cdiv class=\"note-card medium-note\"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003ePointers\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e () {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003eb\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003egetPointer\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Value is\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eb\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003egetPointer\u003c/span\u003e () (\u003cspan style=\"color:#a6e22e\"\u003emyPointer\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e234\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e new(\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003ea\u003c/span\u003e = \u003cspan style=\"color:#ae81ff\"\u003e234\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003ePointers point to a memory location of a variable. Go is fully garbage-collected.\u003c/p\u003e","tags":null,"title":"Basic Types"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/go/advanced/_index.bn/","summary":"","tags":null,"title":"অ্যাডভান্সড"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) Switch switch day { case \u0026#34;sunday\u0026#34;: // cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default! fallthrough case \u0026#34;saturday\u0026#34;: rest() default: work() } Loop for count := 0; count \u0026lt;= 10; count++ { fmt.Println(\u0026#34;My counter is at\u0026#34;, count) } entry := []string{\u0026#34;Jack\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Jones\u0026#34;} for i, val := range entry { fmt.Printf(\u0026#34;At position %d, the character %s is present\\n\u0026#34;, i, val) n := 0 x := 42 for n != x { n := guess() } ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/go/basic/flow-control/","summary":"\u003c!-- Condition --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eCondition\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sunday\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e||\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;saturday\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003erest\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;monday\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eisTired\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003egroan\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003ework\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edoThing\u003c/span\u003e(); \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Uh oh\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Switch --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eSwitch\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eswitch\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ecase\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sunday\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#75715e\"\u003e// cases don\u0026#39;t \u0026#34;fall through\u0026#34; by default!\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003efallthrough\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003ecase\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;saturday\u0026#34;\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003erest\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#66d9ef\"\u003edefault\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#a6e22e\"\u003ework\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Loop --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eLoop\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ecount\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e; \u003cspan style=\"color:#a6e22e\"\u003ecount\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026lt;=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e10\u003c/span\u003e; \u003cspan style=\"color:#a6e22e\"\u003ecount\u003c/span\u003e\u003cspan style=\"color:#f92672\"\u003e++\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;My counter is at\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003ecount\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003eentry\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e []\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e{\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Jack\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;John\u0026#34;\u003c/span\u003e,\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Jones\u0026#34;\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ei\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eval\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003erange\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eentry\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintf\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;At position %d, the character %s is present\\n\u0026#34;\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003ei\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eval\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#a6e22e\"\u003ex\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e42\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efor\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003ex\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003en\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eguess\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e","tags":null,"title":"Flow Control"},{"categories":null,"contents":" Condition if day == \u0026#34;sunday\u0026#34; || day == \u0026#34;saturday\u0026#34; { rest() } else if day == \u0026#34;monday\u0026#34; \u0026amp;\u0026amp; isTired() { groan() } else { work() } if _, err := doThing(); err != nil { fmt.Println(\u0026#34;Uh oh\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/go/advanced/files/","summary":"\u003c!-- Condition --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eCondition\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;sunday\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e||\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;saturday\u0026#34;\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003erest\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eday\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e==\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;monday\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eisTired\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003egroan\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e} \u003cspan style=\"color:#66d9ef\"\u003eelse\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003ework\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003e_\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003edoThing\u003c/span\u003e(); \u003cspan style=\"color:#a6e22e\"\u003eerr\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e!=\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enil\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Uh oh\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e","tags":null,"title":"File Manipulation"},{"categories":null,"contents":" Variable NAME=\u0026#34;John\u0026#34; echo $NAME echo \u0026#34;$NAME\u0026#34; echo \u0026#34;${NAME} Condition if [[ -z \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is empty\u0026#34; elif [[ -n \u0026#34;$string\u0026#34; ]]; then echo \u0026#34;String is not empty\u0026#34; fi ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/bash/basic/","summary":"\u003c!-- Variable --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eVariable\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eNAME\u003cspan style=\"color:#f92672\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;John\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho $NAME\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$NAME\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eecho \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e${\u003c/span\u003eNAME\u003cspan style=\"color:#e6db74\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#e6db74\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e\n\n\u003c!-- Condition --\u003e\n\u003cdiv class=\"note-card \"\u003e\n    \u003cdiv class=\"item\"\u003e\n        \u003ch5 class=\"note-title\"\u003e\u003cspan\u003eCondition\u003c/span\u003e\u003c/h5\u003e\n        \n            \u003cdiv class=\"card\"\u003e\n                \u003cdiv class=\"card-body\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[[\u003c/span\u003e -z \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$string\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e]]\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003ethen\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;String is empty\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eelif\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e[[\u003c/span\u003e -n \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e$string\u003cspan style=\"color:#e6db74\"\u003e\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e]]\u003c/span\u003e; \u003cspan style=\"color:#66d9ef\"\u003ethen\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e  echo \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;String is not empty\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efi\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n            \u003c/div\u003e\n        \n    \u003c/div\u003e\n\u003c/div\u003e","tags":null,"title":"Bash Variables"},{"categories":null,"contents":"1 设置 Word 多级标题模板 在 Word 2021 上建立本科毕业论文的多级标题模板，最终实现效果如下：\n图 1.1: 多级标题最终效果 1.1 设置各级标题的样式\n首先在 “开始 —\u0026gt; 样式” 里设置每一级标题的格式。设置好样式后，选中文档内容再选择相应样式即可。\n图 1.2: 创建各类标题样式的入口 1.1.1 正文\n本文正文要求：中文宋体（西文 Times New Roman）五号、不加粗、首行缩进、对齐方式为两端对齐、段前段后 0 间距、行距为固定值 20 磅。设置如下：\n图 1.3: 正文的中英文和对齐方式设置 图 1.4: 正文段落设置 其中“段落”在“修改样式”左下角的“格式”中打开。\n1.1.2 一级标题\n一级标题要求：中西文为黑体（SimHei）三号，首行无缩进，对齐方式为居中、段前段后 0 行间距、行距为单倍行距。设置如下：\n图 1.5: 一级标题的中英文和段落设置 1.1.3 其他样式设置\n二、三级标题以及段落列表设置步骤同理，按照要求设置即可。\n1.2 设置各级列表\n点击 “开始 —\u0026gt; 段落 —\u0026gt; 多级列表”，然后点击多级列表最下面的“定义新的多级列表”开始设置。\n图 1.6: 创建多级列表入口 1.2.1 第1级别（对应一级标题）\n显然，图中每一个级别对应每一级别的标题。从第1级别开始设置。1级标题要求编号格式为“1 (空格) 一级标题”，设置如下：\n图 1.7: 一级列表的字体和段落设置 1.2.2 第2级别（对应二级标题） 设置步骤同理，此处需要注意的是2级标题的编号要求为“1.1 (空格) 二级标题”，即“1级编号.2级编号”，需要包含第1级别编号。设置步骤如下：\n图 1.8: 二级列表的字体和段落设置 “1级编号.2级编号”的设置技巧是：\n设置好“此级别的编号样式”。\n先到一级列表处把“要在库中显示的级别”重新设置为“级别1”，然后选择“包含的级别编号来自”为“级别1”。最后选择“此级别的编号样式”，可以看到编号按顺序显示。\n在1级别编号后加上“.”，即可呈现目标格式。\n1.2.3 第3级别（对应三级标题）\n设置步骤同理，此处需要注意的是三级标题的要求为“1.1.1 (空格) 三级标题”，即“1级编号.2级编号.3级编号”，需要包含第1和第2级别编号。设置步骤如下：\n图 1.9: 三级列表的字体和段落设置 “1级编号.2级编号.3级编号”的设置技巧是：\n设置好“此级别的编号样式”。\n先把一级列表重新设置级别1，二级列表重新设置级别2。然后，先选一次级别1，再选一次级别2。\n在1级别和2级别编号后加上“.”，即可呈现目标格式。\n2 页面设置 2.1 页面边距和大小设置\n用A4（210×297mm）标准大小的白纸打印。页边距按以下标准设置：上边距为：30 mm；下边距25mm；左边距和右边距为：25mm；装订线：10mm。设置步骤如下：\n图 2.1: 全文的页边距和纸张大小设置 注意要应用到全文。\n2.2 文档分节\n在论文叙述中，我们需要对全文不同部分进行分节，以便后续设置页眉、页脚和目录等。设置步骤如下：\n图 2.2: 在末尾添加分节符 图 2.3: 添加分节符的效果 3 图表 3.1 插入图片设置\n3.1.1 插入图片显示不全\n由于我们设置正文格式行间距为固定值 20 磅，所以在插入图表时，会出现只显示图片一部分的情况（图 3.1 所示）。\n图 3.1: 插入图片显示不全 此时需要更改图片的行间距为单倍行距。设置步骤如下：\n图 3.2: 调整图片间距 之后，可以看到图片显示完整（图 3.3 所示）。\n图 3.3: 图片显示完全 3.1.2 插入图片题注\n我们需要在图表下方加上题注，题注的格式是：“图（空格）章节号（点）图片在本章的编号”，即“图 1.1 图片名”。\n首先要创建本章节图注标签，设置步骤如下：\n图 3.4: 图注创建标签入口 图 3.5: 创建图注 插入图注之后的效果（图 3.6 所示）。\n图 3.6: 初始图注效果 可以看到图注的格式满足要求，但是文字样式不符合要求。\n下面需要调整图注的文字样式，设置步骤如下：\n图 3.7: 图注样式修改入口 修改图注的的中英文字体和段落设置（中文宋体（西文 Times New Roman）小五号、不加粗、对齐方式为居中对齐、段前段后 0 间距、但被行距），设置步骤如下：\n图 3.8: 图注的中英文设置 图 3.9: 图注段落设置 最后，可以看到图注的格式满足要求（图 3.10 所示）。\n图 3.10: 最终图注效果 ","date":"May 11, 2025","hero":"/posts/miscellanea/word-editing-tips/images/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/word-editing-tips/","summary":"\u003ch3 id=\"1-设置-word-多级标题模板\"\u003e1 设置 Word 多级标题模板\u003c/h3\u003e\n\u003cp\u003e在 Word 2021 上建立本科毕业论文的多级标题模板，最终实现效果如下：\u003c/p\u003e\n\u003cdiv style=\"text-align: center; width: 60%; margin: 0 auto;\"\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/多级标题多级列表-最终效果.png\" alt=\"final-result\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e图 1.1:\u003c/strong\u003e 多级标题最终效果 \u003c/center\u003e\n\u003c/div\u003e\n\u003cbr\u003e\n\u003cp\u003e\u003cstrong\u003e1.1 设置各级标题的样式\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e首先在 “开始 —\u0026gt; 样式” 里设置每一级标题的格式。设置好样式后，选中文档内容再选择相应样式即可。\u003c/p\u003e\n\u003cdiv style=\"text-align: center; width: 60%; margin: 0 auto;\"\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/多级标题-创建各类样式.png\" alt=\"create-style\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e图 1.2:\u003c/strong\u003e 创建各类标题样式的入口 \u003c/center\u003e\n\u003c/div\u003e\n\u003cbr\u003e\n\u003cp\u003e\u003cstrong\u003e1.1.1 正文\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e本文正文要求：中文宋体（西文 Times New Roman）五号、不加粗、首行缩进、对齐方式为两端对齐、段前段后 0 间距、行距为固定值 20 磅。设置如下：\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between;\"\u003e\n    \u003cimg src=\"images/多级标题-正文中文.png\" alt=\"Text in Chinese\" style=\"width: 45%\"/\u003e\n    \u003cimg src=\"images/多级标题-正文英文.png\" alt=\"Text in English\" style=\"width: 45%\"/\u003e\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cstrong\u003e图 1.3:\u003c/strong\u003e 正文的中英文和对齐方式设置\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center; width: 100%; margin: 0 auto;\"\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/多级标题-正文段落.png\" alt=\"Text paragraph\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e图 1.4:\u003c/strong\u003e 正文段落设置 \u003c/center\u003e\n\u003c/div\u003e\n\u003cbr\u003e\n\u003cp\u003e其中“段落”在“修改样式”左下角的“格式”中打开。\u003c/p\u003e","tags":null,"title":"一些 Word 编辑技巧"},{"categories":null,"contents":"磁矢势的推导 磁矢势其引入方式与电势类似，但有点硬凑出来的感觉。\n下面我们从电磁场的基本性质出发，系统推导磁矢势的数学表达式。\n与电势的类比\n对于稳恒电场，其基本性质由以下方程组描述：\n$$ \\begin{cases} \\nabla \\cdot \\vec{E} = \\rho / \\varepsilon_0 \\\\ \\nabla \\times \\vec{E} = 0 \\end{cases} $$\n电势的引入基于电场无旋性 $ \\nabla \\times \\vec{E} = 0 $。由于任意标量函数的梯度的旋度恒为零 $ \\nabla \\times (\\nabla U) = 0 $，我们可以定义电势 $ U $ 满足：\n$$ \\vec{E} = - \\nabla U $$\n顺便提一下，$ \\nabla \\times (\\nabla U) $ 类似于两个平行向量叉乘，围不成四边形，有向面积为 0。\n磁场的特殊性\n我们想按照电势的推导来推出类似磁势的东西，但却不能。\n对于稳恒磁场，其基本性质为：\n$$ \\begin{cases} \\nabla \\cdot \\vec{B} = 0 \\\\ \\nabla \\times \\vec{B} = \\mu_0 \\vec{j} \\end{cases} $$\n$$ \\because \\nabla \\times \\vec{B} \\neq 0 \\\\ \\therefore \\vec{E} \\neq - \\nabla U_m $$\n与电场不同，磁场是有旋场 $ \\nabla \\times \\vec{B} \\neq 0 $，因此不能简单地定义标量磁势。\n我们可以从磁场的无源性 $ \\nabla \\cdot \\vec{B} = 0 $ 入手\n$$ \\because \\nabla \\cdot \\vec{B} = 0, \\nabla \\cdot (\\nabla \\times \\vec{A}) = 0 \\\\ \\therefore \\vec{B} = \\nabla \\times \\vec{A} $$\n引入矢量函数 $ \\vec{A} $ 满足 $ \\nabla \\cdot (\\nabla \\times \\vec{A}) = 0 $。\n顺便提一下，$ \\nabla \\cdot (\\nabla \\times \\vec{A}) $ 类似于三个同一平面内的向量做混合积，围不成六面体，有向体积为 0。\n磁矢势的具体表达式 为了得到 $ \\vec{A} $ 的具体形式，我们从毕奥-萨伐尔定律出发进行推导。\n数学准备\n考虑图1所示的坐标系，定义：\n$\\vec{r} = (x, y, z)$ ：场点位置矢量 $\\vec{r\u0026rsquo;} = (x\u0026rsquo;, y\u0026rsquo;, z\u0026rsquo;)$ ：源点位置矢量 $ R = |\\vec{r} - \\vec{r\u0026rsquo;}| $ $ d \\vec{l\u0026rsquo;} $ ：电流元方向 图 1: 变量标识 利用矢量恒等式：\n$$ \\nabla \\times (\\varphi \\vec{C}) = \\nabla \\varphi \\times \\vec{C} + \\varphi (\\nabla \\times \\vec{C}) $$\n令 $ \\varphi = \\frac{1}{R}， \\vec{C} = d \\vec{l\u0026rsquo;} $，则有：\n$$ \\nabla \\times (\\frac{1}{R} d \\vec{l\u0026rsquo;}) = \\nabla \\frac{1}{R} \\times d \\vec{l\u0026rsquo;} + \\frac{1}{R} (\\nabla \\times d \\vec{l\u0026rsquo;}) $$\n计算各项：\n$$ \\nabla = (\\frac{\\partial}{\\partial x}, \\frac{\\partial}{\\partial y}, \\frac{\\partial}{\\partial z}) \\\\ \\nabla (\\frac{1}{R}) = \\nabla (\\frac{1}{|\\vec{r} - \\vec{r\u0026rsquo;}|}) = - \\frac{\\vec{R}}{R^3} \\\\ \\nabla \\times d \\vec{l\u0026rsquo;} = \\nabla \\times (x\u0026rsquo;, y\u0026rsquo;, z\u0026rsquo;) = 0 $$\n因此：\n$$ \\begin{aligned} \\nabla \\times (\\frac{1}{R} d \\vec{l\u0026rsquo;}) \u0026amp;= - \\frac{\\vec{R}}{R^3} \\times d \\vec{l\u0026rsquo;} + \\frac{1}{R} (0) \\\\ \u0026amp;= \\frac{d \\vec{l\u0026rsquo;} \\times \\vec{R}}{R^3} \\end{aligned} $$\n与毕奥-萨伐尔定律的联系\n毕奥-萨伐尔定律给出：\n$$ \\vec{B} = \\frac{\\mu_0}{4 \\pi} \\oint_{L\u0026rsquo;} \\frac{I d \\vec{l\u0026rsquo;} \\times \\vec{R}}{R^3} = \\frac{\\mu_0 I}{4 \\pi} \\oint_{L\u0026rsquo;} \\frac{d \\vec{l\u0026rsquo;} \\times \\vec{R}}{R^3} $$\n将前面的结果代入：\n$$ \\begin{aligned} \\vec{B} \u0026amp;= \\frac{\\mu_0 I}{4 \\pi} \\oint_{L\u0026rsquo;} \\nabla \\times (\\frac{1}{R} d \\vec{l\u0026rsquo;}) \\\\ \u0026amp;= \\nabla \\times (\\frac{\\mu_0 I}{4 \\pi} \\oint_{L\u0026rsquo;} (\\frac{1}{R} d \\vec{l\u0026rsquo;})) \\qquad (\\nabla \\times \\vec{a} + \\nabla \\times \\vec{b} = \\nabla \\times (\\vec{a} + \\vec{b}))\\\\ \u0026amp;= \\nabla \\times (\\frac{\\mu_0}{4 \\pi} \\oint_{L\u0026rsquo;} \\frac{I d \\vec{l\u0026rsquo;}}{R}) \\end{aligned} $$\n比较 $ \\vec{B} = \\nabla \\times \\vec{A} $，得到磁矢势的表达式：\n$$ \\vec{A} = \\frac{\\mu_0}{4 \\pi} \\oint_{L\u0026rsquo;} \\frac{I d \\vec{l\u0026rsquo;}}{R} $$\n磁矢势的物理意义 磁矢势虽然不像电势那样有直接的物理意义，但可以通过以下关系理解：\n$$ \\vec{B} = \\nabla \\times \\vec{A} \\ \\implies \\iint \\vec{B} \\cdot d \\vec{S} = \\oint \\vec{A} \\cdot d \\vec{l} \\ $$\n$ \\vec{A} $ 的物理意义（不像是物理意义）：在任意时刻，$ \\vec{A} $ 沿任一闭合回路的线积分等于该时刻通过回路内的磁通量。\n磁矢势的散度 磁矢势的旋度是磁感应强度，它的散度是：\n$$ \\nabla \\cdot \\vec{A} = \\nabla \\cdot (\\frac{\\mu_0}{4 \\pi} \\oint_{L\u0026rsquo;} \\frac{I d \\vec{l\u0026rsquo;}}{R}) = \\frac{\\mu_0 I}{4 \\pi} \\oint_{L\u0026rsquo;} \\nabla \\cdot (\\frac{d \\vec{l\u0026rsquo;}}{R}) \\qquad (\\nabla \\cdot (\\vec{a} + \\vec{b}) = \\nabla \\cdot \\vec{a} + \\nabla \\cdot \\vec{b}) $$\n由求导公式：\n$$ \\because \\frac{1}{R} = \\frac{1}{|\\vec{r} - \\vec{r\u0026rsquo;}|} \\\\ \\therefore \\nabla (\\frac{1}{R}) = - \\nabla\u0026rsquo; (\\frac{1}{R}) $$\n利用矢量恒等式：\n$$ \\begin{aligned} \\nabla \\cdot (\\varphi \\vec{A}) \u0026amp;= \\vec{A} \\cdot \\nabla \\varphi + \\varphi (\\nabla \\cdot \\vec{A}) \\\\ \\implies \\nabla \\cdot (\\frac{d \\vec{l\u0026rsquo;}}{R}) \u0026amp;= d \\vec{l\u0026rsquo;} \\cdot \\nabla (\\frac{1}{R}) + \\frac{1}{R} (\\nabla \\cdot d \\vec{l\u0026rsquo;}) \\\\ \u0026amp;= d \\vec{l\u0026rsquo;} \\cdot (- \\nabla\u0026rsquo; (\\frac{1}{R})) + \\frac{1}{R} (0) \\\\ \u0026amp;= - \\nabla\u0026rsquo; (\\frac{1}{R}) \\cdot d \\vec{l\u0026rsquo;} \\end{aligned} $$\n由斯托克斯定理：\n$$ \\begin{aligned} \\nabla \\cdot \\vec{A} \u0026amp;= \\frac{\\mu_0 I}{4 \\pi} \\oint_{L\u0026rsquo;} \\nabla \\cdot (\\frac{d \\vec{l\u0026rsquo;}}{R}) \\\\ \u0026amp;= \\frac{\\mu_0 I}{4 \\pi} \\oint_{L\u0026rsquo;} - \\nabla\u0026rsquo; (\\frac{1}{R}) \\cdot d \\vec{l\u0026rsquo;} \\\\ \u0026amp;= - \\frac{\\mu_0 I}{4 \\pi} \\oint_{L\u0026rsquo;} \\nabla\u0026rsquo; (\\frac{1}{R}) \\cdot d \\vec{l\u0026rsquo;} \\\\ \u0026amp;= - \\frac{\\mu_0 I}{4 \\pi} \\iint [\\nabla\u0026rsquo; \\times \\nabla\u0026rsquo; (\\frac{1}{R})] \\cdot d \\vec{S} = 0 \\end{aligned} $$\n对面电流和体电流，该式均成立。\n$$ \\nabla \\cdot \\vec{A} \\equiv 0 $$\n","date":"April 20, 2025","hero":"/posts/miscellanea/magnetic-vector-potential/images/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/magnetic-vector-potential/","summary":"\u003ch3 id=\"磁矢势的推导\"\u003e磁矢势的推导\u003c/h3\u003e\n\u003cp\u003e磁矢势其引入方式与电势类似，但有点硬凑出来的感觉。\u003c/p\u003e\n\u003cp\u003e下面我们从电磁场的基本性质出发，系统推导磁矢势的数学表达式。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e与电势的类比\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e对于稳恒电场，其基本性质由以下方程组描述：\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{cases}\n\\nabla \\cdot \\vec{E} = \\rho / \\varepsilon_0 \\\\\n\\nabla \\times \\vec{E} = 0\n\\end{cases}\n$$\u003c/p\u003e\n\u003cp\u003e电势的引入基于电场无旋性 $ \\nabla \\times \\vec{E} = 0 $。由于任意标量函数的梯度的旋度恒为零 $ \\nabla \\times (\\nabla U) = 0 $，我们可以定义电势 $ U $ 满足：\u003c/p\u003e\n\u003cp\u003e$$\n\\vec{E} = - \\nabla U\n$$\u003c/p\u003e\n\u003cp\u003e顺便提一下，$ \\nabla \\times (\\nabla U) $ 类似于两个平行向量叉乘，围不成四边形，有向面积为 0。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e磁场的特殊性\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e我们想按照电势的推导来推出类似磁势的东西，但却不能。\u003c/p\u003e\n\u003cp\u003e对于稳恒磁场，其基本性质为：\u003c/p\u003e\n\u003cp\u003e$$\n\\begin{cases}\n\\nabla \\cdot \\vec{B} = 0 \\\\\n\\nabla \\times \\vec{B} = \\mu_0 \\vec{j}\n\\end{cases}\n$$\u003c/p\u003e","tags":null,"title":"磁矢势"},{"categories":null,"contents":"一、结论速览 ✅ 关键结论：Tx 端电流从同名端流入时，Rx 端电流从同名端流出，互感值的正负取决于 Rx 端电压和 Rx 端电流的参考方向关系：\n关联参考方向时：$ M \u0026lt; 0 $ 非关联参考方向时：$ M \u0026gt; 0 $ 二、理论分析 我们首先来看一个简单的互感线圈模型，其中电压 $ U_R $ 和电流 $ I_R $ 取非关联参考方向，且同名端对齐：\n图 1: 互感线圈结构示意图 假设通过线圈的电流为 $ I = I_m sin(\\omega t) $，我们重点分析前半个周期。\n1.第一个 1/4 周期分析（$ t = 0 \\to \\frac{\\pi}{2} $）\n在这个时间段内：\n电流为正且在增加（$ \\frac{dI}{dt} \u0026gt; 0 $）\n根据右手定则，磁通量方向如图 1 所示，并且在增大\n因为感应电动势的方向总是企图由它产生的感应电流建立一个附加磁通量，以阻碍引起感应电动势的那个磁通量的变化。可以知道感应电流 $ I_{eT} $ 产生向下的磁通量，感应电流 $ I_{eR} $ 产生向上的磁通量\n对应的感应电动势 $ e_T, e_R $，感应电流 $ I_{eT}, I_{eR} $ 和 方向如图 2 所示：\n图 2: 在 $ t = 0 \\to \\frac{\\pi}{2} $ 时的感应电动势和感应电流 感应电压 $ U_R $ 是外电路测量值，满足：$ U_R \u0026gt; 0 $ 根据感应电压公式 $ U = -e_R = M \\frac{d I_T}{dt} $ 此时 $ \\frac{d I_T}{dt} \u0026gt; 0 $，故 $ M \u0026gt; 0 $ 2.第二个 1/4 周期分析（$ t = \\frac{\\pi}{2} \\to \\pi $）\n在这个时间段内：\n电流正向减小（$ \\frac{dI}{dt} \u0026lt; 0 $） 根据右手定则，磁通量方向如图 1 所示，并且在减小 感应电流 $ I_{eT} $ 产生向上的磁通量，感应电流 $ I_{eR} $ 产生向下的磁通量 感应电流和电动势方向如图 3 所示：\n感应电压 $ U_R \u0026lt; 0 $ 根据感应电压公式 $ U_R = -e_R = M \\frac{dI}{dt} $ 此时 $ \\frac{dI}{dt} \u0026lt; 0 $，仍得 $ M \u0026gt; 0 $ 图 3: 在 $ t = \\frac{\\pi}{2} \\to \\pi $ 时的感应电动势和感应电流 同名端的定义：具有磁耦合关系的两个线圈，当任何一个线圈中通过的电流发生变化时，在两线圈上引起的感应电动势的极性始终保持一致的端子称为同名端。\n可以看到上面的分析中，$ e_T $ 和 $ e_R $ 的方向始终保持一致。\n用这种方法分析 $ U $ 和 $ I $ 取关联方向时，互感值取负号。线圈换另外一个方向得到的结论也是相同的。在此就不做展开了，有兴趣的读者可以自行尝试。\n三、MATLAB Simscape 仿真验证 为了直观验证理论分析，我使用 MATLAB Simscape 搭建了 1Tx-1Rx 电路仿真模型。\n理论电路图如图 4 所示：\n图 4: 1Rx-1Tx 电路结构示意图 仿真电路参数\n交流电压源：幅值 $ 10 V $，频率 $ 1 Hz $ 电阻 $ 10 \\Omega $ 电感 $ 0.159155 H $ 电容 $ 0.159155 F $ 非关联参考方向分析\n仿真电路图如图 5 所示:\n图 5: 1Rx-1Tx 仿真电路图（电压电流取非关联参考方向） 关键波形测量\n电流传感器测量 Tx 回路电流 $ I_T $ 电流传感器测量 Rx 回路电流 $ I_R $ 对于这个电路（我们假设 Tx 和 Rx 电路处于谐振状态），我们列电路方程如下：\n$$ Rx: I_R Z_R = U_R = j \\omega M I_T $$\n图 6: 电压电流取非关联参考方向时，Tx 电路电流（左）与 Rx 电路电流（右）波形 相位关系分析\n从仿真波形可以清晰观察到：\n相位差：$ I_T $ 滞后 $ I_R $ $ 0.005 s $ 频域关系：$ I_R Z_R = U_R = j \\omega M I_T $ ✅ 验证结果：只有当 $ M \u0026gt; 0 $ 时，$ I_T $ 才会滞后 $ I_R $ $ 90 \\degree $，这与我们的理论分析完全一致。\n关联参考方向分析\n仿真电路图如图 7 所示:\n图 7: 1Rx-1Tx 仿真电路图（电压电流取关联参考方向） 对于这个电路（我们假设 Tx 和 Rx 电路处于谐振状态），我们列电路方程如下：\n$$ Rx: I_R Z_R + U_R = 0 \\ \\implies I_R Z_R = -U_R = -j \\omega M I_T $$\n从上面的仿真波形可以清晰观察到：\n相位差：$ I_T $ 滞后 $ I_R $ $ 0.005 s $ 频域关系：$ I_R Z_R = -U_R = -j \\omega M I_T $ ✅ 验证结果：只有当 $ M \u0026lt; 0 $ 时，$ I_T $ 才会滞后 $ I_R $ $ 90 \\degree $。\n","date":"April 2, 2025","hero":"/posts/wpt/mutual-indcution/images/WPTscenario.gif","permalink":"https://qingbo12.github.io/posts/wpt/mutual-indcution/","summary":"\u003ch3 id=\"一结论速览\"\u003e一、结论速览\u003c/h3\u003e\n\u003cp\u003e✅ 关键结论：Tx 端电流从同名端流入时，Rx 端电流从同名端流出，互感值的正负取决于 Rx 端电压和 Rx 端电流的参考方向关系：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e关联参考方向时：$ M \u0026lt; 0 $\u003c/li\u003e\n\u003cli\u003e非关联参考方向时：$ M \u0026gt; 0 $\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"二理论分析\"\u003e二、理论分析\u003c/h3\u003e\n\u003cp\u003e我们首先来看一个简单的互感线圈模型，其中电压 $ U_R $ 和电流 $ I_R $ 取非关联参考方向，且同名端对齐：\u003c/p\u003e\n\u003cdiv style=\"text-align: center; width: 50%; margin: 0 auto;\"\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/mutual-induction-coil.png\" alt=\"mutual-induction-coil\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e图 1:\u003c/strong\u003e 互感线圈结构示意图 \u003c/center\u003e\n\u003c/div\u003e\n\u003cbr/\u003e\n\u003cp\u003e假设通过线圈的电流为 $ I = I_m sin(\\omega t) $，我们重点分析前半个周期。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1.第一个 1/4 周期分析（$ t = 0 \\to \\frac{\\pi}{2} $）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e在这个时间段内：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e电流为正且在增加（$ \\frac{dI}{dt} \u0026gt; 0 $）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e根据右手定则，磁通量方向如图 1 所示，并且在增大\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e因为感应电动势的方向总是企图由它产生的感应电流建立一个附加磁通量，以阻碍引起感应电动势的那个磁通量的变化。可以知道感应电流 $ I_{eT} $ 产生向下的磁通量，感应电流 $ I_{eR} $ 产生向上的磁通量\u003c/p\u003e","tags":null,"title":"互感值的正负判断：从理论到仿真的保姆级解析"},{"categories":null,"contents":"前言 在电路分析中，自感值的正负判断是电磁学学习的一个重要知识点。很多初学者容易混淆关联参考方向和非关联参考方向下的自感值符号问题。本文将通过理论分析和 MATLAB 仿真相结合的方式，深入浅出地讲解这个知识点。\n核心结论 ✅ 关键结论：自感值的正负取决于电压和电流的参考方向关系：\n关联参考方向时：$ L \u0026gt; 0 $ 非关联参考方向时：$ L \u0026lt; 0 $ 理论分析 我们首先来看一个简单的线圈模型，此时电压和电流取关联参考方向：\n图 1: 线圈结构示意图 假设通过线圈的电流为 $ I = I_m sin(\\omega t) $，我们重点分析前半个周期。\n1.第一个 1/4 周期分析（$ t = 0 \\to \\frac{\\pi}{2} $）\n在这个时间段内：\n电流为正且在增加（$ \\frac{dI}{dt} \u0026gt; 0 $）\n根据右手定则，磁通量向上增加\n由楞次定律可知，感应电动势 $ e $ 会产生感应电流，感应电流产生的磁通量方向向下\n对应的感应电流 $ I_e $ 和感应电动势 $ e $ 方向如图 2(a) 所示：\n为了更清楚地理解，我们把感应电动势和感应电流抽象到我们熟悉的电源模型，如图 2(b) 所示。\n自感电压 $ U = U_{ab} $ 是外电路测量值，满足：$ U = \\phi_a - \\phi_b \u0026gt; 0 $ 根据自感电压公式 $ U = -e = L \\frac{dI}{dt} $ 此时 $ \\frac{dI}{dt} \u0026gt; 0 $，故 $ L \u0026gt; 0 $ 图 2: $ U $ 和 $ I $ 取关联方向在 $ t = 0 \\to \\frac{\\pi}{2} $ 时的自感电压和感应电动势 2.第二个 1/4 周期分析（$ t = \\frac{\\pi}{2} \\to \\pi $）\n在这个时间段内：\n电流正向减小（$ \\frac{dI}{dt} \u0026lt; 0 $） 磁通量向上减小 感应电动势 $ e $ 会产生感应电流，感应电流产生的磁通量方向向上 感应电流和电动势方向如图 3(a) 所示：\n自感电压 $ U = U_{ab} $，满足：$ U = \\phi_a - \\phi_b \u0026lt; 0 $ 根据自感电压公式 $ U = -e = L \\frac{dI}{dt} $ 此时 $ \\frac{dI}{dt} \u0026lt; 0 $，仍得 $ L \u0026gt; 0 $ 图 3: $ U $ 和 $ I $ 取关联方向在 $ t = \\frac{\\pi}{2} \\to \\pi $ 时的自感电压和感应电动势 用这种方法分析 $ U $ 和 $ I $ 取非关联方向时，自感值取负号。线圈换另外一个方向得到的结论也是相同的。在此就不做展开了，有兴趣的读者可以自行尝试。\nMATLAB Simscape 仿真验证 为了直观验证理论分析，我使用 MATLAB Simscape 搭建了 RL 电路仿真模型。\n仿真电路参数\n交流电压源：幅值 $ 10 V $，频率 $ 50 Hz $ 电阻 $ 10 \\Omega $ 电感 $ 1 \\mu H $ 关联参考方向仿真\n仿真电路图如图 4 所示:\n图 4: RL 仿真电路图 关键波形测量\n电流传感器测量回路电流 $ I $ 电压传感器测量电感两端电压 $ U $（与电流取关联参考方向） 电流和电压的仿真波形图如图 5 所示:\n图 5: RL 电路电流（左）与电感电压（右）波形 相位关系分析\n从仿真波形可以清晰观察到：\n相位差：电感电压超前电流 $ 0.005 s $ 理论计算：$ \\frac{1}{4} T = \\frac{1}{4 f} = \\frac{1}{4 \\times 50} = 0.005 s (90 \\degree) $ 频域关系：$ U = -e = j \\omega L I $ ✅ 验证结果：只有当 $ L \u0026gt; 0 $ 时，电压才会超前电流 $ 90 \\degree $，这与我们的理论分析完全一致。\n非关联参考方向仿真\n仿真电路图如图 6 所示，得到的波形仿真波形图如图 7 所示:\n图 6: RL 仿真电路图（电压电流取非关联参考方向） 图 7: 电压电流取非关联参考方向时，RL 电路电流（左）与电感电压（右）波形 相位关系分析\n从仿真波形可以清晰观察到：\n相位差：电感电压滞后电流 $ 0.005 s $ 频域关系：$ U = -e = j \\omega L I $ ✅ 验证结果：只有当 $ L \u0026lt; 0 $ 时，电压才会滞后电流 $ 90 \\degree $，这与我们的理论分析完全一致。\n","date":"March 29, 2025","hero":"/posts/wpt/induction/images/WPTscenario.gif","permalink":"https://qingbo12.github.io/posts/wpt/induction/","summary":"\u003ch3 id=\"前言\"\u003e前言\u003c/h3\u003e\n\u003cp\u003e在电路分析中，自感值的正负判断是电磁学学习的一个重要知识点。很多初学者容易混淆关联参考方向和非关联参考方向下的自感值符号问题。本文将通过理论分析和 MATLAB 仿真相结合的方式，深入浅出地讲解这个知识点。\u003c/p\u003e\n\u003ch3 id=\"核心结论\"\u003e核心结论\u003c/h3\u003e\n\u003cp\u003e✅ 关键结论：自感值的正负取决于电压和电流的参考方向关系：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e关联参考方向时：$ L \u0026gt; 0 $\u003c/li\u003e\n\u003cli\u003e非关联参考方向时：$ L \u0026lt; 0 $\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"理论分析\"\u003e理论分析\u003c/h3\u003e\n\u003cp\u003e我们首先来看一个简单的线圈模型，此时电压和电流取关联参考方向：\u003c/p\u003e\n\u003cdiv style=\"text-align: center; width: 30%; margin: 0 auto;\"\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/coil.png\" alt=\"coil\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e图 1:\u003c/strong\u003e 线圈结构示意图 \u003c/center\u003e\n\u003c/div\u003e\n\u003cbr/\u003e\n\u003cp\u003e假设通过线圈的电流为 $ I = I_m sin(\\omega t) $，我们重点分析前半个周期。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1.第一个 1/4 周期分析（$ t = 0 \\to \\frac{\\pi}{2} $）\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e在这个时间段内：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e电流为正且在增加（$ \\frac{dI}{dt} \u0026gt; 0 $）\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e根据右手定则，磁通量向上增加\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e由楞次定律可知，感应电动势 $ e $ 会产生感应电流，感应电流产生的磁通量方向向下\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对应的感应电流 $ I_e $ 和感应电动势 $ e $ 方向如图 2(a) 所示：\u003c/p\u003e","tags":null,"title":"自感值的正负判断：从理论分析到仿真验证"},{"categories":null,"contents":"📍 安徽省美术馆 | 3月22日打卡\n周末来打卡合肥这座超有设计感的美术馆了！2022 年才开放，建筑本身就像一件艺术品✨ 黑灰色旋转楼梯+钢索吊桥，随手一拍就是工业风大片！📸\n🔥 重点看了这几个快撤的展（抓紧冲！）：\n1️⃣ 「柔软与坚硬·刘春杰艺术展」\n一整面鲁迅画像超震撼！还有“废纸三千”装置艺术，废纸堆也能成为艺术，绝了！\n2️⃣ 「敬惜字纸——当代观念性纸艺术巡展」\n纸还能这样玩？！火车票拼成的“游子身上衣”太戳泪点了😭 细节满分！\n3️⃣ 「烟云飞动——童乃寿黄山画展」\n全是黄山主题！云海磅礴，看完直接种草黄山行⛰️ 想立刻买票去爬山！\n📌 地址： 合肥滨湖新区巢湖北路（近渡江战役纪念馆）\n美术馆外观 狂人日记：从来如此便对吗 游子身上衣 黄山烟云 ","date":"March 26, 2025","hero":"/posts/life/ahart-march/images/image-life.jpg","permalink":"https://qingbo12.github.io/posts/life/ahart-march/","summary":"\u003cp\u003e📍 安徽省美术馆 | 3月22日打卡\u003c/p\u003e\n\u003cp\u003e周末来打卡合肥这座超有设计感的美术馆了！2022 年才开放，建筑本身就像一件艺术品✨ 黑灰色旋转楼梯+钢索吊桥，随手一拍就是工业风大片！📸\u003c/p\u003e\n\u003cp\u003e🔥 重点看了这几个快撤的展（抓紧冲！）：\u003c/p\u003e\n\u003cp\u003e1️⃣ 「柔软与坚硬·刘春杰艺术展」\u003c/p\u003e\n\u003cp\u003e一整面鲁迅画像超震撼！还有“废纸三千”装置艺术，废纸堆也能成为艺术，绝了！\u003c/p\u003e\n\u003cp\u003e2️⃣ 「敬惜字纸——当代观念性纸艺术巡展」\u003c/p\u003e\n\u003cp\u003e纸还能这样玩？！火车票拼成的“游子身上衣”太戳泪点了😭 细节满分！\u003c/p\u003e\n\u003cp\u003e3️⃣ 「烟云飞动——童乃寿黄山画展」\u003c/p\u003e\n\u003cp\u003e全是黄山主题！云海磅礴，看完直接种草黄山行⛰️ 想立刻买票去爬山！\u003c/p\u003e\n\u003cp\u003e📌 地址： 合肥滨湖新区巢湖北路（近渡江战役纪念馆）\u003c/p\u003e\n\u003cdiv\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/美术馆外观.jpg\" alt=\"美术馆外观\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e\u003c/strong\u003e 美术馆外观 \u003c/center\u003e\n\u003c/div\u003e\n\u003cdiv\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/从来如此便对吗.jpg\" alt=\"从来如此便对吗\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e\u003c/strong\u003e 狂人日记：从来如此便对吗 \u003c/center\u003e\n\u003c/div\u003e\n\u003cdiv\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/游子身上衣.jpg\" alt=\"游子身上衣\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e\u003c/strong\u003e 游子身上衣 \u003c/center\u003e\n\u003c/div\u003e\n\u003cdiv\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/黄山烟云.jpg\" alt=\"黄山烟云\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003e\u003c/strong\u003e 黄山烟云 \u003c/center\u003e\n\u003c/div\u003e","tags":null,"title":"安徽省美术馆3月展打卡手记"},{"categories":null,"contents":"非机动车骑行规则 走非机动车道 不能闯红灯 不能逆行 不要去斑马线上抢发车位 上斑马线要推车 不要停在危险区等红绿灯，防止车右拐撞上 过斑马线时，有车停住也不要加速通过，因为旁边的车不一定能看到 路口通行\nFigure 1: 路口往前或者往左往右. 路口往前或者往左往右时，不要停在斑马线上等，也不要从斑马线上通过。\nFigure 2: 去马路对面. 去马路对面时，不要从斑马线上通过，也不要从旁边通过。应该下车推行。\n自行车骑行 座高：以右脚为例：把脚蹬转到最下方，人坐上后，脚心位置踩上，腿能伸直为标准。这样当用前脚掌蹬到最下方时，腿会有一点弯度！\n踩踏：前脚掌踩踏，脚尖朝前，大腿发力\n","date":"March 7, 2025","hero":"/posts/life/cycle/images/image-life.jpg","permalink":"https://qingbo12.github.io/posts/life/cycle/","summary":"\u003ch3 id=\"非机动车骑行规则\"\u003e非机动车骑行规则\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e走非机动车道\u003c/li\u003e\n\u003cli\u003e不能闯红灯\u003c/li\u003e\n\u003cli\u003e不能逆行\u003c/li\u003e\n\u003cli\u003e不要去斑马线上抢发车位\u003c/li\u003e\n\u003cli\u003e上斑马线要推车\u003c/li\u003e\n\u003cli\u003e不要停在危险区等红绿灯，防止车右拐撞上\u003c/li\u003e\n\u003cli\u003e过斑马线时，有车停住也不要加速通过，因为旁边的车不一定能看到\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e路口通行\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between;\"\u003e\n    \u003cimg src=\"images/intersection-regular.png\" alt=\"intersection-regular\" style=\"width: 45%\"/\u003e\n    \u003cimg src=\"images/intersection-wrong.png\" alt=\"intersection-wrong\" style=\"width: 45%\"/\u003e\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cstrong\u003eFigure 1:\u003c/strong\u003e 路口往前或者往左往右.\n\u003c/div\u003e\n\u003cp\u003e路口往前或者往左往右时，不要停在斑马线上等，也不要从斑马线上通过。\u003c/p\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between;\"\u003e\n    \u003cimg src=\"images/zebracrossing-cycle.png\" alt=\"zebracrossing-cycle\" style=\"width: 45%\"/\u003e\n    \u003cimg src=\"images/zebracrossing-reverse.png\" alt=\"zebracrossing-reverse\" style=\"width: 45%\"/\u003e\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cstrong\u003eFigure 2:\u003c/strong\u003e 去马路对面.\n\u003c/div\u003e\n\u003cp\u003e去马路对面时，不要从斑马线上通过，也不要从旁边通过。应该下车推行。\u003c/p\u003e\n\u003ch3 id=\"自行车骑行\"\u003e自行车骑行\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e座高：以右脚为例：把脚蹬转到最下方，人坐上后，脚心位置踩上，腿能伸直为标准。这样当用前脚掌蹬到最下方时，腿会有一点弯度！\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e踩踏：前脚掌踩踏，脚尖朝前，大腿发力\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e","tags":null,"title":"非机动车骑行规则"},{"categories":null,"contents":"A-Star Path Planning Algorithm 应用场景\n很多NPC或者机器人（AI）需要自动寻路的功能来达到与玩家交互的功能。例如吃鸡游戏中，机器人在城镇中跑毒就需要自动寻路，不然可能就被墙卡死了。A-Star（A*）寻路算法原理与实现\n算法简介\nA* 算法是一种很常用的路径查找和图形遍历算法，最初发表于 1968 年，由 Stanford 研究院的 Peter Hart, Nils Nilsson 以及 Bertram Raphael 发表。它可以被认为是 Dijkstra 算法的扩展。\n由于借助启发函数的引导，A* 算法通常拥有更好的性能。路径规划之 A* 算法\n发展历程\n广度优先搜索：从起点开始，首先遍历起点周围邻近的点，然后再遍历已经遍历过的点邻近的点，逐步的向外扩散，直到找到终点。\n这种算法就像洪水（Flood fill）一样向外扩张。\nDijkstra算法：在Dijkstra算法中，需要计算每一个节点距离起点的总移动代价。同时，还需要一个优先队列结构。对于所有待遍历的节点，放入优先队列中会按照代价进行排序。\n在算法运行的过程中，每次都从优先队列中选出代价最小的作为下一个遍历的节点。直到到达终点为止。\n最佳优先搜索：在一些情况下，如果我们可以预先计算出每个节点到终点的距离，则我们可以利用这个信息更快的到达终点。\n其原理也很简单。与Dijkstra算法类似，我们也使用一个优先队列，但此时以每个节点到达终点的距离作为优先级，每次始终选取到终点移动代价最小（离终点最近）的节点作为下一个遍历的节点。\nA*算法\nA*算法实际上是综合上面这些算法的特点于一身的。\nA*算法通过下面这个函数来计算每个节点的优先级。\n$$ f(n) = g(n) + h(n) $$\n其中：\n$ f(n) $ 是节点 $ n $ 的综合优先级。当我们选择下一个要遍历的节点时，我们总会选取综合优先级最高（值最小）的节点。 $ g(n) $ 是节点 $ n $ 距离起点的代价。 $ h(n) $ 是节点 $ n $ 距离终点的预计代价，这也就是 A* 算法的启发函数。 A* 算法在运算过程中，每次从优先队列中选取 $ f(n) $ 值最小（优先级最高）的节点作为下一个待遍历的节点。\n另外，A* 算法使用两个集合来表示待遍历的节点，与已经遍历过的节点，这通常称之为 open_set 和 close_set。\n完整的 A* 算法描述如下：\n* 初始化open_set和close_set； * 将起点加入open_set中，并设置优先级为0（优先级最高）； * 如果open_set不为空，则从open_set中选取优先级最高的节点n： * 如果节点n为终点，则： * 从终点开始逐步追踪parent节点，一直达到起点； * 返回找到的结果路径，算法结束； * 如果节点n不是终点，则： * 将节点n从open_set中删除，并加入close_set中； * 遍历节点n所有的邻近节点： * 如果邻近节点m在close_set中，则： * 跳过，选取下一个邻近节点 * 如果邻近节点m也不在open_set中，则： * 设置节点m的parent为节点n * 计算节点m的优先级 * 将节点m加入open_set中 关于距离\n欧几里得距离\n指两个节点之间的直线距离\n计算方法:\nfunction heuristic(node) = dx = abs(node.x - goal.x) dy = abs(node.y - goal.y) return D * sqrt(dx * dx + dy * dy) 欧几里得距离有两个弊端：\n计算过程中伴随着平方与开根号运算，并且需要使用浮点数，性能差。\n$ h(n) $ 的值永远小于或等于格子 $ n $ 到终点的最短实际距离。\n曼哈顿距离\n简单来说就是只准水平或垂直移动的最短距离，示意图如下：\nFigure 1: Manhattan Distance. 计算方法：\nfunction heuristic(node) = dx = abs(node.x - goal.x) dy = abs(node.y - goal.y) return D * (dx + dy) 相比欧几里得距离，只需要计算加减法，并且连浮点数都不需要。\n由于我们的格子可以对角线移动，因此不考虑障碍物的话，或者障碍物在两个格子形成的包围盒内，曼哈顿距离肯定大于或等于格子 $ n $ 到终点的最短实际距离。\n对角线+直线距离\n如果可以对角线移动，那么我们就可以根据水平方向的差值与竖直方向的差值中较小的那个值，计算出对角线，然后再平移。示意图如下：\nFigure 2: Diagonal Distance. 计算方法：\nfunction heuristic(node) = dx = abs(node.x - goal.x) dy = abs(node.y - goal.y) return D * (dx + dy) + (D2 - 2 * D) * min(dx, dy) $ h(n) $ 的值小于或等于格子 $ n $ 到终点的最短实际距离。不考虑障碍物的情况下，肯定等于格子 $ n $ 到终点的最短实际距离。\n由于计算对角线同样需要开根号以及浮点数。为了加快计算，我们可以假设两个格子间的距离为 10，然后直接认为对角线距离为 14（不是 $ \\sqrt{200} $ 了），这样就可以避免浮点数和根号运算了。\n启发函数\n启发函数会影响 A* 算法的行为。路径规划之 A* 算法\n如果 $ h(n) $ 始终小于等于节点 $ n $ 到终点的代价，则 A* 算法保证一定能够找到最短路径。但是当 $ h(n) $ 的值越小，算法将遍历越多的节点，也就导致算法越慢。\n如果 $ h(n) $ 的值比节点 $ n $ 到终点的代价要大，则 A* 算法不能保证找到最短路径，不过此时会很快。\n如果 $ h(n) $ 完全等于节点 $ n $ 到终点的代价，则 A* 算法将找到最佳路径，并且速度很快。可惜的是，并非所有场景下都能做到这一点。因为在没有达到终点之前，我们很难确切算出距离终点还有多远。\n在极端情况下，当启发函数 $ h(n) $ 始终为 $ 0 $，则将由 $ g(n) $ 决定节点的优先级，此时算法就退化成了 Dijkstra 算法。\n若 $ h(n) $ 远远大于 $ g(n) $，那么 $ f(n) $ 的值就主要取决于 $ h(n) $，A* 算法就演变成了 BFS 算法。\n第 1、2 点解释：A-Star（A*）寻路算法原理与实现\n来看一个例子：\nFigure 3: 启发式函数对算法速度和结果的影响. 若使用曼哈顿距离，$ f(1) = g(1) + h(1) = 14 + 190 = 204 $，$ f(2) = g(2) + h(2) = 74 + 90 = 184 $，就是说我宁可考虑格子 2 也不会去考虑格子 1。\n但是使用对角线距离，$ f(1) = g(1) + h(1) = 14 + 136 = 150 $，$ f(2) = g(2) + h(2) = 74 + 78 = 152 $，那就变得格子 1 要优先考虑。\n两种 $ h(n) $ 得到的路径如下：\nFigure 4: 左：曼哈顿距离的结果，右：对角线距离的结果 可以看到对角线距离考虑的节点更多，但是路径更短、结果更好。\n题外话\n计算 $ g(n) $ 有两种方法 路径规划之 A* 算法\n直接用对角线距离计算与起点间的距离 父节点与起点的距离加上当前节点与父节点的距离 前一种方法需要每次都计算，后一种方法需要存储父节点信息。一个耗费计算时间，一个耗费存储空间，根据实际情况来选择。\n当我们地图被某些障碍物分割，只留了一两个可通过的小口时，可以将寻路过程拆解开来，例如下图：A-Star（A*）寻路算法原理与实现\nFigure 5: 地图中间有一个可通过的小口. 可以将起点到终点的寻路过程拆分为起点到A点，A点到终点的两段寻路过程，来减少不必要的计算。\n当你有相当多的寻路者和一块很大的地图时，寻路占用了大量的 CPU 时间。A星算法详解(个人认为最详细,最通俗易懂的一个版本)\n使用小地图或者更少的寻路者。\n千万不要同时给多个寻路者寻路。取而代之的是把它们放入队列中，分散到几个游戏周期中。如果你的游戏以每秒 40 周期的速度运行，没人能察觉到。但是如果同时有大量的寻路者在寻路的话，他们会马上就发现游戏慢下来了。\n考虑在地图中使用更大的方格。这减少了寻路时需要搜索的方格数量。如果你是有雄心的话，你可以设计多套寻路方案，根据路径的长度而使用在不同场合。这也是专业人士的做法，对长路径使用大方格，当你接近目标时使用小方格。\n","date":"March 6, 2025","hero":"/posts/miscellanea/algorithm/images/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/algorithm/","summary":"\u003ch3 id=\"a-star-path-planning-algorithm\"\u003eA-Star Path Planning Algorithm\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e应用场景\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e很多NPC或者机器人（AI）需要\u003cstrong\u003e自动寻路\u003c/strong\u003e的功能来达到与玩家交互的功能。例如吃鸡游戏中，机器人在城镇中跑毒就需要自动寻路，不然可能就被墙卡死了。\u003ca href=\"https://zhuanlan.zhihu.com/p/385733813\" target=\"_blank\" rel=\"noopener\"\u003eA-Star（A*）寻路算法原理与实现\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e算法简介\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA* 算法是一种很常用的路径查找和图形遍历算法，最初发表于 1968 年，由 Stanford 研究院的 Peter Hart, Nils Nilsson 以及 Bertram Raphael 发表。它可以被认为是 Dijkstra 算法的扩展。\u003c/p\u003e\n\u003cp\u003e由于借助启发函数的引导，A* 算法通常拥有更好的性能。\u003ca href=\"https://paul.pub/a-star-algorithm/\" target=\"_blank\" rel=\"noopener\"\u003e路径规划之 A* 算法\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e发展历程\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e广度优先搜索：从起点开始，首先遍历起点周围邻近的点，然后再遍历已经遍历过的点邻近的点，逐步的向外扩散，直到找到终点。\u003c/p\u003e\n\u003cp\u003e这种算法就像洪水（Flood fill）一样向外扩张。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDijkstra算法：在Dijkstra算法中，需要计算每一个节点距离起点的总移动代价。同时，还需要一个优先队列结构。对于所有待遍历的节点，放入优先队列中会按照代价进行排序。\u003c/p\u003e\n\u003cp\u003e在算法运行的过程中，每次都从优先队列中选出代价最小的作为下一个遍历的节点。直到到达终点为止。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e最佳优先搜索：在一些情况下，如果我们可以预先计算出每个节点到终点的距离，则我们可以利用这个信息更快的到达终点。\u003c/p\u003e\n\u003cp\u003e其原理也很简单。与Dijkstra算法类似，我们也使用一个优先队列，但此时以每个节点到达终点的距离作为优先级，每次始终选取到终点移动代价最小（离终点最近）的节点作为下一个遍历的节点。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eA*算法\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA*算法实际上是综合上面这些算法的特点于一身的。\u003c/p\u003e\n\u003cp\u003eA*算法通过下面这个函数来计算每个节点的优先级。\u003c/p\u003e\n\u003cp\u003e$$\nf(n) = g(n) + h(n)\n$$\u003c/p\u003e\n\u003cp\u003e其中：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$ f(n) $ 是节点 $ n $ 的综合优先级。当我们选择下一个要遍历的节点时，我们总会选取综合优先级最高（值最小）的节点。\u003c/li\u003e\n\u003cli\u003e$ g(n) $ 是节点 $ n $ 距离起点的代价。\u003c/li\u003e\n\u003cli\u003e$ h(n) $ 是节点 $ n $ 距离终点的预计代价，这也就是 A* 算法的启发函数。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA* 算法在运算过程中，每次从优先队列中选取 $ f(n) $ 值最小（优先级最高）的节点作为下一个待遍历的节点。\u003c/p\u003e","tags":null,"title":"Algorithms"},{"categories":null,"contents":"Requirement-Driven Magnetic Beamforming for MIMO Wireless Power Transfer Optimization Motivation For the general MIMO setup, MultiSpot doesn\u0026rsquo;t consider peak current/voltage constraints. Yang et al. studied the magnetic beamforming problem in an MIMO MRC-WPT system to find all the boundary points of the multi-user power region with the peak current and voltage constraints for each TX. They simplified the problem by ignoring the mutual inductance among RXs. These researches lack the ability to control the power distribution among receivers, and could not be applied to the scenario when different RXs have different requirements of power which can be denoted as weight factors. Contribution We formulate the requirement-driven magnetic beamforming design in the MIMO MRC-WPT system as a weighted sum-power maximization problem. We consider the peak current/voltage constraints, and provides a high efficient, provable optimal solution.\nWe discuss the WSPMax problem with limited power budget where the peak current/voltage constraints are ignorable. We introduce a close-form theoretical bound of the WSPMax problem, and demonstrate that the power transfer efficiency maximization problem can be solved through TX-only solution without any feedback from the RXs, despite the fact that the beamforming is impacted by the interactions between TXs and RXs.\nMethods Proof of $ \\phi_s = - \\lambda_{Y,s} $\n$ \\phi_s = \\frac{\\vec{x_s^{\\ast}} X_2 \\vec{x_s}}{\\vec{x_s^{\\ast}} X_1 \\vec{x_s}} $, $ \\vec{Y} = - X_1^{-1} X_2 $, $ \\lambda_s \\vec{x_s} = \\vec{Y} \\vec{x_s} $.\n$$ \\begin{aligned} \\phi_s \u0026amp;= \\frac{\\vec{x_s^{\\ast}} X_2 \\vec{x_s}}{\\vec{x_ s^{\\ast}} X_1 \\vec{x_s}} \\\\ \u0026amp;= \\frac{\\lambda_s \\vec{x_s^{\\ast}} X_2 \\vec{x_s}}{\\vec{x_s^{\\ast}} X_1 \\lambda_s \\vec{x_s}} \\\\ \u0026amp;= \\frac{\\lambda_s \\vec{x_s^{\\ast}} X_2 \\vec{x_s}}{\\vec{x_s^{\\ast}} X_1 \\vec{Y} \\vec{x_s}} \\\\ \u0026amp;= \\frac{\\lambda_s \\vec{x_s^{\\ast}} X_2 \\vec{x_s}}{\\vec{x_s^{\\ast}} X_1 (- X_1^{-1} X_2) \\vec{x_s}} \\\\ \u0026amp;= -\\lambda_s \\end{aligned} $$\nProof of $ \\lambda_{\\ddot{Y}, s} = - \\frac{\\lambda_{\\ddot{Z}, s}}{\\lambda_{\\ddot{Z}, s} + 1}$\n$ \\lambda_{\\ddot{Y}, s} \\vec{x_s} = \\vec{\\ddot{Y}} \\vec{x_s} $, $ \\vec{\\ddot{Y}} = - (R_T + H^* R_R H)^{−1} (H^* R_R H) $, $ \\lambda_{\\ddot{Z}, s} \\vec{x_s} = \\vec{\\ddot{Z}} \\vec{x_s} $.\n$$ \\begin{aligned} \\lambda_{\\ddot{Y}, s} \\vec{x_s} = \\vec{\\ddot{Y}} \\vec{x_s} \\\\ \u0026amp;\\Rightarrow \\lambda_{\\ddot{Y}, s} \\vec{x_s} = - (R_T + H^* R_ RH)^{−1} (H^* R_R H) \\vec{x_s} \\\\ \u0026amp;\\Rightarrow \\lambda_{\\ddot{Y}, s} (R_T + H^* R_R H) \\vec{x_s} = - (H^* R_R H) \\vec{x_s} \\\\ \u0026amp;\\Rightarrow \\lambda_{\\ddot{Y}, s} R_T^{-1} (R_T + H^* R_R H) \\vec{x_s} = - R_T^{-1} (H^* R_R H) \\vec{x_s} \\\\ \u0026amp;\\Rightarrow \\lambda_{\\ddot{Y}, s} (I + \\ddot{Z}) \\vec{x_s} = - \\ddot{Z} \\vec{x_s} \\\\ \u0026amp;\\Rightarrow (\\lambda_{\\ddot{Y}, s} + 1) \\ddot{Z} \\vec{x_s} = - \\lambda_{\\ddot{Y}, s} \\vec{x_s} \\\\ \u0026amp;\\Rightarrow \\ddot{Z} \\vec{x_s} = - \\frac{\\lambda_{\\ddot{Y}, s}}{(\\lambda_{\\ddot{Y}, s} + 1)} \\vec{x_s} \\\\ \\end{aligned} $$\nHence, $ \\lambda_{\\ddot{Y}, s} = - \\frac{\\lambda_{\\ddot{Z}, s}}{\\lambda_{\\ddot{Z}, s} + 1}$.\nReflections Introduction When discussing studies involving multiple TXs and/or multiple RXs, the authors could explain the rationale behind using MIMO, such as the efficiency of beamforming, rather than simply listing the three types. Providing this context enhances the reader\u0026rsquo;s understanding of the advantages and practical implications of MIMO systems.\nMethods In solving the WSPMax problem with peak current/voltage constraints, the authors assumed that a controller exists for communication with all TXs/RXs. This provides a foundation for designing the communication system in the furture work.\nExperiments The merit of this study lies in the fact that the authors not only evaluated the performance of their algorithm but also verified its convergence properties.\nHowever, for the performance evaluation, the authors set $ w_q = 1 $ for each $ RX_q $ in a context unrelated to specific requirements. This approach might have been adopted due to the challenges associated with assigning different weights to different $ RX $ instances.\n","date":"December 13, 2024","hero":"/posts/wpt/wspmax/images/WPTscenario.gif","permalink":"https://qingbo12.github.io/posts/wpt/wspmax/","summary":"\u003ch2 id=\"requirement-driven-magnetic-beamforming-for-mimo-wireless-power-transfer-optimization\"\u003eRequirement-Driven Magnetic Beamforming for MIMO Wireless Power Transfer Optimization\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eFor the general MIMO setup, MultiSpot \u003cstrong\u003edoesn\u0026rsquo;t consider peak current/voltage constraints\u003c/strong\u003e. Yang et al. studied the magnetic beamforming problem in an MIMO MRC-WPT system to find all the boundary points of the multi-user power region with the peak current and voltage constraints for each TX. They simplified the problem by \u003cstrong\u003eignoring the mutual inductance among RXs\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eThese researches \u003cstrong\u003elack the ability to control the power distribution among receivers\u003c/strong\u003e, and could not be applied to the scenario when different RXs have different requirements of power which can be denoted as weight factors.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eWe \u003cstrong\u003eformulate the requirement-driven magnetic beamforming design in the MIMO MRC-WPT system as a weighted sum-power maximization problem\u003c/strong\u003e. We \u003cstrong\u003econsider the peak current/voltage constraints\u003c/strong\u003e, and provides a high efficient, provable optimal solution.\u003c/p\u003e","tags":null,"title":"WSPMax Read"},{"categories":null,"contents":"Lagrangian relaxation The explanation I provide is based on Lagrangian relaxation-Wikipedia and Chapter 12 Lagrangian Relaxation-ens-lyon\nLagrangian relaxation is a relaxation method which approximates a difficult problem of constrained optimization by a simpler problem.\nThe method penalizes violations of inequality constraints using a Lagrange multiplier, which imposes a cost on violations. These added costs are used instead of the strict inequality constraints in the optimization. In practice, this relaxed problem can often be solved more easily than the original problem.\nMathematical description\nSuppose we are given a linear programming problem, with $ x \\in R^n $ and $ A \\in R^{m \\times n} $, of the following form:\n$$ \\begin{aligned} \\text{max} \\quad \u0026amp; c^{\\top} x \\\\ \\text{s.t.} \\quad \u0026amp; (1) A x \\leq b \\end{aligned} $$\nWe may introduce the constraint (2) into the objective:\n$$ \\begin{aligned} \\text{max} \\quad \u0026amp; c^{\\top} x + \\lambda^{\\top} (b - A x) \\end{aligned} $$\nIf we let $ \\lambda = (\\lambda _{1},\\ldots ,\\lambda {m{2}}) $ be nonnegative weights, we get penalized if we violate the constraint (1), and we are also rewarded if we satisfy the constraint strictly. The above system is called the Lagrangian relaxation of our original problem.\nThe LR solution as a bound\nOf particular use is the property that for any fixed set of $ \\vec{\\lambda} \\succeq \\vec{0} $, values, the optimal result to the Lagrangian relaxation problem will be no smaller than the optimal result to the original problem. To see this, let $ \\hat{x} $ be the optimal solution to the original problem, and let $ \\bar{x} $ be the optimal solution to the Lagrangian relaxation. We can then see that\n$$ c^{\\top} \\hat{x} \\leq c^{\\top} \\hat{x} + \\vec{\\lambda}^{\\top} (b - A \\hat {x}) \\leq c^{\\top} \\bar{x} + \\vec{\\lambda}^{\\top} (b - A \\bar{x}) $$\nThe first inequality is true because $ \\hat {x} $ is feasible in the original problem and the second inequality is true because $ \\bar {x} $ is the optimal solution to the Lagrangian relaxation.\nIterating towards a solution of the original problem\nThe above inequality tells us that if we minimize the maximum value we obtain from the relaxed problem, we obtain a tighter limit on the objective value of our original problem. Namely, if we find the minimum $ c^{\\top} \\bar{x} + \\vec{\\lambda}^{\\top} (b - A \\bar{x}) $, according to the above inequality, we have $ \\hat{x} = \\bar{x} $, and the optimal value for the original problem is found. Thus we can address the original problem by instead exploring the partially dualized problem\n$$ \\text{min} \\quad P(\\lambda) \\quad \\text{s.t.} \\quad \\lambda \\geq 0 $$\nwhere we define $ P(\\lambda) $ as\n$$ \\begin{aligned} \\text{max} \\quad \u0026amp; c^{\\top} x + \\lambda^{\\top} (b - A x) \\end{aligned} $$\nA Lagrangian relaxation algorithm thus proceeds to explore the range of feasible $ \\lambda $ values while seeking to minimize the result returned by the inner $ P $ problem. Each value returned by $ P $ is a candidate upper bound to the problem, the smallest of which is kept as the best upper bound. If we additionally employ a heuristic, probably seeded by the $ \\bar {x} $ values returned by $ P $, to find feasible solutions to the original problem, then we can iterate until the best upper bound and the cost of the best feasible solution converge to a desired tolerance.\nGradient descent method\nSet $ k = 0 $ and choose $ \\lambda_0 \\in R^n $; Compute $ P(\\lambda_k) $ and a vector $ x_k \\in X $ where it is achieved; Calculate gradient $ g_k = b - A x_k $ of the function $ P $ at $ \\lambda_k $; If $ g_k = 0 $, then stop, the optimal solution is $ P(\\lambda_k) $ Compute $ \\lambda_{k+1} = max(0, \\lambda_k +\\theta^T_k g_k) $ where $ \\theta_k $ is the stepsize at this step. Increment $ k $ and go to Step 2. Lagrange multiplier The explanation I provide is based on Lagrange multiplier-Wikipedia and Lagrange multipliers intro | Constrained optimization (article)-Khan Academy\nIn mathematical optimization, the method of Lagrange multipliers is a strategy for finding the local maxima and minima of a function subject to equation constraints.\nStatement\nLet $ f: \\mathbb{R}^n \\to \\mathbb{R} $ be the objective function, $ g: \\mathbb{R}^n \\to \\mathbb{R}^c $ be the constraints function, both belonging to $ C^1 $ (i.e., having continuous first derivatives). Our optimization problem is:\n$$ \\begin{aligned} \\text{max} \\quad \u0026amp; f(x) \\\\ \\text{s.t.} \\quad \u0026amp; g(x) = c, \\end{aligned} $$\nwhere $ c \\in \\mathbb{R}^c $.\nMethod\nIntroduce the Lagrangian\nDefine a new function, the Lagrangian, by introducing a variable, known as the Lagrange multiplier. The Lagrangian function is given by: $$ \\mathcal{L}(\\lambda, x) = f(x) + \\lambda^{\\top} (g(x) - c) $$\nFind Critical Points\nSet the gradient of $ \\mathcal{L} $ with respect to both $ \\lambda $ and $ x $ equal to the zero vector: $$ \\nabla {\\mathcal{L}} (\\lambda, x) = \\vec{0} $$ This yields the system of equations: $$ \\begin{aligned} \\nabla_x \\mathcal{L}(\\lambda, x) \u0026amp;= \\nabla f(x) + \\nabla g(x)^{\\top} \\lambda = \\vec{0}, \\\\ \\nabla_{\\lambda} \\mathcal{L}(\\lambda, x) \u0026amp;= g(x) - c = \\vec{0}. \\end{aligned} $$\nEvaluate Solutions\nSolve the system to find the critical points, denoted $ (x_0, \\lambda_0) $. Evaluate $ f(x) $ at each solution $ x_0 $. The solution corresponding to the highest value of $ f(x) $ is the maximum, and the one corresponding to the lowest value is the minimum (if required).\nSingle constraint\nFigure 1: The red curve shows the constraint $ g(x, y) = c $. The blue curves are contours of $ f(x, y) $. The point where the red constraint tangentially touches a blue contour is the maximum of $ f(x, y) $ along the constraint, since $ d1 \u003e d2 $. For the case of only one constraint and only two choice variables (as exemplified in Figure 1), consider the optimization problem\n$$ \\begin{aligned} \\underset{x,y}{\\text{maximize}} \\quad\u0026amp; f(x,y) \\ \\text{subject to}\\quad\u0026amp; g(x,y) = c. \\end{aligned} $$\nWe assume that both $ f $ and $ g $ have continuous first partial derivatives.\nThe method of Lagrange multipliers relies on the intuition that at a maximum, $ f(x, y) $ cannot be increasing in the direction of any such neighboring point that also has $ g(x, y) = 0 $. If it were, we could walk along $ g = 0 $ to get higher, meaning that the starting point wasn\u0026rsquo;t actually the maximum. Viewed in this way, it is an exact analogue to testing if the derivative of an unconstrained function is $ 0 $, that is, we are verifying that the directional derivative is 0 in any relevant (viable) direction.\nWe can visualize contours of $ f $ given by $ f(x, y) = d $ for various values of $ d $, and the contour of $ g $ given by $ g(x, y) = c $.\nSuppose we walk along the contour line with $ g = c $. We are interested in finding points where $ f $ almost does not change as we walk, since these points might be maxima.\nThere are two ways this could happen:\nWe could touch a contour line of $ f $ (walk along the contour line with $ g = c $ and a contour line of $ f $ simultaneously), since by definition $ f $ does not change as we walk along its contour lines. This would mean that the tangents to the contour lines of $ f $ and $ g $ are parallel here. We have reached a \u0026ldquo;level\u0026rdquo; part of $ f $, meaning that $ f $ does not change in any direction. To check the first possibility (we touch a contour line of $ f $), notice that since the gradient of a function is perpendicular to the contour lines, the tangents to the contour lines of $ f $ and $ g $ are parallel if and only if the gradients of $ f $ and $ g $ are parallel. Thus we want points $ (x, y) $ where $ g(x, y) = c $ and\n$$ \\nabla_{x,y} f = \\lambda \\nabla_{x,y} g, $$\nfor some $ \\lambda $\nwhere\n$$ \\nabla_{x,y} f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right), \\qquad \\nabla_{x,y} g = \\left( \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right) $$\nare the respective gradients. The constant $ \\lambda $ is required because although the two gradient vectors are parallel, the magnitudes of the gradient vectors are generally not equal. This constant is called the Lagrange multiplier. (In some conventions $ \\lambda $ is preceded by a minus sign).\nNotice that this method also solves the second possibility, that $ f $ is level: if $ f $ is level, then its gradient is zero, and setting $ \\lambda = 0 $ is a solution regardless of $ \\nabla_{x,y} g $.\nTo incorporate these conditions into one equation, we introduce an auxiliary function\n$$ \\mathcal{L}(x,y,\\lambda) \\equiv f(x,y) + \\lambda \\cdot g(x,y), ,$$\nand solve\n$$ \\nabla_{x,y,\\lambda} \\mathcal{L}(x, y, \\lambda) = 0 ~.$$\nNote that this amounts to solving three equations in three unknowns. This is the method of Lagrange multipliers.\nNote that $ \\nabla_{\\lambda} \\mathcal{L}(x, y, \\lambda) = 0 $ implies $ g(x,y) = 0 $, as the partial derivative of $ \\mathcal{L} $ with respect to $ \\lambda $ is $ g(x,y) ~. $\nTo summarize\n$$ \\nabla_{x,y,\\lambda} \\mathcal{L}(x, y, \\lambda) = 0 \\iff \\begin{cases} \\nabla_{x,y} f(x , y) = -\\lambda , \\nabla_{x,y} g(x , y) \\\\ g(x,y) = 0 \\end{cases} $$\nThe method generalizes readily to functions on $ n $ variables\n$$ \\nabla_{x_1, \\dots, x_n,\\lambda} \\mathcal{L}(x_1, \\dots, x_n, \\lambda) = 0 $$\nwhich amounts to solving $ n + 1 $ equations in $ n + 1 $ unknowns.\nThe constrained extrema of $ f $ are critical point of the Lagrangian $ \\mathcal{L} $, but they are not necessarily local extrema of $ \\mathcal{L} $.\nMultiple constraints\nFigure 2: A paraboloid constrained along two intersecting lines. Figure 3: Contour map of Figure 2. The method of Lagrange multipliers can be extended to solve problems with multiple constraints using a similar argument. Consider a paraboloid subject to two line constraints that intersect at a single point. As the only feasible solution, this point is obviously a constrained extremum. However, the level set of $ f $ is clearly not parallel to either constraint at the intersection point (see Figure 3); instead, it is a linear combination of the two constraints\u0026rsquo; gradients. In the case of multiple constraints, that will be what we seek in general: The method of Lagrange seeks points not at which the gradient of $ f $ is a multiple of any single constraint\u0026rsquo;s gradient necessarily, but in which it is a linear combination of all the constraints\u0026rsquo; gradients.\nConcretely, suppose we have $ M $ constraints and are walking along the set of points satisfying $ g_i(\\mathbf{x}) = 0, i=1, \\dots, M ,.$ Every point $ \\mathbf{x} $ on the contour of a given constraint function $g_i$ has a space of allowable directions: the space of vectors perpendicular to $ \\nabla g_i(\\mathbf{x}) , .$ The set of directions that are allowed by all constraints is thus the space of directions perpendicular to all of the constraints\u0026rsquo; gradients. Denote this space of allowable moves by $\\ A\\ $ and denote the span of the constraints\u0026rsquo; gradients by $ S ,.$ Then $ A = S^{\\perp}, ,$ the space of vectors perpendicular to every element of $ S ,.$\nWe are still interested in finding points where $ f $ does not change as we walk, since these points might be (constrained) extrema. We therefore seek $ \\mathbf{x} $ such that any allowable direction of movement away from $\\mathbf{x}$ is perpendicular to $ \\nabla f(\\mathbf{x}) $ (otherwise we could increase $f$ by moving along that allowable direction). In other words, $ \\nabla f(\\mathbf{x}) \\in A^{\\perp} = S ,.$ Thus there are scalars $ \\lambda_1, \\lambda_2,\\ \\dots, \\lambda_M $ such that\n$$ \\nabla f(\\mathbf{x}) = \\sum_{k=1}^M \\lambda_k , \\nabla g_k (\\mathbf{x}) \\quad \\iff \\quad \\nabla f(\\mathbf{x}) - \\sum_{k=1}^M {\\lambda_k \\nabla g_k (\\mathbf{x})} = 0 ~. $$\nThese scalars are the Lagrange multipliers. We now have $ M $ of them, one for every constraint.\nAs before, we introduce an auxiliary function\n$$ \\mathcal{L} (x_1, \\ldots, x_n, \\lambda_1, \\ldots, \\lambda_M) = f (x_1, \\ldots, x_n) - \\sum\\limits_{k=1}^M {\\lambda_k g_k ( x_1, \\ldots , x_n )} $$\nand solve\n$$ \\nabla_{x_1, \\ldots , x_n, \\lambda_1, \\ldots, \\lambda_M} \\mathcal{L} (x_1, \\ldots, x_n, \\lambda_1, \\ldots, \\lambda_M) = 0 \\iff \\begin{cases} \\nabla f(\\mathbf{x}) - \\sum_{k=1}^M {\\lambda_k , \\nabla g_k (\\mathbf{x})} = 0 \\\\ g_1(\\mathbf{x}) = \\cdots = g_M(\\mathbf{x}) = 0 \\end{cases} $$\nwhich amounts to solving $ n+M $ equations in $ n+M $ unknowns.\nThe constraint qualification assumption when there are multiple constraints is that the constraint gradients at the relevant point are linearly independent.\n","date":"December 12, 2024","hero":"/posts/miscellanea/optimization/images/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/optimization/","summary":"\u003ch3 id=\"lagrangian-relaxation\"\u003eLagrangian relaxation\u003c/h3\u003e\n\u003cp\u003eThe explanation I provide is based on \u003ca href=\"https://en.wikipedia.org/wiki/Lagrangian_relaxation\" target=\"_blank\" rel=\"noopener\"\u003eLagrangian relaxation-Wikipedia\u003c/a\u003e and \u003ca href=\"https://www.ens-lyon.fr/DI/wp-content/uploads/2012/01/LagrangianRelax.pdf\" target=\"_blank\" rel=\"noopener\"\u003eChapter 12 Lagrangian Relaxation-ens-lyon\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eLagrangian relaxation is a relaxation method which approximates a\ndifficult problem of constrained optimization by a simpler problem.\u003c/p\u003e\n\u003cp\u003eThe method penalizes violations of inequality constraints using a Lagrange\nmultiplier, which imposes a cost on violations. These added costs are used\ninstead of the strict inequality constraints in the optimization. In\npractice, this relaxed problem can often be solved more easily than the\noriginal problem.\u003c/p\u003e","tags":null,"title":"Optimization Methods"},{"categories":null,"contents":"Issues 1. Built-in Shortcode leads to Hugo timeout.\nBecause of the networks, I can\u0026rsquo;t send requests to Twitter, YouTube, etc. So I delete the contents about Twitter and YouTube. They are in content\\posts\\category\\sub-category\\rich-content\\index.md, content\\posts\\category\\sub-category\\rich-content\\index.bn.md, hugo.yaml shareButtons:, data\\en\\sections\\about.yaml socialLinks:.\n2. Only readme when deploying to github pages.\nNeed to set your deployment branch to gh-pages.\n","date":"November 24, 2024","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/posts/building-issues/","summary":"\u003ch3 id=\"issues\"\u003eIssues\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e1. Built-in Shortcode leads to Hugo timeout.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eBecause of the networks, I can\u0026rsquo;t send requests to Twitter, YouTube, etc. So I delete the contents about Twitter and YouTube. They are in content\\posts\\category\\sub-category\\rich-content\\index.md, content\\posts\\category\\sub-category\\rich-content\\index.bn.md,\nhugo.yaml shareButtons:,\ndata\\en\\sections\\about.yaml socialLinks:.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2. Only readme when deploying to github pages.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eNeed to set your deployment branch to gh-pages.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/posts/building-issues/deployBranchSetting.png\" alt=\"deployBranchSetting\"\u003e\u003c/p\u003e","tags":null,"title":"Building issues"},{"categories":null,"contents":"0.1 Determinant Theorem: $ det(AB) = det(A)det(B) $ Let $ A, B $ be a square matrices of order n.\nLet $ det(A) $ be the determinant of $ A $.\nLet $ AB $ be the matrix product of $ A $ and $ B $.\nThen:\n$ det(AB) = det(A)det(B) $\nProof\nThe proof I provide is based on Determinant of Matrix Product\nConsider two cases:\n$ A $ is singular.\n$ A $ is nonsingular.\nproof of case1\nAssume $ A $ is singular.\nThen:\n$$ det(A) = 0 $$\nAlso if A is singular then so is AB.\nIndeed, if $ AB $ has an inverse $ C $, then:\n$$ ABC = E $$\nwhereby $ BC $ is a right inverse of $ A $.\nIt follows by Left or Right Inverse of Matrix is Inverse that in that case $ BC $ is the inverse of $ A $. This contradicts the assumption.\nIt follows that:\n$$ det(AB)=0 $$\nThus:\n$$ 0 = 0 \\times det(B) $$\nthat is:\n$$ det(AB) = 0 = det(A)det(B) $$\nproof of case2\nAssume $ A $ is nonsingular.\nThen $ A $ is a product of elementary row matrices, $E $.\nLet $ A=E_kE_{k−1}⋯E_1 $.\nSo:\n$$ det(AB) = det(E_kE_{k−1}⋯E_1B) $$\nIt remains to be shown that for any square matrix $ D $ of order n:\n$$ det(ED) = det(E) det(D) $$\n$ det(ED) = -|D| = det(E) det(D) $ for swap 2 rows\n$ det(ED) = k|D| = det(E) det(D) $ for apply k to 1 row\n$ det(ED) = |D| = det(E) det(D) $ for row(i) + k x row(j)\nThen:\n$ det(AB) = det(E_kE_{k−1}⋯E_1B) $\n$ det(AB) = det(E_k) det(E_{k−1}⋯E_1(B)) $\n$ det(AB) = \\alpha det(B) $\nand:\n$ det(A) = det(E_kE_{k−1}⋯E_1I) $\n$ = det(E_kE_{k−1}⋯E_1(I)) $\n$ = \\alpha det(I) $\nTherefore:\n$ det(AB)=det(A)det(B) $\n0.2 Real Symmetric Matrix Theorem: all real eigenvalues If $ A $ is a (real) $ n \\times n $ symmetric matrix, then $ A $ has n real eigenvalues (counted bytheir multiplicities). For each eigenvalue, we can find a real eigenvector associated with it.\nProof\nThe proof I provide is based on Orthogonally Diagonalizable Matrices\nTheorem: orthogonally diagonalizable A real symmetric matrix must be orthogonally diagonalizable.\nProof\nThe proof I provide is based on Orthogonally Diagonalizable Matrices\nThis is a proof by induction, and it uses some simple facts about partitioned matrices and change of coordinates.\nThis is obviously true for every $ 1 \\times 1 $ matrix $ A $ if $ A = [a] $, then $ E_1^{-1} (a) E_1 = (a) $\nAssume now that every $ (n - 1) \\times (n - 1) $ real symmetric matrix is orthogonally diagonalizable.\nConsider an $ n \\times n $ real symmetric matrix $ A $ where $ n \\gt 1 $. By the preceding theorem, we can find a real eigenvalue $ \\lambda $ of $ A $, together with a real eigenvector $ v_1 $. By normalizing, we can assume $ v_1 $ is a unit eigenvector. Add vectors to extend $ (v_1) $ to a basis for $ R^n $ and then use the Gram Schmidt process to get an orthonormal basis for $ R^n $: $ (v_1, v_2, \u0026hellip;, v_n) $.\nLet\n$ T_1 = (v_1, v_2, \u0026hellip;, v_n) $.\n$ T_1 $ is orthogonal.\nNow look that\n$$ T_1^{-1} A T_1 = T_1^{-1} (A v_1, A v_2, \u0026hellip;, A v_n) $$\n$$ T_1^{-1} A T_1 = (T_1^{-1} \\lambda_1 v_1, T_1^{-1} A v_2, \u0026hellip;, T_1^{-1} A v_n) $$\nBecause $ T_1^{-1} T_1 = E $, we have\n$$ T_1^{-1} (v_1, v_2, \u0026hellip;, v_n) = (\\epsilon_1, \\epsilon_2, \u0026hellip;, \\epsilon_n) $$\nThus, $ T_1^{-1} v_1 = \\epsilon_1 $, So column 1 of $ T_1^{-1} A T_1 $ is $ \\lambda_1 \\epsilon_1 $. Suppose now that\n$$ T_1^{-1} A T_1 = \\begin{pmatrix} \\lambda_1 \u0026amp; \\alpha \\\\ \\mathbf{0} \u0026amp; \\beta \\end{pmatrix} $$\n$ T_1^{-1} A T_1 $ is symmetric, because\n$$ (T_1^{-1} A T_1)^{\\top} = (T_1^{\\top} A T_1)^{\\top} = T_1^{\\top} A^{\\top} T_1^{\\top \\top} = T_1^{\\top} A T_1 = T_1^{-1} A T_1 $$\nSo $ \\alpha = \\mathbf{0} $, and $ \\beta $ is a real symmetric matrix. By the induction hypothesis, $ \\beta $ is diagonalizable. Thus, we has $ T_2 $ such that\n$$ T_2^{-1} \\beta T_2 = diag\\{\\lambda_2, \\lambda_3, \u0026hellip;, \\lambda_n\\} $$\nLet\n$$ T = T_1 \\begin{pmatrix} 1 \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; T_2 \\end{pmatrix} $$\n$$ T^{-1} A T = \\begin{pmatrix} 1 \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; T_2 \\end{pmatrix}^{-1} T_1^{-1} A T_1 \\begin{pmatrix} 1 \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; T_2 \\end{pmatrix} $$\n$$ T^{-1} A T = \\begin{pmatrix} 1 \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; T_2^{-1} \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; \\beta \\end{pmatrix} \\begin{pmatrix} 1 \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; T_2 \\end{pmatrix} $$\n$$ T^{-1} A T = \\begin{pmatrix} \\lambda_1 \u0026amp; \\mathbf{0} \\\\ \\mathbf{0} \u0026amp; T_2^{-1} \\beta T_2 \\end{pmatrix} $$\n$$ T^{-1} A T = diag\\{\\lambda_1, \\lambda_2, \u0026hellip;, \\lambda_n\\} $$\nTherefore, a real symmetric matrix must be orthogonally diagonalizable.\nTheorem: r-fold root, $ A - \\lambda E $ has a rank of $ n - r $ If $ A $ is a real symmetric matrix of order n. $ \\lambda $ is an r-fold root of its characteristic polynomial\nThen the matrix $ A - \\lambda E $ has a rank of $ n - r $. This implies that there are exactly $ r $ linearly independent eigenvectors associated with the eigenvalue $ \\lambda $.\nProof\nThe proof I provide is based on Yiwen\u0026rsquo;s Zhihu Answer\nSuppose that $ A\u0026rsquo;s $ r -fold root is $ \\lambda_k $.\nBecause $ A $ is real symmetric matrix, so we have orthogonal matrix $ P $ such that:\n$$ P^{-1} A P = \\begin{pmatrix} \\lambda_1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda_2 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\blue{\\lambda_{k}} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\blue{\\lambda_{k}} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\blue{\\lambda_{k}} \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\blue{\\lambda_{k}} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\lambda_n \\end{pmatrix} $$\nWe don\u0026rsquo;t care whether $ \\lambda_i $ is 0 or not.\nFor matrix $ A - \\lambda_k E $, we have:\n$$ P^{-1} (A - \\lambda_k E) P = P^{-1} A P - \\lambda_k P^{-1} E P = $$\n$$ \\varLambda - \\lambda_k E = \\begin{pmatrix} \\lambda_1 - \\blue{\\lambda_{k}} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; \\lambda_2 - \\blue{\\lambda_{k}} \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\blue{0} \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\blue{0} \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\blue{0} \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\blue{0} \u0026amp; \\cdots \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\cdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\lambda_n - \\blue{\\lambda_{k}} \\end{pmatrix} $$\n$\\because \\lambda_i \\neq \\lambda_k \\Rightarrow \\lambda_i - \\lambda_k \\neq 0, (i \\neq k) $\n$ \\therefore rank(A - \\lambda_k E) = rank(\\varLambda - \\lambda_k E) = n - r $\n0.3 Vector spaces $ A \\in M_{m,n}(\\mathbf{F}) $ as a linear transformation $ x \\rightarrow Ax $ from $ F^n $ to $ F^m $. The domain of this linear transformation is $ F^n $; its range is $ range A = \\{y \\in F^m : y =Ax \\} $ for some $ x \\in F^n $; its null space is $ nullspace A = {x \\in F^n : Ax = 0} $. The range of $ A $ is a subspace of $ F^m $, and the null space of $ A $ is a subspace of $ F^n $. The dimension of $ nullspace A $ is denoted by $ nullity A $; the dimension of $ range A $ is denoted by $ rank A $.\nTo avoid ambiguities,\nThe range of a linear transformation can be denoted as $ \\operatorname{range} A $, $ \\operatorname{im} A $.\nThe dimension of the range of a linear transformation can be denoted as $ \\operatorname{rank} A $.\nThe null space of a linear transformation can be denoted as $ \\operatorname{nullspace} A $, $ \\operatorname{ker} A $.\nThe dimension of the null space of a linear transformation can be denoted as $ \\operatorname{nullity} A $.\nThe dimension of $ \\operatorname{range} A $ is denoted by $ \\operatorname{rank} A $ The proof I provide is based on rank-of-matrix-equals-dimension-of-range\nSuppose that we are given that the $ A \\in M_{m, n}(\\mathbf{F}) $ has column-rank (and therefore coincident row-rank) $ r $. That is, the maximal set of linearly independent columns of $ A $ contains $ r $ vectors. It follows that the span of the columns of $ A $, henceforth the column space of $ A $, is an r-dimensional subspace of $ R^m $.\nEach column of $ A $ is a vector with $ m $ entries, so the columns live in $ F^m $. If $ n \\gt m $, These columns must be dependent, so their dimension $ r \\leq m $. Hence, the column space of $ A $ is a subspace of $ F^m $.\nLet $ a_1, \\cdots, a_n $ denote the columns of $ A $. Consider an arbitrary element $ y $ inside the column space of $ A $. By definition, this mean that there exist coefficients $ x_1, \\cdots, x_n $ such that\n$$ y = a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n $$\nNote that we can rewrite the above sum as a product. In particular, we have\n$$ y = \\begin{pmatrix} a_1 \u0026amp; a_2 \u0026amp; \\cdots \u0026amp; a_n \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} = A x $$\nwhere $ x = (x_1, x_2, \\cdots, x_n)^{\\top} \\in R^n $. So, any element $ y $ from the column space of $ A $ is also an element of the range of $ A $.\nSo the dimension of $ range A $ is denoted by $ rank A $.\nRank-nullity theorem Matrices For $ A \\in M_{m, n}(\\mathbf{F}) $, we have:\n$$ rank(A) + nullity(A) = n $$\nProof\nThe proof I provide is based on The Rank-Nullity Theorem.\nIf $ rank(A) = n $, then by the Invertible Matrix Theorem, the only solution to $ Ax = 0 $ is the trivial solution $ x = 0 $. Hence, in this case, $ nullspace(A) = {0} $, so $ nullity(A) = 0 $ and Equation holds.\nNow suppose $ rank(A) = r \\lt n $. In this case, there are $ n − r \u0026gt; 0 $ free variables in the solution to $ Ax = 0 $. Let $ t_{1}, t_{2},\u0026hellip;,t_{n−r} $ denote these free variables (chosen as those variables not attached to a leading one in any row-echelon form of $ A $), and let $ x_1, x_2,\u0026hellip;, x_{n−r} $ denote the solutions obtained by sequentially setting each free variable to 1 and the remaining free variables to zero. Note that $ \\{ x_1, x_2,\u0026hellip;, x_{n−r} \\} $ is linearly independent. Moreover, every solution to $ Ax = 0 $ is a linear combination of $ x_1, x_2,\u0026hellip;, x_{n−r} $:\n$$ x = t_1x_1 + t_2x_2 +···+ t_{n−r}x_{n−r}, $$\nwhich shows that $ \\{ x_1, x_2,\u0026hellip;, x_{n−r} \\} $ spans $nullspace(A) $. Thus, $ \\{ x_1, x_2,\u0026hellip;, x_{n−r} \\} $ is a basis for nullspace(A), so $ nullity(A) = n − r $ and Equation holds.\nLinear transformations Let $ T:V \\rightarrow W $ be a linear transformation between two vector spaces where $ T $ \u0026rsquo;s domain $ V $ is finite dimensional. Then\n$$ rank(T) + nullity(T) = dim V $$\nProof\nThe proof I provide is based on Rank–nullity theorem.\nLet $ V, W $ be vector spaces over some field $ F $, and $ T $ defined as in the statement of the theorem with $ \\dim V = n $.\nAs $ \\operatorname{Ker}T \\subset V $ is a subspace, there exists a basis for it. Suppose $ \\dim\\operatorname{Ker}T = k $ and let $ \\mathcal{K} := \\{v_1, \\ldots, v_k\\} \\subset \\operatorname{Ker}(T) $ be such a basis.\nWe may now, by the Steinitz exchange lemma, extend $ \\mathcal{K} $ with $ n-k $ linearly independent vectors $ w_1, \\ldots, w_{n-k} $ to form a full basis of $ V $.\nLet\n$ \\mathcal{S} := \\{w_1, \\ldots, w_{n-k}\\} \\subset V \\setminus \\operatorname{Ker}(T) $\nsuch that\n$ \\mathcal{B} := \\mathcal{K} \\cup \\mathcal{S} = \\{v_1, \\ldots, v_k, w_1, \\ldots, w_{n-k}\\} \\subset V $\nis a basis for $ V $. From this, we know that\n$$ \\operatorname{Im} T = \\operatorname{Span}T(\\mathcal{B}) = \\operatorname{Span}\\{T(v_1), \\ldots, T(v_k), T(w_1), \\ldots, T(w_{n-k})\\} = \\operatorname{Span}{T(w_1), \\ldots, T(w_{n-k})} = \\operatorname{Span}T(\\mathcal{S}). $$\nWe now claim that $ T(\\mathcal{S}) $ is a basis for $ \\operatorname{Im} T $. The above equality already states that $ T(\\mathcal{S}) $ is a generating set for $ \\operatorname{Im} T $; it remains to be shown that it is also linearly independent to conclude that it is a basis.\nSuppose $ T(\\mathcal{S}) $ is not linearly independent, and let\n$ \\sum_{j=1}^{n-k} \\alpha _j T(w_j) = 0_W $\nfor some $ \\alpha _j \\in F $.\nThus, owing to the linearity of $ T $, it follows that\n$$ T \\left(\\sum_{j=1}^{n-k} \\alpha_j w_j \\right) = 0_W \\implies \\left(\\sum_{j=1}^{n-k} \\alpha_j w_j \\right) \\in \\operatorname{Ker} T = \\operatorname{Span} \\mathcal{K} \\subset V. $$\nThis is a contradiction to $ \\mathcal{B} $ being a basis, unless all $ \\alpha _j $ are equal to zero. This shows that $ T(\\mathcal{S}) $ is linearly independent, and more specifically that it is a basis for $ \\operatorname{Im}T $.\nTo summarize, we have $ \\mathcal{K} $, a basis for $ \\operatorname{Ker}T $, and $ T(\\mathcal{S}) $, a basis for $ \\operatorname{Im}T $.\nFinally we may state that\n$$ \\operatorname{Rank}(T) + \\operatorname{Nullity}(T) = \\dim \\operatorname{Im} T + \\dim \\operatorname{Ker}T = |T(\\mathcal{S})| + |\\mathcal{K}| = (n-k) + k = n = \\dim V . $$\nThis concludes our proof.\nApplication: $ \\dim C(AB) = \\dim C(B) - \\dim ( \\operatorname{nullspace} (A) \\cap C(B)) $ The application I provide is based on $ \\dim C(AB) = \\dim C(B) - \\dim ( \\operatorname{nullspace} (A) \\cap C(B)) $\nconsidering the map $ T: C(B) \\rightarrow C(AB) $ given by $ T(y) = Ay $, for all $ y \\in C(B) $.\nthe kernel of $ T $ is $ \\operatorname{nullspace} (A) \\cap C(B) $. Indeed, $ y \\in \\operatorname{Ker} (T) $ if and only if $ y \\in C(B) $ and $ A y = 0 $, if and only if $ y \\in \\operatorname{nullspace} (A) \\cap C(B) $.\nNow use the rank-nullity theorem, we have\n$$ \\operatorname{Rank}(T) + \\operatorname{Nullity}(T) = \\dim C(AB) + \\dim (\\operatorname{nullspace} (A) \\cap C(B)) = \\dim C(B) = \\dim V. $$\n$$ \\dim C(AB) = \\dim C(B) - \\dim (\\operatorname{nullspace} (A) \\cap C(B)) $$\n0.4 Rank Sylvester inequality If $ A \\in M_{m,k} (F) $ and $ B \\in M_{k,n} (F) $, then\n$$ (rank A + rank B) - k \\leq rank (AB) $$\nProof\nThe proof I provide is based on Prove Sylvester rank inequality.\nIf we prove that $ dim ker A + dim ker B \\geq dim ker (AB) $. By using the rank-nullity theorem, we can then deduce that $ (rank A + rank B) - k \\leq rank (AB) $.\nFirstly, we show that $ kerB \\subseteq ker(AB) $.\nAny $ x \\in kerB $, we have:\n$$ B x = 0 $$\nNow, applying the matrix $ A $ to both sides of this equation:\n$$ A B x = A 0 = 0 \\Rightarrow x \\in ker(AB) $$\nThus $ kerB \\subseteq ker(AB) $.\nThen we show that $ dim ker A + dim ker B \\geq dim ker (AB) $.\nLet $ \\beta = \\{\\alpha_1,…,\\alpha_r\\} $ be a basis for $ kerB $. Because $ kerB \\subseteq ker(AB) $, so we can extend $ \\beta $ to a basis for $ ker(AB) $. Suppose $ \\{\\alpha_1, \\cdots, \\alpha_r, \\alpha_{r+1}, \\cdots, \\alpha_n \\} $ be basis for $ ker(AB) $. Since $ \\alpha_{i + 1}, \\cdots, \\alpha_n \\notin kerB $, we have that $ B \\alpha_i \\neq 0 $ for $ i \\in \\{ r \\lt i \\lt n + 1 \\} $.\nWe show that $ \\{B \\alpha_{r+1}, \\cdots, B \\alpha_n \\} $ is linear independent. If we can show that, then we would have $ dim ker A \\geq n - r $. (Note: $ dim ker A \\neq dim ker (AB) $, Because $ B \\alpha_i = 0 $, for $ i \\in \\{ 1 \\leq i \\leq r \\} $, if we add $ B \\alpha_1 $ to $ \\{B \\alpha_{r+1}, \\cdots, B \\alpha_n \\} $, we would have $ \\{ B \\alpha_1, B \\alpha_{r+1}, \\cdots, B \\alpha_n \\} $ is linear dependent. So we could only say $ dim ker A \\geq n - r $.)\nAssume that there exist scalars $ \\lambda_1, \\cdots, \\lambda_n $, not all zero, such that $ \\sum_{i = r + 1}^n \\lambda_i B \\alpha_i = 0 $. Since $ B $ is linear, we have $ B \\sum_{i = r + 1}^n \\lambda_i \\alpha_i = 0 $, so that $ \\sum_{i = r + 1}^n \\lambda_i \\alpha_i $ belongs to the kernel of $ B $. On other hand, we already know that $ \\beta = \\{\\alpha_1,…,\\alpha_r\\} $ be a basis for $ kerB $. Next since the set $ \\{\\alpha_1, \\cdots, \\alpha_r, \\alpha_{r+1}, \\cdots, \\alpha_n \\} $ is an independent set, we infer that $ \\lambda_i $ must be zero for all $ i = r + 1, \\cdots, n $.\nNow one can see that\n$$ dim ker A + dim ker B \\geq n - r + r = n \\Rightarrow dim ker A + dim ker B \\geq dim ker (AB) $$\nUsing the rank-nullity theorem, we have\n$$ k - rank A + n - rank B \\geq n - rank (AB) \\Rightarrow (rank A + rank B) - k \\leq rank (AB) $$\nFrobenius inequality Let $ A \\in M_{m \\times k}(\\bf F) $, $ B \\in M_{k \\times p}(\\bf F) $ and $ C \\in M_{p \\times n}(\\bf F) $, then $ \\operatorname{rank}(AB) + \\operatorname{rank}(BC) \\leq \\operatorname{rank}(B) + \\operatorname{rank}(ABC) $\nProof\nThe proof I provide is based on Frobenius rank inequality.\nWe can use the rank-nullity theorem to show that\n$$ \\tag{1} \\operatorname{rank}(AB) = \\operatorname{rank}(B) - \\dim (\\operatorname{im}(B) \\cap \\operatorname{ker} (A)) $$\nSince $ \\operatorname{im}(BC) \\subseteq \\operatorname{im}(B) $, we have\n$$ \\tag{2} \\operatorname{im}(BC) \\cap \\operatorname{ker}(A) \\subseteq \\operatorname{im}(B) \\cap \\operatorname{ker}(A) $$\nNow we want to write $ \\operatorname{rank}(ABC) $ in such a way that $ \\operatorname{im}(BC) \\cap \\operatorname{ker}(A) $ pops up, so we could make use of (2). Analogously to (1):\n$$ \\tag{3} \\operatorname{rank} (ABC) = \\operatorname{rank} (BC) - \\dim (\\operatorname{im} (BC) \\cap \\operatorname{ker} (A)) $$\nFrom (1) and (3), we have\n$$ \\operatorname{rank}(AB) + \\operatorname{rank}(BC) = \\operatorname{rank}(B) + \\operatorname{rank}(ABC) + \\underbrace{ \\dim (\\operatorname{im} (BC) \\cap \\operatorname{ker} (A)) - \\dim (\\operatorname{im}(B) \\cap \\operatorname{ker} (A)) }_{\\leq 0 \\text{ due to (2)}} $$\nwhich implies the desired inequality.\nLeft or right multiplication by a nonsingular matrix leaves rank unchanged If $ A \\in M_m(\\bf F) $ and $ C \\in M_n(\\bf F) $ are nonsingular and $ B \\in M_{m,n}(\\bf F) $, then $ \\operatorname{rank} AB = \\operatorname{rank} B = \\operatorname{rank} BC = \\operatorname{rank} ABC $; that is, left or right multiplication by a nonsingular matrix leaves rank unchanged.\nProof\nFirstly, we show that $ \\operatorname{rank} (B) = \\operatorname{rank} (BC) $.\nThe proof is based on How to prove that $ \\operatorname{im}(B) = \\operatorname{im}(BA)$?.\nWe know that $ C $ is a linear transformation from $ \\bf F^n $ to $ \\operatorname{im} (C) $. Since $ C $ is nonsingular, it follows that $ C $ is onto, implying $ \\operatorname{im} (C) = \\bf F^n $.\nNow consider the linear transformation $ BC $. This composition can be viewed in two stages:\nThe transformation $ x \\rightarrow Cx $, where $ C: \\bf F^n \\rightarrow \\bf F^n $.\nThe transformation $ Cx \\rightarrow BCx $, where $ B: \\bf F^n $ to $ \\operatorname{im} (BC) $.\n$ Cx $ spans all of $ \\bf F^n $, which aligns with the domain of $ B $. Thus, the transformation $ Cx \\rightarrow BCx $ behaves exactly as $ B $, mapping $ \\bf F^n $ to $ \\operatorname{im} (B) $.\nHence, $ \\operatorname{im} (BC) = \\operatorname{im} (B) $.\nThis concludes our proof.\nThen we show that $ \\operatorname{rank} (AB) = \\operatorname{rank} (B) $.\nWe know that $ A $ is a linear transformation from $ \\bf F^m $ to $ \\operatorname{im} (A) $. Since $ A $ is nonsingular, it follows that $ A $ is onto, meaning $ \\operatorname{im} (A) = \\bf F^m $. Furthermore, because $ \\dim(\\operatorname{im}(A)) = \\dim(\\operatorname{domain}(A)) = m $, $ A $ is injective as well.\nNow consider the linear transformation $ AB $. This composition can be viewed in two stages:\nThe transformation $ x \\rightarrow Bx $, where $ B: \\bf F^n \\rightarrow \\operatorname{im} (B) $.\nThen $ Bx \\rightarrow ABx $, where $ A: \\operatorname{im} (B) $ to $ \\operatorname{im} (AB) $.\nSince A is injective, $ \\dim (\\operatorname{im} (AB)) = \\dim (\\operatorname{im} (B)) $.\nThis concludes our proof.\nIf $ A \\in M_{m,n}(\\bf C) $, then $ \\operatorname{rank} A^* A = \\operatorname{rank} A $ Proof\nThe proof I provide is based on Prove $\\operatorname{rank}A^TA=\\operatorname{rank}A$.\nLet $ x \\in \\operatorname{nullspace} (A) $.\n$$ A x = 0 $$ $$ \\Rightarrow A^* A x = 0 $$ $$ \\Rightarrow x \\in \\operatorname{nullspace} (A^* A) $$\nHence, $ \\operatorname{nullspace} (A) \\subseteq \\operatorname{nullspace} (A^* A) $.\nAgian let $ x \\in \\operatorname{nullspace} (A^* A) $.\n$$ A^* A x = 0 $$ $$ \\Rightarrow x^* A^* A x = 0 $$ $$ \\Rightarrow (A x)^* A x = 0 $$ $$ \\Rightarrow A x = 0 $$ $$ \\Rightarrow x \\in \\operatorname{nullspace} (A) $$\nHence, $ \\operatorname{nullspace} (A^* A) \\subseteq \\operatorname{nullspace} (A) $.\nTherefore, $$ \\operatorname{nullspace} (A) = \\operatorname{nullspace} (A^* A) $$ $$ \\dim (\\operatorname{nullspace} (A)) = \\dim (\\operatorname{nullspace} (A^* A)) $$ $$ \\dim (\\operatorname{domain} (A)) - \\operatorname{rank} (A) = \\dim (\\operatorname{domain} (A^* A)) - \\operatorname{rank} (A^* A) $$ $$ \\operatorname{rank} (A) = \\operatorname{rank} (A^* A) $$\nCR Factorization Let $ A \\in M_{m,n}(\\bf F) $, $ \\operatorname{rank} (A) = r $. Suppose $ C $ contains the first $ r $ independent columns of $ A $. Suppose $ R $ contains the $ r $ nonzero rows of $ \\operatorname{rref} (A) $. Then $ A = C R $.\nNow suppose the matrix $ B $ contains the first $ r $ independent rows of $ A $. $ W $ is the $ r $ by $ r $ matrix where $ C $ meets $ B $. Then $ A = C W^{-1} B $.\nThe establishment I provided is based on LU and CR Elimination.\nEstablish $ A = C R $\nHere are the steps to establish $ A = C R $. We know that an invertible elimination matrix $ E $ (a product of simple steps) gives $ E A = R_0 = \\operatorname{rref} (A) $. Then $ A = E^{-1} R_0 $. Drop the $ m - r $ zero rows of $ R_0 $ and the last $ m - r $ columns of $ E^{-1} $. This leaves $ A = C \\begin{bmatrix} I \u0026amp; F \\end{bmatrix} P $, where the identity matrix in $ R $ allows us to identify $ C $ in the columns of $ E^{-1} $.\nEstablish $ A = C W^{-1} B $\nSuppose the $ r $ independent columns of $ A $ in the first $ r $ columns, and there are $ r $ independent rows of $ A $ in the first $ r $ rows.\nLet $ A = \\begin{bmatrix} W \u0026amp; H \\\\ J \u0026amp; K \\end{bmatrix} $, $ C = \\begin{bmatrix} W \\\\ J \\end{bmatrix} $, $ B = \\begin{bmatrix} W \u0026amp; H \\end{bmatrix} $.\nCombinations $ V $ of the rows of $ B $ must produce the dependent rows in $ \\begin{bmatrix} J \u0026amp; K \\end{bmatrix} $. Then $ \\begin{bmatrix} J \u0026amp; K \\end{bmatrix} = V B = \\begin{bmatrix} V W \u0026amp; V H \\end{bmatrix} $. And $ C = \\begin{bmatrix} I \\\\ V \\end{bmatrix} W $.\nCombinations $ T $ of the columns of $ A $ must produce the dependent columns in $ \\begin{bmatrix} H \\\\ K \\end{bmatrix} $. Then $ \\begin{bmatrix} H \\\\ K \\end{bmatrix} = C T = \\begin{bmatrix} W T \\\\ J T \\end{bmatrix} $. And $ B = W \\begin{bmatrix} I \u0026amp; T \\end{bmatrix} $.\n$$ A = \\begin{bmatrix} W \u0026amp; H \\\\ J \u0026amp; K \\end{bmatrix} = \\begin{bmatrix} W \u0026amp; H \\\\ VW \u0026amp; VH \\end{bmatrix} = \\begin{bmatrix} W \u0026amp; WT \\\\ VW \u0026amp; VWT \\end{bmatrix} = \\begin{bmatrix} I \\\\ V \\end{bmatrix} \\begin{bmatrix} W \\end{bmatrix} \\begin{bmatrix} I \u0026amp; T \\end{bmatrix} = C W^{-1} B $$\nWedderburn\u0026rsquo;s rank-one reduction formula Let $ A \\in M_{m,n}(\\bf F) $. If $ x \\in \\bf F^n $ and $ y \\in \\bf F^m $, and if\n$$ \\omega = y^{\\top} A x \\neq 0. $$\nThen\n$$ \\operatorname{rank} (A - \\omega^{-1} A x y^{\\top} A ) = \\operatorname{rank} (A) - 1. $$\nIn general, if $ X \\in M_{n,k} (\\bf F) $ and $ Y \\in M_{m,k} (\\bf F) $, and if\n$$ W = Y^{\\top} A X $$\nis nonsingular, then\n$$ \\operatorname{rank} (A - A X W^{-1} Y^{\\top} A) = \\operatorname{rank} A - k $$\nProof\nThe proof I provide is based on Generalized Wedderburn Rank Reduction.\nProof of rank-one reduction formula\nFor any $ v $ in $ \\operatorname{nullspace} (A) $, $$ A v = 0 \\Rightarrow (A - \\omega^{-1} A x y^{\\top} A) v = 0 $$\nHence, $ \\operatorname{nullspace} (A) \\subseteq \\operatorname{nullspace} (A - \\omega^{-1} A x y^{\\top} A) $.\nSince $ \\omega = y^{\\top} A x \\neq 0 \\Rightarrow A x \\neq 0 \\Rightarrow x \\notin \\operatorname{nullspace} (A) $, but\n$$ (A - \\omega^{-1} A x y^{\\top} A) x = (A - A) = 0 \\Rightarrow x \\in \\operatorname{nullspace} (A - \\omega^{-1} A x y^{\\top} A) . $$\nHence, $ \\operatorname{nullity} (A - \\omega^{-1} A x y^{\\top} A) - 1 \\geq \\operatorname{nullity} (A) $. Also, $ \\dim (\\operatorname{domain} (A)) = \\dim (\\operatorname{domain} (A - \\omega^{-1} A x y^{\\top} A)) $.\nTherefore\n$$ \\operatorname{rank} (A - \\omega^{-1} A x y^{\\top} A) \\leq \\operatorname{rank} (A) - 1. $$\nAssume now $ (A - \\omega^{-1} A x y^{\\top} A) u = 0 $. Let $ \\lambda = \\omega^{-1} y^{\\top} A u $. then\n$$ A (u - \\lambda x) = 0 $$\nhence $ u \\in \\operatorname{nullspace} (A) + \\bf F x $. Therefore\n$$ \\operatorname{rank} (A - \\omega^{-1} A x y^{\\top} A) = \\operatorname{rank} (A) - 1. $$\nNote that:\n$ A x $: A vector in the column space of $ A $.\n$ y^{\\top} A $: A row vector in the row space of $ A $.\n$ (Ax)(y^{\\top} A) $: A rank-1 matrix since it\u0026rsquo;s the outer product of a vector $ (A x) $ and a row vector $ (y^{\\top} A) $.\nThus, the term $ A x y^{\\top} A $ represents a modification to $ A $ in the direction of a rank-1 update.\nProof of general formula\nThe proof is similar.\nSince $ \\operatorname{rank} (M) = k $, the $ k $ columns of $ X $ are linearly independent, and $ A X \\in \\bf F^{m \\times k} $ is of rank $ k $, so no linear combination of columns of $ X $ is contained in the nullspace of $ A $. But we have\n$$ ( A - A X M^{-1} Y^{\\top} A ) X = 0 .$$\nWe know that $ \\operatorname{nullspace} (A) \\subseteq \\operatorname{nullspace} (A - A X M^{-1} Y^{\\top} A) $, so\n$$ \\operatorname{rank} (A - A X M^{-1} Y^{\\top} A) = n - \\dim(\\operatorname{nullspace} (A - A X M^{-1} Y^{\\top} A)) \\leq n - ((n - \\operatorname{rank} (A)) + k) = \\operatorname{rank} (A) - k. $$\nLet $ U \\in M_{n \\times k} (\\bf F) $ be any matrix such that\n$$ \\operatorname{rank} (A - A X M^{-1} Y^{\\top} A) U = 0. $$\nLet $ \\Lambda = M^{-1} Y^{\\top} A U $, then\n$$ A (U - X \\Lambda) = 0, $$\nhence\n$$ U \\in \\operatorname{nullspace} (A) + X \\Lambda. $$\nThis implies that\n$$ \\operatorname{nullspace} (A - A X M^{-1} Y^{\\top} A) \\subseteq \\operatorname{nullspace} (A) + \\mathcal{C} (X) \\cong \\operatorname{nullspace} (A) \\oplus \\mathcal{C} (X) ,$$\nand finally,\n$$ \\operatorname{rank} (A - A X M^{-1} Y^{\\top} A) = \\operatorname{rank} (A) - k. $$\n0.5 The Euclidean inner product Coordinates form of inner product and $ cos(\\theta) $ The inner product $ \u0026lt;F, s\u0026gt; $ is proposed to descrip the work done by force $ F $ acting along a displacement $ s $.\n$$ W = \u0026lt;F, s\u0026gt; = |F| \\sdot |s| \\sdot cos(\\theta) $$\nFrom this, the cosine of the angle $ \\theta $ between $ F $ and $ s $ can be expressed as:\n$$ cos(\\theta) = \\frac{\u0026lt;F, s\u0026gt;}{|F| \\sdot |s|} $$\nIn coordinates, the inner product $ \u0026lt;F, s\u0026gt; $ is defined as:\n$$ \u0026lt;F, s\u0026gt; = s^* F = s_x F_x + s_y F_y + s_z F_z $$\nThis decomposition shows that the work done in each direction corresponds to the component-wise product: The work in the $ x $ direction is $ s_x F_x $, the work in the $ y $ direction is $ s_y F_y $, and the work in the $ z $ direction is $ s_z F_z $.\nThus, the coordinate form provides a clear and meaningful breakdown of the total work into contributions from each axis.\n0.6 Determinants again The Cauchy-Binet formula Let $ A \\in M_{m,n} (\\bf F) $, $ B \\in M_{n,m} (\\bf F) $, and $ C = A B $.\nCauchy-Binet formula expresses determinant $ | C | $ in terms of the minors of $ A $ and $ B $:\n$$ \\tag{1} \\begin{vmatrix} c_{11} \u0026amp; \\cdots \u0026amp; c_{1m} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ c_{m1} \u0026amp; \\cdots \u0026amp; c_{mm} \\end{vmatrix} = \\sum_{1 \\leq k_1 \u0026lt; k_2 \u0026lt; \\cdots \u0026lt; k_m \\leq n} \\begin{vmatrix} a_{1 k_1} \u0026amp; \\cdots \u0026amp; a_{1 k_m} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m k_1} \u0026amp; \\cdots \u0026amp; a_{m k_m} \\end{vmatrix} \\begin{vmatrix} b_{k_1 1} \u0026amp; \\cdots \u0026amp; b_{k_1 m} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ b_{k_m 1} \u0026amp; \\cdots \u0026amp; b_{k_m m} \\end{vmatrix} $$\nor in a concise notation,\n$$ \\tag{1\u0026rsquo;} C \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\\\ 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\end{pmatrix} = \\sum_{1 \\leq k_1 \u0026lt; k_2 \u0026lt; \\cdots \u0026lt; k_m \\leq n} A \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\\\ k_1 \u0026amp; k_2 \u0026amp; \\cdots \u0026amp; k_m \\end{pmatrix} B \\begin{pmatrix} k_1 \u0026amp; k_2 \u0026amp; \\cdots \u0026amp; k_m \\\\ 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\end{pmatrix} $$\nDerivation\nThe derivation I provide is based on Volume 1 of Gantmacher\u0026rsquo;s classic The Theory of Matrices, in chapter 1 $ \\S $ 2.\nThe determinant of $ C $ can be represented in the form\n$$ \\begin{vmatrix} c_{11} \u0026amp; \\cdots \u0026amp; c_{1m} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ c_{m1} \u0026amp; \\cdots \u0026amp; c_{mm} \\end{vmatrix} = \\begin{vmatrix} \\sum_{\\alpha_1 = 1}^n a_{1 \\alpha_1} b_{\\alpha_1 1} \u0026amp; \\cdots \u0026amp; \\sum_{\\alpha_m = 1}^n a_{1 \\alpha_m} b_{\\alpha_m m} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\sum_{\\alpha_1 = 1}^n a_{m \\alpha_1} b_{\\alpha_1 1} \u0026amp; \\cdots \u0026amp; \\sum_{\\alpha_m = 1}^n a_{m \\alpha_m} b_{\\alpha_m m} \\end{vmatrix} $$\nExpanding the determinant, we consider the multilinear property of determinants: the determinant is linear in each row. Substituting each term into the determinant:\n$$ = \\sum_{\\alpha_1, \\alpha_2, \\cdots, \\alpha_m = 1}^n \\begin{vmatrix} a_{1 \\alpha_1} b_{\\alpha_1 1} \u0026amp; \\cdots \u0026amp; a_{1 \\alpha_m} b_{\\alpha_m m} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m \\alpha_1} b_{\\alpha_1 1} \u0026amp; \\cdots \u0026amp; a_{m \\alpha_m} b_{\\alpha_m m} \\end{vmatrix} $$\nNow, by the property of determinants that \u0026ldquo;multiplying a column by a number multiplies the determinant by this number,\u0026rdquo; we can factor out $ b_{\\alpha_i i} $ from the $ i $ -th column of the determinant:\n$$ = \\sum_{\\alpha_1, \\alpha_2, \\cdots, \\alpha_m = 1}^n \\begin{vmatrix} a_{1 \\alpha_1} \u0026amp; \\cdots \u0026amp; a_{1 \\alpha_m} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m \\alpha_1} \u0026amp; \\cdots \u0026amp; a_{m \\alpha_m} \\end{vmatrix} b_{\\alpha_1 1} b_{\\alpha_2 2} \\cdots b_{\\alpha_m m} $$\n$$ \\tag{2} = \\sum_{\\alpha_1, \\alpha_2, \\cdots, \\alpha_m = 1}^n A \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\\\ \\alpha_1 \u0026amp; \\alpha_2 \u0026amp; \\cdots \u0026amp; \\alpha_m \\end{pmatrix} b_{\\alpha_1 1} b_{\\alpha_2 2} \\cdots b_{\\alpha_m m} $$\nIf m \u0026gt; n, the matrices $ A $ and $ B $ do not have minors of order $ m $. In that case the right-hand sides of (1) and (1\u0026rsquo;) are to be replaced by zero.\nNow let $ m \\leq n $. Then in the sum on the right-hand side of (2) all those summands will be zero in which at least two of the subscripts $ \\alpha_1, \\alpha_2, \\cdots, \\alpha_m $ are equal. (If two columns of $ A $ are equal, its determinant is zero.) All the remaining summands of (2) can be split into groups of $ m! $ terms each by combining into one group those summands that differ from each other only in the order of the subscripts $ \\alpha_1, \\alpha_2, \\cdots, \\alpha_m $. Now within one such group the sum of the corresponding terms is\n$$ \\sum \\varepsilon (\\alpha_1, \\alpha_2, \\cdots, \\alpha_m) A \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\\\ k_1 \u0026amp; k_2 \u0026amp; \\cdots \u0026amp; k_m \\end{pmatrix} b_{\\alpha_1 1} b_{\\alpha_2 2} \\cdots b_{\\alpha_m m} $$\n$$ = A \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\\\ k_1 \u0026amp; k_2 \u0026amp; \\cdots \u0026amp; k_m \\end{pmatrix} \\sum \\varepsilon (\\alpha_1, \\alpha_2, \\cdots, \\alpha_m) b_{\\alpha_1 1} b_{\\alpha_2 2} \\cdots b_{\\alpha_m m} $$\nBy the Leibniz formula for determinants, the summation over all permutations can be expressed compactly. Thus, the expression becomes:\n$$ = A \\begin{pmatrix} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\\\ k_1 \u0026amp; k_2 \u0026amp; \\cdots \u0026amp; k_m \\end{pmatrix} B \\begin{pmatrix} k_1 \u0026amp; k_2 \u0026amp; \\cdots \u0026amp; k_m \\\\ 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; m \\end{pmatrix} $$\nHere, $ k_1 \u0026lt; k_2 \u0026lt; \\cdots \u0026lt; k_m $ is the normal order of the subscripts $ \\alpha_1, \\alpha_2, \\cdots, \\alpha_m $ and $ \\varepsilon (\\alpha_1, \\alpha_2, \\cdots, \\alpha_m) = (-1)^N $ where $ N $ is the number of transpositions of the indices needed to put the permutation $ \\alpha_1, \\alpha_2, \\cdots, \\alpha_m $ into normal order.\nHence from (2) we obtain (1\u0026rsquo;).\nGeneralization\nLet $ A \\in M_{m,k} (\\bf F) $, $ B \\in M_{k,n} (\\bf F) $, and $ C = A B $. Furthermore, let $ 1 \\leq r \\leq min \\{m, k, n\\} $, and let $ \\alpha \\subseteq \\{1, \\dots, m\\} $ and $ \\beta \\subseteq \\{1, \\dots, n\\} $ be index sets, each of cardinality $ r $. An expression for the $ \\alpha $, $ \\beta $ minor of $ C $ is\n$$ \\operatorname{det} C [\\alpha, \\beta] = \\sum_\\gamma \\operatorname{det} A [\\alpha, \\gamma] \\operatorname{det} B [\\gamma, \\beta] $$\nin which the sum is taken over all index sets $ \\gamma \\subseteq \\{1, \\dots, k\\} $ of cardinality $ r $.\nDerivation\nGiven that\n$$ C [\\alpha, \\beta] = A [\\alpha, [k]] B [[k], \\beta] $$\nand applying the Cauchy-Binet formula, we arrive at the generalized form above.\nThe Sylvester-Franke Theorem If $ A \\in M_n $ and $ 1 \\leq k \\leq n $, then $ \\operatorname{det} C_k(A) = (\\operatorname{det} A)^e $, where $ e = {\\begin{pmatrix} n - 1 \\\\ k - 1\\end{pmatrix}} $.\nProof\nThe proof I provide is based on Leonard Tornheim, The Sylvester-Franke Theorem, The American Mathematical Monthly.\nThere are three types $ E_1 $, $ E_2 $, $ E_3 $ of elementary transformations: $ E_1 $ is the multiplication of a row or column by a constant $ k $; $ E_2 $ is the interchange of two adjacent rows or columns; and $ E_3 $ is the addition of k times a row or column to another row or column. If $ B $ is transformed into $ C $ by $ E_i $, then\n$$ \\tag{1} \\operatorname{det} C = \\mu_i \\operatorname{det} B, $$\nwhere $ \\mu_1 = k $, $ \\mu_2 = -1 $, and $ \\mu_3 = +1 $.\nThe proof of the theorem will be based on the following lemma.\nLemma 0.6.1 Let $ B $ be an arbitrary $ n \\times n $ square matrix such that\n$$ \\tag{2} \\operatorname{det} C_k(B) = (\\operatorname{det} B)^e $$\nIf $ B $ is transformed into $ C $ by an elementary transformation $ E $, then $ \\operatorname{det} C_k(C) = (\\operatorname{det} C)^e $.\nProof of Lemma 0.6.1\nWe state first that\n$$ \\tag{3} \\operatorname{det} C_k(C) = \\mu_i^e \\operatorname{det} C_k(B). $$\nWe shall prove this in detail only for $ i = 2 $; the other two cases are easier. Suppose that the $ u $-th and ($ u + 1 $)-th rows of $ B $ have been interchanged to give a matrix $ C $. The minors of $ C $ which do not involve the $ u $-th or ($ u + 1 $)-th rows equal the corresponding minors of $ B $. If both $ u $ and $ u + 1 $ appear in $ \\lambda (p) $, then $ C_{\\lambda (p) \\lambda (q)} = -B_{\\lambda (p) \\lambda (q)} $. The number of rows of $ C_k (C) $ having such minors is $ {\\begin{pmatrix} n - 2 \\\\ k - 2 \\end{pmatrix}} $. Next suppose $ u $ appears in $ \\lambda (p) $ but $ u + 1 $ does not. Let $ \\lambda (p\u0026rsquo;) $ be obtained from $ \\lambda (p) $ by replacing $ u $ by $ u + 1 $. Then $ C_{\\lambda (p) \\lambda (q)} = B_{\\lambda (p\u0026rsquo;) \\lambda (q)} $ and $ B_{\\lambda (p\u0026rsquo;) \\lambda (q)} = C_{\\lambda (p) \\lambda (q)} $ since the same elements are used and in the same arrangement, because the $ u $-th and ($ u + 1 $)-th rows are adjacent. Thus from this cause $ {\\begin{pmatrix} n - 2 \\\\ k - 1 \\end{pmatrix}} $ pairs of rows have been interchanged in going from $ C_k(B) $ to $ C_k(C) $. (There are $ 2 \\times {\\begin{pmatrix} n - 2 \\\\ k - 1 \\end{pmatrix}} $ rows in $ C $ that need to be changed to match $ B $. However, these rows must be interchanged within $ C $, forming $ {\\begin{pmatrix} n - 2 \\\\ k - 1 \\end{pmatrix}} $ pairs.) Thus the total number of changes in sign in going from $ \\operatorname{det} C_k(B) $ to $ \\operatorname{det} C_k(C) $ is\n$$ {\\begin{pmatrix} n - 2 \\\\ k - 2 \\end{pmatrix}} + {\\begin{pmatrix} n - 2 \\\\ k - 1 \\end{pmatrix}} = {\\begin{pmatrix} n - 1 \\\\ k - 1 \\end{pmatrix}} = e.$$\nFrom (3), (1), and (2) it now follows that\n$$ \\operatorname{det} C_k(C) = \\mu_i^e \\operatorname{det} C_k(B) = \\mu_i^e (\\operatorname{det} B)^e = (\\mu_i \\operatorname{det} B)^e = (\\operatorname{det} C)^e, $$\nand the proof of the Lemma 0.6.1 is complete.\nProof of Sylvester-Franke Theorem\nThe proof of the Sylvester-Franke Theorem can now be given as follows. It is known that there exists an $ n \\times n $ matrix\n$$ D = \\begin{pmatrix} I_r \u0026amp; 0 \\\\ 0 \u0026amp; 0 \\end{pmatrix} $$\nwith $ 0 \\leq r \\leq n $.\n$ D $ can be ransformed into the given $ n \\times n $ matrix $ A $ by a finite sequence of elementary transformations $ E^{(1)} $, $ E^{(2)} $, $ \\dots $, $ E^{(v)} $.\nIf $ r = n $, $ \\operatorname{det} D = 1 $ and if $ r \u0026lt; n $, $ \\operatorname{det} D = 0 $. Also if $ r = n $, $ C_k (D) = I_{\\begin{pmatrix} n \\\\ k \\end{pmatrix}} $, and $ \\operatorname{det} C_k (D) = 1 $; otherwise $ \\operatorname{det} C_k (D) = 0 $. Hence the relation $ \\operatorname{det} C_k (D) = (\\operatorname{det} D)^e $ holds in both cases. Then by the lemma, $ \\operatorname{det} C_k (E^{(1)} D) = \\operatorname{det} C_k (D_1) = (\\operatorname{det} D_1)^e $, $ \\operatorname{det} C_k (E^{(2)} D_1) = \\operatorname{det} C_k (D_2) = (\\operatorname{det} D_2)^e $, and so on. If follows by induction that $ \\operatorname{det} C_k (E^{(v)} D_{v - 1}) = \\operatorname{det} C_k (A) = (\\operatorname{det} A)^e $, and the proof is complete.\nLaplace expansion Let $ n \\in \\mathbf{N} $. Let $ A = (a_{i,j})^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $ be an $ n \\times n $ matrix.\n(a) For every $ p \\in \\{1, 2, \\dots, n\\} $, we have\n$$ \\operatorname{det} A = \\sum_{q = 1}^n (-1)^{p + q} a_{p,q} \\operatorname{det} (A_{\\sim p, \\sim q}). $$\n(b) For every $ q \\in \\{1, 2, \\dots, n\\} $, we have\n$$ \\operatorname{det} A = \\sum_{p = 1}^n (-1)^{p + q} a_{p,q} \\operatorname{det} (A_{\\sim p, \\sim q}). $$\nProof\nThe proof I provide is based on 6.12. Laplace expansion, in Notes on the combinatorial fundamentals of algebra, Darij Grinberg.\nLemma 0.6.2\nFor every $ n \\in N $, let $ [n] $ denote the set $ \\{1, 2, \\dots, n\\}$. Let $ n \\in \\mathbf{N} $. For every $ p \\in [n] $, we define a permutation $ g_p \\in S_n $ by $ g_p = cyc_{p, p + 1, \\dots, n} $ (where I am using the notations of Definition 5.37 in Notes).\n(a) We have $ g_p(1), g_p(2), \\dots, g_p(n - 1) = (1, 2, \\dots, \\hat{p}, \\dots, n) $ for every $ p \\in [n] $.\n(b) We have $ (-1)^{g_p} = (-1)^{n-p} $ for every $ p \\in [n] $.\n(c) Let $ p \\in [n] $. We define a map\n$$ g_p^{\\prime}: [n - 1] \\to [n] \\setminus {p} $$\nby\n$$ (g_p^{\\prime} (i) = g_p (i) \\quad \\text{for every } i \\in [n - 1]). $$\nThis map $ g_p^{\\prime} $ is well-defined and bijective.\n(d) Let $ p \\in [n] $ and $ q \\in [n] $. We define a map\n$$ T: \\{\\tau \\in S_n | \\tau(n) = n\\} \\to \\{\\tau \\in S_n | \\tau(p) = q\\} $$\nby\n$$ (T(\\sigma) = g_q \\circ \\sigma \\circ g_p^{-1} \\quad \\text{for every } \\sigma \\in \\{\\tau \\in S_n | \\tau(n) = n\\}). $$\nThen, this map $ T $ is well-defined and bijective.\nLemma 0.6.3\nLet $ n \\in \\mathbf{N} $. Let $ A = (a_{i,j})^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $ be an $ n \\times n $ matrix. Let $ p \\in \\{1, 2, \\dots, n\\} $ and $ q \\in \\{1, 2, \\dots, n\\} $. Then,\n$$ \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(p) = q}} (-1)^{\\sigma} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, \\sigma(i)} = (-1)^{p + q} \\operatorname{det} (A_{\\sim p, \\sim q}). $$\nProof of Lemma 0.6.3\nLet us use all notations introduced in Lemma 0.6.2.\nNow, the definition of $ A_{\\sim p, \\sim q} $ yields\n$$ \\begin{aligned} A_{\\sim p, \\sim q} \u0026amp;= \\operatorname{sub} \\substack{1, 2, \\dots, \\hat{q}, \\dots, n \\\\ 1, 2, \\dots, \\hat{p}, \\dots, n} A = \\operatorname{sub} \\substack{g_q(1), g_q(2), \\dots, g_q(n - 1) \\\\ g_p(1), g_p(2), \\dots, g_p(n - 1)} A \\\\ \u0026amp;= (a_{g_p(i), g_q(j)})^{1 \\leq i \\leq n - 1, 1 \\leq j \\leq n - 1} \\end{aligned} $$\nNow, let us recall the map $ T: \\{\\tau \\in S_n | \\tau(n) = n\\} \\to \\{\\tau \\in S_n | \\tau(p) = q\\} $ defined in Lemma 0.6.2 (d). Lemma 0.6.2 (d) says that this map $ T $ is well-defined and bijective. Every $ \\sigma \\in \\{\\tau \\in S_n | \\tau(n) = n\\} $ satisfies\n$$ \\tag{1} (-1)^{T(\\sigma)} = (-1)^{p + q} \\cdot (-1)^{\\sigma} $$\nand\n$$ \\tag{2} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, (T(\\sigma))(i)} = \\prod_{i = 1}^{n - 1} a_{g_p(i), g_q(\\sigma(i))}. $$\nProof of result (1)\nApplying Lemma 0.6.2 (b) to $ q $ instead of $ p $, we obtain $ (-1)^{g_q} = (-1)^{n-q} = (-1)^{n+q} $ (since $ n - q \\equiv n + q \\mod 2 $).\nThe definition of $ T(\\sigma) $ yields $ T(\\sigma) = g_q \\circ \\sigma \\circ g_p^{-1} $. Thus,\n$$ T(\\sigma) \\circ g_p = g_q \\circ \\sigma \\circ g_p^{-1} \\circ g_p = g_q \\circ \\sigma, $$\nso that\n$$ \\begin{aligned} (-1)^{T(\\sigma) \\circ g_p} \u0026amp;= (-1)^{g_q \\circ \\sigma} = (-1)^{g_q} \\cdot (-1)^{\\sigma} \\\\ \u0026amp;= (-1)^{n + q} \\cdot (-1)^{\\sigma}. \\end{aligned} $$\nCompared with\n$$ \\begin{aligned} (-1)^{T(\\sigma) \\circ g_p} \u0026amp;= (-1)^{T(\\sigma)} \\cdot (-1)^{g_p} \\\\ \u0026amp;= (-1)^{T(\\sigma)} \\cdot (-1)^{n - p}, \\end{aligned} $$\nthis yields\n$$ (-1)^{T(\\sigma)} \\cdot (-1)^{n - p} = (-1)^{n + q} \\cdot (-1)^{\\sigma} $$\nWe can divide both sides of this equality by $ (-1)^{n-p} $ (since $ (-1)^{n-p} \\in \\{1, -1\\} $ is clearly an invertible integer), and thus we obtain\n$$ (-1)^{T(\\sigma)} = (-1)^{p + q} \\cdot (-1)^{\\sigma}. $$\nThis proves result (1).\nProof of result (2)\nLet $ \\sigma \\in \\{\\tau \\in S_n | \\tau(n) = n\\} $. Let us recall the map $ g_p^{\\prime}: [n-1] \\to [n] \\setminus {p} $ introduced in Lemma 0.6.2 (c). Lemma 0.6.2 (c) says that this map $ g_p^{\\prime} $ is well-defined and bijective. In other words, $ g_p^{\\prime} $ is a bijection. Let $ i \\in [n-1] $. Then, $ g_p^{\\prime} (i) = g_p(i) $ (by the definition of $ g_p^{\\prime} $). Also, the definition of $ T $ yields $ T(\\sigma) = g_q \\circ \\sigma \\circ g_p^{-1} $, so That\n$$ T(\\sigma) (g_p^{\\prime} (i)) = (g_q \\circ \\sigma \\circ g_p^{-1}) (g_p (i)) = g_q (\\sigma (g_p^{-1} (g_p (i)))) = g_q (\\sigma(i)). $$\nFrom $ g_p^{\\prime} (i) = g_p (i) $ and $ (T(\\sigma)) (g_p^{\\prime} (i)) = g_q (\\sigma(i)) $, we obtain\n$$ a_{g_p^{\\prime} (i), (T(\\sigma)) (g_p^{\\prime} (i))} = a_{g_p (i), g_q (\\sigma(i))}. $$\nNow, let us forget that we fixed $ i $. We thus have proven the above equation for every $ i \\in [n-1] $. But now, we have\n$$ \\begin{aligned} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, (T(\\sigma))(i)} \\\\ \u0026amp;= \\prod_{\\substack{i \\in [n]; \\\\ i \\neq p}} a_{i, (T(\\sigma))(i)} = \\prod_{i \\in [n] \\setminus \\{p\\}} a_{i, (T(\\sigma))(i)} = \\prod_{i \\in [n - 1]} a_{g_p^{\\prime} (i), (T(\\sigma)) (g_p^{\\prime} (i))} \\\\ \u0026amp;\\quad (\\text{here, we have substituted } g_p^{\\prime} (i) \\text{ for } i \\text{, since } g_p^{\\prime}: [n - 1] \\to [n] \\setminus \\{p\\} \\text{ is a bijection}) \\\\ \u0026amp;= \\prod_{i \\in [n - 1]} a_{g_p(i), g_q(\\sigma(i))}. \\end{aligned} $$\nThis proves result (2).\nNow,\n$$ \\begin{aligned} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(p) = q}} (-1)^{\\sigma} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, \\sigma(i)} \\\\ \u0026amp;= \\sum_{\\substack{\\sigma \\in \\{\\tau \\in S_n | \\tau(p) = q\\}}} (-1)^{\\sigma} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, \\sigma(i)} \\\\ \u0026amp;= \\sum_{\\substack{\\sigma \\in \\{\\tau \\in S_n | \\tau(n) = n\\}}} (-1)^{T(\\sigma)} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, (T(\\sigma))(i)} \\\\ \u0026amp;\\quad (\\text{here, we have substituted }T(\\sigma) \\text{ for } \\sigma \\text{ in the sum, since the map } T: \\{\\tau \\in S_n | \\tau(n) = n\\} \\to \\{\\tau \\in S_n | \\tau(p) = q\\} \\text{ is a bijection}) \\\\ \u0026amp;= \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(n) = n}} (-1)^{p + q} \\cdot (-1)^{\\sigma} \\prod_{i = 1}^{n - 1} a_{g_p(i), g_q(\\sigma(i))} \\\\ \u0026amp;= (-1)^{p + q} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(n) = n}} (-1)^{\\sigma} \\prod_{i = 1}^{n - 1} a_{g_p(i), g_q(\\sigma(i))} \\\\ \u0026amp;= (-1)^{p + q} \\det \\left(a_{g_p(i), g_q(j)}\\right)^{1 \\leq i \\leq n - 1, 1 \\leq j \\leq n - 1} \\\\ \u0026amp;= (-1)^{p + q} \\operatorname{det}(A_{\\sim p, \\sim q}). \\end{aligned} $$\nThis proves Lemma 0.6.3.\nNow, we can finally prove Laplace expansion\nProof of Laplace expansion (a)\nLet $ p \\in \\{1, 2, \\dots, n\\} $.\n$$ \\begin{aligned} \\operatorname{det} A \u0026amp;= \\sum_{\\sigma \\in S_n} (-1)^{\\sigma} \\prod_{i = 1}^{n} a_{i, \\sigma(i)} \\\\ \u0026amp;= \\sum_{q \\in \\{1, 2, \\dots, n\\}} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(p) = q}} (-1)^{\\sigma} \\prod_{i = 1}^{n} a_{i, \\sigma(i)} \\\\ \u0026amp;= \\sum_{q \\in \\{1, 2, \\dots, n\\}} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(p) = q}} (-1)^{\\sigma} \\prod_{i \\in \\{1, 2, \\dots, n\\}} a_{i, \\sigma(i)} \\\\ \u0026amp;= \\sum_{q \\in \\{1, 2, \\dots, n\\}} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(p) = q}} (-1)^{\\sigma} a_{p, \\sigma(p)} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, \\sigma(i)} \\\\ \u0026amp;= \\sum_{q = 1}^{n} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(p) = q}} (-1)^{\\sigma} a_{p, q} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, \\sigma(i)} \\\\ \u0026amp;= \\sum_{q = 1}^{n} a_{p, q} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(p) = q}} (-1)^{\\sigma} \\prod_{\\substack{i \\in \\{1, 2, \\dots, n\\}; \\\\ i \\neq p}} a_{i, \\sigma(i)} \\\\ \u0026amp;= \\sum_{q = 1}^{n} a_{p, q} (-1)^{p + q} \\operatorname{det} (A_{\\sim p, \\sim q}) = \\sum_{q = 1}^{n} (-1)^{p + q} a_{p, q} \\operatorname{det} (A_{\\sim p, \\sim q}) \\end{aligned} $$\nThis proves Laplace expansion (a).\nLaplace expansion in multiple rows/columns Let $ n \\in \\mathbf{N} $. Let $ A \\in \\mathbf{F}^{n \\times n} $. For any subset $ I $ of $ \\{1, 2, \\dots, n\\} $, we let $ I $ denote the complement $ \\{1, 2, \\dots, n\\} \\setminus I $ of $ I $.\n(a) For every subset $ P $ of $ \\{1, 2, \\dots, n\\} $, we have\n$$ \\operatorname{det} A = \\sum_{\\substack{Q \\subseteq \\{1, 2, \\dots, n\\}; \\\\ |Q| = |P|}} (-1)^{\\sum P + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} A). $$\n(b) For every subset $ Q $ of $ \\{1, 2, \\dots, n\\} $, we have\n$$ \\operatorname{det} A = \\sum_{\\substack{P \\subseteq \\{1, 2, \\dots, n\\}; \\\\ |P| = |Q|}} (-1)^{\\sum P + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} A). $$\nProof\nThe proof I provide is based on 6.22. Laplace expansion in multiple rows/columns, in Notes on the combinatorialfundamentals of algebra, Darij Grinberg.\nLemma 0.6.4\nLet $ n \\in \\mathbf{N} $. For any subset $ I $ of $ \\{1, 2, \\dots, n\\} $, we let $ I $ denote the complement $ \\{1, 2, \\dots, n\\} \\setminus I $ of $ I $.\nLet $ A = (a_{i,j})^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $ and $ B = (b_{i,j})^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $ be two $ n \\times n $ matrices. Let $ Q $ be a subset of $ \\{1, 2, \\dots, n\\} $. Let $ k = |Q| $. Then,\n$$ \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(\\{1, 2, \\dots, k\\}) = Q}} (-1)^{\\sigma} (\\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{i, \\sigma(i)}) (\\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{i, \\sigma(i)}) \\\\ = (-1)^{(1 + 2 + \\dots + k) + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ (\\{1, 2, \\dots, k\\})} A) (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ (\\{k + 1, k + 2, \\dots, n\\})} B). $$\nProof of Lemma 0.6.4\nWe begin with some simple observations. The definition of $ Q $ yields $ \\tilde{Q} = \\{1, 2, \\dots, n\\} \\setminus Q $. Since $ Q $ is a subset of $ \\{1, 2, \\dots, n\\} $, this leads to $ |\\tilde{Q}| = |\\{1, 2, \\dots, n\\}| - |Q| = n - k $. Thus, $ n - k = |\\tilde{Q}| \\geq 0 $, so that $ n \\geq k $ and thus $ k \\in \\{1, 2, \\dots, n\\} $.\nLet $ (q_1, q_2, \\dots, q_k) $ be the list of all elements of $ Q $ in increasing order (with no repetitions). Let $ (r_1, r_2, \\dots, r_{n−k}) $ be the list of all elements of $ \\tilde{Q} $ in increasing order (with no repetitions).\nThe lists $ w(Q) $ and $ (q_1, q_2, \\dots, q_k) $ must be identical. In other words, we have $ w(Q) = (q_1, q_2, \\dots, q_k) $. Similarly, $ w(\\tilde{Q}) = (r_1, r_2, \\dots, r_{n−k}) $.\nfor every $ \\alpha ∈ S_k $ and $ \\beta ∈ S_{n−k} $, we can define an element $ \\sigma_{Q, \\alpha, \\beta} ∈ S_n $ according to Exercise 5.14 (a) in Notes. Consider this $ \\sigma_{Q, \\alpha, \\beta} $. Exercise 5.14 (b) in Notes shows that for every $ \\alpha ∈ S_k $ and $ \\beta ∈ S_{n−k} $, we have\n$$ \\ell (\\sigma_{Q, \\alpha, \\beta}) = \\ell (\\alpha) + \\ell (\\beta) + \\sum Q - (1 + 2 + \\dots + k) $$\nand\n$$ (-1)^{\\sigma_{Q, \\alpha, \\beta}} = (-1)^{\\alpha} \\cdot (-1)^{\\beta} \\cdot (-1)^{\\sum Q - (1 + 2 + \\dots + k)}. $$\nExercise 5.14 (c) in Notes shows that the map\n$$ S_k \\times S_{n−k} \\to {\\tau \\in S_n | \\tau(\\{1, 2, \\dots, k\\}) = Q}, \\\\ (\\alpha, \\beta) \\mapsto \\sigma_{Q, \\alpha, \\beta} $$\nis well-defined and a bijection.\nWe have $ w(Q) = (q_1, q_2, \\dots, q_k) $. Thus,\n$$ \\begin{aligned} \\operatorname{sub} \\substack{w(Q) \\\\ (1, 2, \\dots, k)} A \u0026amp;= \\operatorname{sub} \\substack{(q_1, q_2, \\dots, q_k) \\\\ (1, 2, \\dots, k)} A = \\operatorname{sub} \\substack{q_1, q_2, \\dots, q_k \\\\ 1, 2, \\dots, k} A = (a_{x, q_y})^{1 \\leq x \\leq k, 1 \\leq y \\leq k} \\\\ \u0026amp;= (a_{i, q_j})^{1 \\leq i \\leq k, 1 \\leq j \\leq k} \\end{aligned} $$\nThus,\n$$ \\begin{aligned} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ (1, 2, \\dots, k)} A) \u0026amp;= \\sum_{\\sigma \\in S_k} (-1)^{\\sigma} \\prod_{i = 1}^{k} a_{i, q_{\\sigma(i)}} \\\\ \u0026amp;= \\sum_{\\alpha \\in S_k} (-1)^{\\alpha} \\prod_{i = 1}^{k} a_{i, q_{\\alpha(i)}} \\\\ \\end{aligned} $$\nAlso, $ w(\\tilde{Q}) = (r_1, r_2, \\dots, r_{n−k}) $. Thus,\n$$ \\begin{aligned} \\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ (k + 1, k + 2, \\dots, n)} B \u0026amp;= \\operatorname{sub} \\substack{(r_1, r_2, \\dots, r_{n−k}) \\\\ (k + 1, k + 2, \\dots, n)} B = \\operatorname{sub} \\substack{r_1, r_2, \\dots, r_{n−k} \\\\ k + 1, k + 2, \\dots, n} B = (b_{k + x, r_y})^{1 \\leq x \\leq n - k, 1 \\leq y \\leq n - k} \\\\ \u0026amp;= (b_{k + i, r_j})^{1 \\leq i \\leq n - k, 1 \\leq j \\leq n - k} \\end{aligned} $$\nThus,\n$$ \\begin{aligned} \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ (k + 1, k + 2, \\dots, n)} B) \u0026amp;= \\sum_{\\sigma \\in S_{n−k}} (-1)^{\\sigma} \\prod_{i = 1}^{n - k} b_{k + i, r_{\\sigma(i)}} \\\\ \u0026amp;= \\sum_{\\beta \\in S_{n−k}} (-1)^{\\beta} \\prod_{i = 1}^{n - k} b_{k + i, r_{\\beta(i)}} \\\\ \\end{aligned} $$\nNow, we claim the following: For any $ \\alpha \\in S_k $ and $ \\beta \\in S_{n−k} $, we have\n$$ \\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{i, \\sigma_{Q, \\alpha, \\beta}(i)} = \\prod_{i = 1}^{k} a_{i, q_{\\alpha(i)}} $$\nand\n$$ \\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{i, \\sigma_{Q, \\alpha, \\beta}(i)} = \\prod_{i = 1}^{n - k} b_{k + i, r_{\\beta(i)}}. $$\nNow,\n$$ \\begin{aligned} \u0026amp; \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(\\{1, 2, \\dots, k\\}) = Q}} (-1)^{\\sigma} (\\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{i, \\sigma(i)}) (\\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{i, \\sigma(i)}) \\\\ \u0026amp;= \\sum_{\\sigma \\in \\{\\tau \\in S_n | \\tau(\\{1, 2, \\dots, k\\}) = Q \\}} (-1)^{\\sigma} (\\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{i, \\sigma(i)}) (\\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{i, \\sigma(i)}) \\\\ \u0026amp;= \\sum_{(\\alpha, \\beta) \\in S_k \\times S_{n−k}} (-1)^{\\sigma_{Q, \\alpha, \\beta}} (\\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{i, \\sigma_{Q, \\alpha, \\beta}(i)}) (\\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{i, \\sigma_{Q, \\alpha, \\beta}(i)}) \\\\ \u0026amp; (\\text{here, we have substituted } \\sigma_{Q, \\alpha, \\beta} \\text{ for } \\sigma \\text{ in the sum, since the map } S_k \\times S_{n−k} \\to {\\tau \\in S_n | \\tau(\\{1, 2, \\dots, k\\}) = Q} \\text{, } (\\alpha, \\beta) \\mapsto \\sigma_{Q, \\alpha, \\beta} \\text{ is a bijection}) \\\\ \u0026amp;= \\sum_{\\alpha \\in S_k} \\sum_{\\beta \\in S_{n−k}} (-1)^{\\sigma_{Q, \\alpha, \\beta}} (\\prod_{i = 1}^{k} a_{i, q_{\\alpha(i)}}) (\\prod_{i = 1}^{n - k} b_{k + i, r_{\\beta(i)}}) \\\\ \u0026amp;= \\sum_{\\alpha \\in S_k} \\sum_{\\beta \\in S_{n−k}} (-1)^{\\alpha} \\cdot (-1)^{\\beta} \\cdot (-1)^{\\sum Q - (1 + 2 + \\dots + k)} (\\prod_{i = 1}^{k} a_{i, q_{\\alpha(i)}}) (\\prod_{i = 1}^{n - k} b_{k + i, r_{\\beta(i)}}) \\\\ \u0026amp;= (-1)^{\\sum Q - (1 + 2 + \\dots + k)} (\\sum_{\\alpha \\in S_k} (-1)^{\\alpha} (\\prod_{i = 1}^{k} a_{i, q_{\\alpha(i)}})) (\\sum_{\\beta \\in S_{n−k}} (-1)^{\\beta} (\\prod_{i = 1}^{n - k} b_{k + i, r_{\\beta(i)}})) \\\\ \u0026amp;= (-1)^{(1 + 2 + \\dots + k) + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ (1, 2, \\dots, k)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ (k + 1, k + 2, \\dots, n)} B) . \\end{aligned} $$\nThis proves Lemma 0.6.4.\nLemma 0.6.5\nLet $ n \\in \\mathbf{N} $. Let $ \\gamma \\in S_n $. Then, the map $ S_n \\to S_n $, $ \\sigma \\to \\sigma \\circ \\gamma $ is a bijection.\nLemma 0.6.6\nLet $ n \\in \\mathbf{N} $. For any subset $ I $ of $ \\{1, 2, \\dots, n\\} $, we let $ \\tilde{I} $ denote the complement $ \\{1, 2, \\dots, n\\} \\setminus I $ of $ I $. Let $ I $ be a subset of $ \\{1, 2, \\dots, n\\} $. Let $ k = |I| $. Then, there exists a $ \\sigma \\in S_n $ satisfying $ (\\sigma (1), \\sigma (2), \\dots, \\sigma (k)) = w(I) $, $ (\\sigma (k + 1), \\sigma (k + 2), \\dots, \\sigma(n)) = w(\\tilde{I}) $, and $ (−1)^{\\sigma} = (−1)^{\\sum I − (1 + 2 + \\dots + k)} $.\nLemma 0.6.7\nLet $ n \\in \\mathbf{N} $. For any subset $ I $ of $ \\{1, 2, \\dots, n\\} $, we let $ \\tilde{I} $ denote the complement $ \\{1, 2, \\dots, n\\} \\setminus I $ of $ I $.\nLet $ A = a_{i, j}^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $ and $ B = b_{i, j}^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $ be two $ n \\times n $ matrices. Let $ P $ and $ Q $ be two subsets of $ \\{1, 2, \\dots, n\\} $ such that $ |P| = |Q| $. Then,\n$$ \\begin{aligned} \u0026amp; \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i \\in P} a_{i, \\sigma(i)}) (\\prod_{i \\in \\tilde{P}} b_{i, \\sigma(i)}) \\\\ \u0026amp;= (-1)^{\\sum P + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} B). \\end{aligned} $$\nProof of Lemma 0.6.7\nDefine $ k \\in \\mathbf{N} $ by $ k = |P| = |Q| $.\nThe definition of $ \\tilde{P} $ yields $ \\tilde{P} = \\{1, 2, \\dots, n\\} \\setminus P $. $ |\\tilde{P}| = n - k $.\nLet $ (p_1, p_2, \\dots, p_k) $ be the list of all elements of $ P $ in increasing order (with no repetitions).\nLet $ (r_1, r_2, \\dots, r_{n−k}) $ be the list of all elements of $ \\tilde{P} $ in increasing order (with no repetitions).\nLemma 0.6.6 shows that there exists a $ \\sigma \\in S_n $ satisfying $ (\\sigma (1), \\sigma (2), \\dots, \\sigma (k)) = w(P) $, $ (\\sigma (k + 1), \\sigma (k + 2), \\dots, \\sigma(n)) = w(\\tilde{P}) $, and $ (−1)^{\\sigma} = (−1)^{\\sum P − (1 + 2 + \\dots + k)} $. Denote this $ \\sigma $ by $ \\gamma $.\nNotice that\n$$ \\begin{aligned} (-1)^{\\sum P - (1 + 2 + \\dots + k)} (-1)^{\\gamma} \u0026amp;= (-1)^{\\sum P - (1 + 2 + \\dots + k)} (-1)^{\\sum P - (1 + 2 + \\dots + k)} \\\\ \u0026amp;= 1 \\end{aligned} $$\nDefine an $ n \\times n $ matrix $ A^{\\prime} $ by $ A^{\\prime} = a_{\\gamma (i), j}^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $. Define an $ n \\times n $ matrix $ B^{\\prime} $ by $ B^{\\prime} = b_{\\gamma(i), j}^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $. Then,\n$$ \\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A = \\operatorname{sub} \\substack{w(Q) \\\\ (1, 2, \\dots, k)} (A^{\\prime}) $$\nand\n$$ \\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} B = \\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ (k + 1, k + 2, \\dots, n)} (B^{\\prime}) . $$\nLemma 0.6.5 shows that the map $ S_n \\to S_n $, $ \\sigma \\to \\sigma \\circ \\gamma $ is a bijection.\nBut $ \\gamma (\\{1, 2, \\dots, k\\}) = P $. Hence, every $ \\sigma \\in S_n $ satisfies\n$$ (\\sigma \\circ \\gamma) (\\{1, 2, \\dots, k\\}) = \\sigma (\\gamma (\\{1, 2, \\dots, k\\})) = \\sigma (P) . $$\nFurthermore, every $ \\sigma \\in S_n $ satisfies\n$$ \\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{\\gamma (i), (\\sigma \\circ \\gamma) (i)} = \\prod_{i \\in P} a_{i, \\sigma(i)} $$\nand\n$$ \\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{\\gamma (i), (\\sigma \\circ \\gamma) (i)} = \\prod_{i \\in \\tilde{P}} b_{i, \\sigma(i)}. $$\nNow, Lemma 0.6.4 yields\n$$ \\begin{aligned} \u0026amp; \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(\\{1, 2, \\dots, k\\}) = Q}} (-1)^{\\sigma} (\\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{\\gamma (i), \\sigma(i)}) (\\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{\\gamma (i), \\sigma(i)}) \\\\ \u0026amp;= \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(\\{1, 2, \\dots, k\\}) = Q}} (-1)^{\\sigma} (\\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{i, \\sigma(i)}^{\\prime})(\\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{i, \\sigma(i)}^{\\prime}) \\\\ \u0026amp;= (-1)^{(1 + 2 + \\dots + k) + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ \\{1, 2, \\dots, k\\}} A^{\\prime}) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ \\{k + 1, k + 2, \\dots, n\\}} B^{\\prime}) \\\\ \u0026amp;= (-1)^{(1 + 2 + \\dots + k) + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} B). \\end{aligned} $$\nComparing this with\n$$ \\begin{aligned} \u0026amp; \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(\\{1, 2, \\dots, k\\}) = Q}} (-1)^{\\sigma} (\\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{\\gamma (i), \\sigma(i)}) (\\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{\\gamma (i), \\sigma(i)}) \\\\ \u0026amp;= \\sum_{\\substack{\\sigma \\in S_n; \\\\ (\\sigma \\circ \\gamma)(\\{1, 2, \\dots, k\\}) = Q}} (-1)^{\\sigma \\circ \\gamma} (\\prod_{i \\in \\{1, 2, \\dots, k\\}} a_{\\gamma (i), (\\sigma \\circ \\gamma)(i)}) (\\prod_{i \\in \\{k + 1, k + 2, \\dots, n\\}} b_{\\gamma (i), (\\sigma \\circ \\gamma)(i)}) \\\\ \u0026amp; (\\text{here, we have substituted } \\sigma \\circ \\gamma \\text{ for } \\sigma \\text{ in the sum, since the map } S_n \\to S_n \\text{, } \\sigma \\mapsto \\sigma \\circ \\gamma \\text{ is a bijection}) \\\\ \u0026amp;= \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} \\cdot (-1)^{\\gamma} (\\prod_{i \\in P} a_{i, \\sigma(i)})(\\prod_{i \\in \\tilde{P}} b_{i, \\sigma(i)}) \\\\ \u0026amp;= (-1)^{\\gamma} \\cdot \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i \\in P} a_{i, \\sigma(i)})(\\prod_{i \\in \\tilde{P}} b_{i, \\sigma(i)}) , \\end{aligned} $$\nwe obtain\n$$ \\begin{aligned} \u0026amp; (-1)^{(1 + 2 + \\dots + k) + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} B) \\\\ \u0026amp;= (-1)^{\\gamma} \\cdot \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i \\in P} a_{i, \\sigma(i)})(\\prod_{i \\in \\tilde{P}} b_{i, \\sigma(i)}) . \\end{aligned} $$\nMultiplying both sides of this equality by $ (-1)^{\\sum P - (1 + 2 + \\dots + k)} $, we obtain\n$$ \\begin{aligned} \u0026amp; (-1)^{\\sum P - (1 + 2 + \\dots + k)} (-1)^{(1 + 2 + \\dots + k) + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} B) \\\\ \u0026amp;= (-1)^{\\sum P - (1 + 2 + \\dots + k)} (-1)^{\\gamma} \\cdot \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i \\in P} a_{i, \\sigma(i)})(\\prod_{i \\in \\tilde{P}} b_{i, \\sigma(i)}) \\\\ \u0026amp; (\\text{here, we have } \\gamma = \\sum P - (1 + 2 + \\dots + k)) \\\\ \u0026amp;= \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i \\in P} a_{i, \\sigma(i)})(\\prod_{i \\in \\tilde{P}} b_{i, \\sigma(i)}) \\end{aligned} $$\nThus,\n$$ \\begin{aligned} \u0026amp; \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i \\in P} a_{i, \\sigma(i)})(\\prod_{i \\in \\tilde{P}} b_{i, \\sigma(i)}) \\\\ \u0026amp;= (-1)^{\\sum P - (1 + 2 + \\dots + k)} (-1)^{(1 + 2 + \\dots + k) + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} B) \\\\ \u0026amp;= (-1)^{\\sum P + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} B) \\end{aligned} $$\nThis proves Lemma 0.6.7.\nFor the sake of convenience, let us restate a simplified particular case of Lemma 0.6.7 for $ A = B $:\nLemma 0.6.8\nLet $ n \\in \\mathbf{N} $. For any subset $ I $ of $ \\{1, 2, \\dots, n\\} $, we let $ \\tilde{I} $ denote the complement $ \\{1, 2, \\dots, n\\} \\setminus I $ of $ I $.\nLet $ A = a_{i, j}^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $ be an $ n \\times n $ matrix. Let $ P $ and $ Q $ be two subsets of $ \\{1, 2, \\dots, n\\} $ such that $ |P| = |Q| $. Then,\n$$ \\begin{aligned} \u0026amp; \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) \\\\ \u0026amp;= (-1)^{\\sum P + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} A). \\end{aligned} $$\nProof of Lemma 0.6.8\nEvery $ \\sigma \\in S_n $ satisfies\n$$ \\prod_{i = 1}^{n} = (\\prod_{i \\in P} a_{i, \\sigma(i)}) (\\prod_{i \\in \\tilde{P}} a_{i, \\sigma(i)}) . $$\nNow,\n$$ \\begin{aligned} \u0026amp; \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) \\\\ \u0026amp;= \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i \\in P} a_{i, \\sigma(i)}) (\\prod_{i \\in \\tilde{P}} a_{i, \\sigma(i)}) \\\\ \u0026amp;= (-1)^{\\sum P + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} A). \\end{aligned} $$\nThis proves Lemma 0.6.8.\nWe can now step to the proof of Laplace expansion in multiple rows/columns (a):\nWrite the $ n \\times n $ matrix $ A $ in the form $ A = a_{i,j}^{1 \\leq i \\leq n, 1 \\leq j \\leq n} $. If $ P $ and $ Q $ are two subsets of $ \\{1, 2, \\dots, n\\} $ satisfying $ |Q| \\neq |P| $, then\n$$ \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) = 0 . $$\nIndeed, the map $ \\sigma: \\{1, 2, \\dots, n\\} \\to \\{1, 2, \\dots, n\\} $ is injective, so the set $ \\sigma (P) $ is a subset of $ \\{1, 2, \\dots, n\\} $ satisfying $ |\\sigma (P)| = |P| $. Hence, $ |P| = \\sigma (P) = |Q| \\neq |P| $. This is absurd. Hence, we have found a contradiction.\nNow, forget that we fixed $ \\sigma $. We thus have found a contradiction for every $ \\sigma \\in S_n $ satisfying $ \\sigma (P) = Q $. Thus, there exists no $ \\sigma \\in S_n $ satisfying $ \\sigma (P) = Q $. Hence, the sum $ \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma (P) = Q}} (−1)^{\\sigma} \\prod_{i = 1}^{n} a_{i, \\sigma (i)} $ is an empty sum. Thus\n$$ \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) = (\\text{empty sum}) = 0 . $$\nLet $ P $ be a subset of $ \\{1, 2, \\dots, n\\} $. Then,\n$$ \\begin{aligned} \\operatorname{det} A \u0026amp;= \\sum_{\\sigma \\in S_n} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) = \\sum_{Q \\subseteq \\{1, 2, \\dots, n\\}} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) \\\\ \u0026amp;= \\sum_{\\substack{Q \\subseteq \\{1, 2, \\dots, n\\}; \\\\ |Q| = |P|}} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) + \\sum_{\\substack{Q \\subseteq \\{1, 2, \\dots, n\\}; \\\\ |Q| \\neq |P|}} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) \\\\ \u0026amp; (\\text{since every subset } Q \\text{ of } \\{1, 2, \\dots, n\\} \\text{ satisfies either } |Q| = |P| \\text{ or } |Q| \\neq |P|. \\text{ When } |Q| \\neq |P| \\text{, this is empty sum.}) \\\\ \u0026amp;= \\sum_{\\substack{Q \\subseteq \\{1, 2, \\dots, n\\}; \\\\ |Q| = |P|}} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) + \\sum_{\\substack{Q \\subseteq \\{1, 2, \\dots, n\\}; \\\\ |Q| \\neq |P|}} 0 \\\\ \u0026amp;= \\sum_{\\substack{Q \\subseteq \\{1, 2, \\dots, n\\}; \\\\ |Q| = |P|}} \\sum_{\\substack{\\sigma \\in S_n; \\\\ \\sigma(P) = Q}} (-1)^{\\sigma} (\\prod_{i = 1}^{n} a_{i, \\sigma(i)}) \\\\ \u0026amp;= \\sum_{\\substack{Q \\subseteq \\{1, 2, \\dots, n\\}; \\\\ |Q| = |P|}} (-1)^{\\sum P + \\sum Q} \\operatorname{det} (\\operatorname{sub} \\substack{w(Q) \\\\ w(P)} A) \\operatorname{det} (\\operatorname{sub} \\substack{w(\\tilde{Q}) \\\\ w(\\tilde{P})} A) . \\end{aligned} $$\nThis proves Laplace expansion in multiple rows/columns (a).\n","date":"November 20, 2024","hero":"/posts/miscellanea/matrix-base/images/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/matrix-base/","summary":"\u003ch2 id=\"01-determinant\"\u003e0.1 Determinant\u003c/h2\u003e\n\u003ch3 id=\"theorem--detab--detadetb-\"\u003eTheorem: $ det(AB) = det(A)det(B) $\u003c/h3\u003e\n\u003cp\u003eLet $ A, B $ be a square matrices of order n.\u003c/p\u003e\n\u003cp\u003eLet $ det(A) $ be the determinant of $ A $.\u003c/p\u003e\n\u003cp\u003eLet $ AB $ be the matrix product of $ A $ and $ B $.\u003c/p\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cp\u003e$ det(AB) = det(A)det(B) $\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProof\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe proof I provide is based on \u003ca href=\"https://proofwiki.org/wiki/Determinant_of_Matrix_Product\" target=\"_blank\" rel=\"noopener\"\u003eDeterminant of Matrix Product\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eConsider two cases:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e$ A $ is singular.\u003c/p\u003e","tags":null,"title":"Matrix Base"},{"categories":null,"contents":"Magnetic MIMO: how to charge your phone in your pocket Motivation Current wireless chargers need to remember to regularly charge our mobile phones which is not the wireless charging we hoped for. We would like to have our cell phones charged in our pockets, and never again worry about forgetting to charge the phone. MIMO RF techniques power phones remotely, however, delivering a large amount of power via radiation can cause local heating inside the human body. Contribution They propose a novel design that focuses the magnetic flux from multiple coils in a steerable beam and points it at the phone, in a manner analogous to multi-antenna beamforming in wireless communications. The design can charge unmodified smart phones at distances up to 40 cm, and works regardless of the phone orientation with respect to the charging pad. Magnetic-Beamforming Figure 1: Illustration of the analogies between standard MIMO and MagMIMO. For a 2-antenna MIMO transmitter.\nThe receiver current $I_L$ is analogous to the received signal y in a MIMO communication system.\nThus, similar to traditional MIMO beamforming, we can maximize the current induced in the receiver\u0026rsquo;s coil, $I_L$, by scaling the current flowing in the transmitter\u0026rsquo;s coils $I_S$ with a magnetic-beamforming vector.\nDerivation of beamforming vector The derivation process refers to the article Maximum Ratio Combining (MRC)\nSetup Consider a wireless link with 2 Tx antennas and 1 Rx antenna. The expressions for the signals at the Rx is given by\n$$ r_1 = h_1 \\cdot s + noise $$\n$$ r_2 = h_2 \\cdot s + noise $$\ns is a symbol of data transmitted, h is the flat fading channel gain between the Tx antenna and the first receive antenna, which is usually modeled as complex Gaussian random variables.\nIntuitive thoughts selection combining: simply scan the power arriving at each antenna and choose the one with the highest SNR. However, selecting the antenna with the best SNR implies that energy at the other antennas is neglected.\nsimple summation: sum the signal from all these antennas. Recall that the channel gains $h_1$ and $h_2$ are complex Gaussian random variables here. This in turn implies that their magnitudes are Rayleigh distributed and phases are uniformly distributed between 0 and 2π. Therefore, adding several complex numbers with random phases tends to average out the summation!\nmaximum ratio combining: weighted sum of the signal from all these antennas.\nComputing the Weights $w_i$ A Maximum Ratio Combining (MRC) receiver is formed through a linear combination of $r_1$ and $r_2$ after weighting them with complex scalars $w_1$ and $w_2$, respectively. The output signal $ z $ is given by\n$$ z = w_1 \\cdot r_1 + w_2 \\cdot r_2 + noise $$\n$$ z = w_1 \\cdot h_1 \\cdot s + w_2 \\cdot h_2 \\cdot s + noise $$\n$$ z = s \\cdot { w_1 \\cdot h_1 + w_2 \\cdot h_2 } + noise $$\nThe general expression for $N_R$ antennas can be written as\n$$ \\tag{1} z = s \\sum_{j=1}^{N_R} w_j \\cdot h_j + noise $$\nEach channel gain $ h_j $ is a complex number with magnitude $ |h_j| $ and phase $ ∡h_j $. Magnitudes and phases of $ w_j $ can be represented in a similar manner. Then, we can write their complex product (in which magnitudes are multiplied while phases are added) below. In complex notation, this is written as\n$$ \\tag{2} w_j \\cdot h_j = |w_j| \\cdot |h_j| \\cdot e^{j(∡w_j + ∡h_j)} $$\nCanceling the Phase If the weight $w_j$ has an opposite phase as compared to that of $h_j$, we have $∡w_j = −∡h_j$ and the complex multiplication will cancel out the phases, see equation (2). On the other hand, noise samples simply add without any phase alignment. With this substitution, the combiner output from equation (1) becomes\n$$ \\tag{3} z = s \\sum_{j=1}^{N_R} |w_j| \\cdot |h_j| + noise $$\nwhere the effect of scaling by $w_j$ on the noise samples is ignored here. This phase cancellation is shown on the left of the figure below. All channel gains are said to be aligned at the Rx in the same direction (all are 0° angle). Since we have not yet modified the magnitude of $w_j$, all $w_j$ have unit magnitude here. This scheme is known as Equal Gain Combining (EGC).\nGrading the Magnitude It is obvious that the branches with better SNR provide a more reliable contribution towards making the modulation symbol decision. Therefore, the branches with higher SNRs should be given more weight as compared to the ones with less signal energy. In the algorithm, this can be accomplished by choosing $|w_j|$ the same as $ \\frac{|h_j|}{\\sqrt{\\sum_{j=1}^{N_R} |h_j|^2}} $ which is a measure of confidence for the channel gain at i-th Rx antenna.\nIn summary, since the conjugate of a complex number inverts its phase but leaves the magnitude unchanged, the optimal $w_j$ can be chosen as complex conjugates of $h_j$, normalized by $\\sqrt{\\sum_{j=1}^{N_R} |h_j|^2}$. $w_j = \\frac{h_j^*}{\\sqrt{\\sum_{j=1}^{N_R} |h_j|^2}}$ When the magnitudes of $w_j$ are chosen according to the above expression, equation (3) becomes\n$$ z = s \\frac{1}{\\sqrt{\\sum_{j=1}^{N_R} |h_j|^2}} \\sum_{j=1}^{N_R} |h_j|^2 + noise $$\nLike vote of country\u0026rsquo;s ideology, everyone should vote in such decisions and every vote should be weighted (on a scale from 0 to 1) in proportion to the person\u0026rsquo;s expertise in the relevant matter.\nWireless Power Hotspot that Charges All of Your Devices Motivation Academic research has taken important steps towards wirelessly delivering power to multiple receivers using magnetic coupling. They assume the receiver coil is aligned with the transmitter coil, and do not deal with different receiver orientations with respect to the transmitter. However, the user cannot benefit from an increase in charging distance if she has to hold her device on top of the charging pad and maintain a perfect alignment with the charger.\nWhile MagMIMO could wirelessly charge our device up to 40 cm and in flexible orientations, it remains limited to a single device at a time.\nWhile one work demonstrates the potential of multi-user charging, it presents an optimal solution only for a single receiver, and uses brute force exploration to determine the optimal solution for two receivers.\nContribution MultiSpot formulates the multi-receiver magnetic charging problem and derives a solution whose equations account for inter-receiver interactions.\nMultiSpot\u0026rsquo;s magnetic coupling can charge mobile phones and wearables up to 50 cm and in flexible orientations.\nMulti-Coil System Figure 2: Circuit Schematic and Denotations of a Multi-Coil Tx, Multiple Rx Wireless Power Delivery System. Receiver u\u0026rsquo;s circuit equation:\n$$ \\tag{4} I_{R_u} Z_{R_u} + \\sum_{v \\neq u} jw M_{R_{uv}} I_{R_v} = \\sum_i jw M_{iu} I_{T_i} $$\nBecause $M$ is Tx-Rx magnetic couplings, for $ \\sum_i jw M_{iu} I_{T_i} $, M\u0026rsquo;s i is leading u.\nThe transmitter voltage at coil i is:\n$$ \\tag{5} V_{T_i} = Z_{T_i} I_{T_i} + \\sum_{k \\neq i} jw M_{T_{ik}} I_{T_k} - \\sum_u jw M_{iu} I_{R_u} $$\nFor convenience, we rewrite equation (4) and equation (5) in matrix form:\nRx Equation:\n$$ Z_R \\vec{i_R} = jw M \\vec{i_T} $$\n$$ \\tag{6} \\vec{i_R} = jw Z_R^{-1} M \\vec{i_T} $$\nTx Equation:\n$$ \\vec{v_T} = Z_T \\vec{i_T} - jw M^{\\top} \\vec{i_R} $$\nsubstituting $\\vec{i_R}$ using equation (6).\n$$ \\vec{v_T} = Z_T \\vec{i_T} + w^2 M^{\\top} Z_R^{-1} M \\vec{i_T} $$\n$$ \\tag{7} \\vec{v_T} = (Z_T + w^2 M^{\\top} Z_R^{-1} M) \\vec{i_T} $$\nProve of $ \\Delta \\vec{v_T} $ differs between the algorithm and the appendix in algorithm,\n$$ \\Delta \\vec{v_T} = (Z_T + \\tilde{Y}) \\Delta \\vec{i_T} $$\nBecause\n$$ \\Delta \\vec{i_T} = \\vec{i_T^{bf}} - \\vec{i_T} $$\nSo\n$$ \\Delta \\vec{v_T} = (Z_T + \\tilde{Y}) (\\vec{i_T^{bf}} - \\vec{i_T}) $$\nin algorithm\n$$ \\vec{v_T^{bf}} = (Z_T + \\tilde{Y}) \\vec{i_T^{bf}} $$\nbecause had applied $ \\vec{v_T^{bf}} $ in the circuits, in fact, also,\n$$ \\vec{v_T^{bf}} = (Z_T + Y) \\vec{i_T} $$\nusing above two equtions,\n$$ \\Delta \\vec{v_T} = \\vec{v_T^{bf}} - (Z_T + \\tilde{Y}) \\vec{i_T} $$\n$$ \\Delta \\vec{v_T} = (Z_T + Y) \\vec{i_T} - (Z_T + \\tilde{Y}) \\vec{i_T} $$\n$$ \\Delta \\vec{v_T} = (Y - \\tilde{Y}) \\vec{i_T} $$\nWhich is identical to the expression of $ \\Delta \\vec{v_T} $ in appendix C.\nUnderstanding of Proof of Theorem 4.3 $$ rank(Y - \\tilde{Y} - \\frac{\\Delta \\vec{v_T}\\Delta \\vec{v_T^{\\top}}}{\\Delta \\vec{v_T^{\\top}} \\vec{i_T}}) \\leq rank(Y - \\tilde{Y}) - 1 $$\n$$ rank(Y - \\tilde{Y} - \\frac{\\Delta \\vec{v_T}\\Delta \\vec{v_T^{\\top}}}{\\Delta \\vec{v_T^{\\top}} \\vec{i_T}} - \\frac{\\Delta \\vec{v_T}\\Delta \\vec{v_T^{\\top}}}{\\Delta \\vec{v_T^{\\top}} \\vec{i_T}}) \\leq rank(Y - \\tilde{Y} - \\frac{\\Delta \\vec{v_T}\\Delta \\vec{v_T^{\\top}}}{\\Delta \\vec{v_T^{\\top}} \\vec{i_T}}) - 1 \\leq rank(Y - \\tilde{Y}) - 1 - 1 $$\n$$ \\cdots $$\nWhen $ rank(Y - \\tilde{Y}) = 0 $, $ Y - \\tilde{Y} = O $, $ Y = \\tilde{Y} $.\n","date":"November 7, 2024","hero":"/posts/wpt/magmimohotspot/images/WPTscenario.gif","permalink":"https://qingbo12.github.io/posts/wpt/magmimohotspot/","summary":"\u003ch2 id=\"magnetic-mimo-how-to-charge-your-phone-in-your-pocket\"\u003eMagnetic MIMO: how to charge your phone in your pocket\u003c/h2\u003e\n\u003ch3 id=\"motivation\"\u003eMotivation\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eCurrent wireless chargers\u003c/strong\u003e need to remember to regularly charge our mobile phones which \u003cstrong\u003eis not the wireless charging we hoped for\u003c/strong\u003e. We would like to have our cell phones charged in our pockets, and never again worry about forgetting to charge the phone.\u003c/li\u003e\n\u003cli\u003eMIMO RF techniques power phones remotely, however, delivering a large amount of power via radiation can cause local heating inside the human body.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"contribution\"\u003eContribution\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eThey propose a novel design that focuses the magnetic flux from multiple coils in a steerable beam and points it at the phone, in a manner analogous to multi-antenna beamforming in wireless communications.\u003c/li\u003e\n\u003cli\u003eThe design can charge unmodified smart phones \u003cstrong\u003eat distances up to 40 cm, and works regardless of the phone orientation\u003c/strong\u003e with respect to the charging pad.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"magnetic-beamforming\"\u003eMagnetic-Beamforming\u003c/h3\u003e\n\u003cdiv\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/AnalogiesBetweenStandardMIMOandMagMIMO.png\" alt=\"Analogies Between Standard MIMO and Magnetic MIMO\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003eFigure 1:\u003c/strong\u003e Illustration of the analogies between standard MIMO and MagMIMO.\u003c/center\u003e\n\u003c/div\u003e\n\u003cbr\u003e\n\u003cp\u003eFor a 2-antenna MIMO transmitter.\u003c/p\u003e","tags":null,"title":"MagMIMOHotspot Read"},{"categories":null,"contents":"Basic circuit Charge It is defined in terms of the ampere by counting the total charge that passes through an arbitrary cross section of a wire during an interval of one second. In the SI system, the fundamental unit of charge is the coulomb (C).\nCurrent We define the current at a specific point and flowing in a specified direction as the instantaneous rate at which net positive charge is moving past that point in the specified direction. Current is symbolized by I or i, and so\n$$i = \\frac{dq}{dt}$$\nThe unit of current is the ampere (A).\nVoltage In a static electric field, it corresponds to the work needed per unit of charge to move a positive test charge from the first point to the second point. Voltage is symbolized by U or u, and so\n$$u = \\frac{dW}{dq}$$\nIn the International System of Units (SI), the derived unit for voltage is the volt (V).\nPower Power is defined as the rate at which work is done or energy is expended. The absorbed power must be proportional both to the number of coulombs transferred per second (current) and to the energy needed to transfer one coulomb through the element (voltage). Thus,\n$$p = vi$$\nThe fundamental unit of power is the watt (W), defined as 1 J/s.\nEnergy Recalling that power is the rate of work, energy (w) is defined as\n$$w(t) = \\int_{t_0}^t p dt = \\int_{t_0}^t vi dt$$\nThe SI unit of energy is the joule (J).\nNoting that energy is the product of power and time (1 joule = 1 watt × 1 second), it is also convenient to define energy in terms of watt hours (Wh).\n$$1 Wh = 3600J$$\nBattery capacity (energy stored) can also be defined in terms of Wh. Since the voltage on a battery is constant, it becomes convenient to separate out the battery voltage and simply refer to the total charge storage on the battery (Q). Thus,\n$$w = \\int vidt = V \\int idt = VQ$$\nThe total charge Q is given in units of amp hours (Ah)\n$$ 1 Ah = 3600 C$$\nCapacitance\nFigure 1: Electrical symbol and current–voltage conventions for a capacitor. $$i = C \\frac{dv}{dt}$$\nSuppose $ v \u0026gt; 0 $.\nIf we charge the capacitor, then $ v $ is increasing. So $\\frac{dv}{dt} \u0026gt; 0$, $ i \\gt 0 $, current is from high voltage to low voltage.\nIf the capacitor is release charges, then $ v $ is decreasing. So $ i \\lt 0 $, current is from low voltage to high voltage.\nInductor\nFigure 2: Electrical symbol and current–voltage conventions for an inductor. $$v = L \\frac{di}{dt}$$\nSuppose $ i \u0026gt; 0 $.\nIf we charge the inductor, then $ i $ is increasing. So $\\frac{di}{dt} \u0026gt; 0$, $ v \u0026gt; 0 $, charges move from high voltage to low voltage, the indcutor is absorb energy.\nIf the capacitor is release charges, then $ i $ is decreasing. So $ v \u0026lt; 0 $, charges move from low voltage to high voltage, the inductor is release energy.\nEnglish-Chinese glossary resistor 电阻（电路元件）\nresistance 电阻\ncapacitor 电容（电路元件）\ncapacitance 电容\ninductor 电感（电路元件）\ninductance 电感\n$$\\omega_0 = \\frac{1}{\\sqrt{LC}}$$\nresonant frequency 谐振频率 $\\omega_0$\n$$v(t) = V_m sin(\\omega t + \\theta)$$\nsinusoids 正弦波\namplitude 幅度 $V_m$\nargument 幅角 $\\omega t + \\theta$\nradian frequency/angular frequency 角频率 $\\omega$\nphase angle 相角 $\\theta$\n$$I_m \\angle \\phi = I_m cos (\\omega t + \\phi)$$\nphasor 相量 $I_m \\angle \\phi$\n$$\\mathbf{Z} = R + jX$$\n$$\\mathbf{Z_R} = R, \\mathbf{Z_L} = j \\omega L, \\mathbf{Z_C} = \\frac{1}{j \\omega C}$$\nimpedance 阻抗 $\\mathbf{Z}$\nreactance 电抗 X (in $\\mathbf{Z} = R + jX$)\n$$\\mathbf{Y} = G + jB = \\frac{1}{\\mathbf{Z}} = \\frac{1}{R + jX}$$\nadmittance 导纳 $\\mathbf{Y}$\nconductance 电导 G\nsusceptance 电纳 B\n","date":"November 7, 2024","hero":"/posts/miscellanea/circuit/images/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/circuit/","summary":"\u003ch2 id=\"basic-circuit\"\u003eBasic circuit\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eCharge\u003c/strong\u003e It is defined in terms of the ampere by counting the total charge that passes through an arbitrary cross section of a wire during an interval of one second. In the SI system, the fundamental unit of charge is the coulomb (C).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCurrent\u003c/strong\u003e We define the current at a specific point and flowing in a specified direction as the instantaneous rate at which net positive charge is moving past that point in the specified direction. Current\nis symbolized by I or i, and so\u003c/p\u003e","tags":null,"title":"Circuit Base"},{"categories":null,"contents":"Scheduling Layer PTE Optimization Requirement-Driven Magnetic Beamforming for MIMO Wireless Power Transfer Optimization Motivation: In MIMO MRC-WPT system, we need to consider the scenario when different RXs have different requirements of power. Problem formulation: weighted sum-power maximization $WSPMax = max_{{i_n^{tx}}} \\sum_q w_q |i^{rx}_q|^2 r^{rx}_q $\nSubject to: $$ \\sum_n \\left|i_n^{tx}\\right|^2 r_n^{tx} + \\sum_q \\left|i_q^{rx}\\right|^2 r_q^{rx} \\leq P_{\\text{max}}, \\ $$\n$$ \\left|i_n^{tx}\\right|^2 \\leq A_n^{tx}, \\quad \\forall 1 \\leq n \\leq N, \\ $$\n$$ \\left|i_q^{rx}\\right|^2 \\leq A_q^{tx}, \\quad \\forall 1 \\leq q \\leq Q, \\ $$\n$$ \\left|v_n^{tx}\\right|^2 \\leq V_n^{tx}, \\quad \\forall 1 \\leq n \\leq N. $$\nJoint Power Routing and Current Scheduling in Multi-Relay Magnetic MIMO WPT System Motivation: Relay\u0026rsquo;s ability to increase charging distance in practical systems is studied. A three-coil wireless power transfer system is more energy efficient than a two coil counterpart is feasible. But the relay On/Off based power routing algorithm can be optimized for system performance while considering the MIMO scenario. Contribution:\nThey propose an RX-independent algorithm, i.e., without any RX cooperation. RX cooperation, i.e., RX feedback communication is time/energy-consuming since it is achieved by using side-channels, e.g., Bluetooth.\nThey propose an efficient searching algorithm other than enumerating all the possible relay On-Off states. PDL Optimization IMP: Impedance Matching Enhanced Power-Delivered-to-Load Optimization for Magnetic MIMO Wireless Power Transfer System Motivation: The power delivered to load(PDL) maximization related issues and the two impedance mismatching phenomena need to be further explored. The first one is the PTE-PDL contradiction phenomenon. When RXs are closely coupled with TXs, the PTE is very high but the achieved PDL is quite limited. The second one is the isolated RX-pair phenomenon. When two RXs are closely coupled, the RX pair forms an \u0026ldquo;isolated island\u0026rdquo; where the energy can not reach, i.e., their PTE and PDL are both limited. Contribution:\nFirst work to maximize the PDL in practical MIMO MRC-WPT systems with bounded TX voltages, together with addressing the impedance mismatching phenomena. Research about maximize the PDL in practical MIMO MRC-WPT systems is existing. Maximize the PDL and addressing the impedance mismatching phenomena in SISO systems is existing.\nIn order to address the two observed phenomena, their basic idea is to handle the first one by tuning TX-RX coupling through special designed adjustable TX coils. To deal with the second one, they use RX grouping mechanism to select not coupled RX pairs to charge. Then, they conduct time scheduling among the chosen RX groups, i.e., each RX group is assigned a unique time slice for charging. Thoughts: Already have many optimization methods. Ether optimize with a new variable or use more efficient algorithm.\nEMR safety optimization Shield: Safety Ensured High-efficient Scheduling for Magnetic MIMO Wireless Power Transfer System Motivation: Ensure electromagnetic radiation (EMR) safety is important: Exposure to high EMR has its potential risks including tissue impairment, brain tumor and mental diseases. Not be investigated before: EMR safety related problems have not been thoroughly investigated. While MagMIMO and MultiSpot systems are likely to contravene EMR safety standards.\nContribution: They propose the first scheme for safety ensured charging in MIMO MRC-WPT systems.\nSafety Guaranteed Power-Delivered-to-Load Maximization for Magnetic Wireless Power Transfer Motivation: Current method is not very good: Shield has two deficiencies. On one hand, it is not a one-size-fits-all scheme, i.e., the derived EMR model can only be suitable for the special circular type of resonator coils. On the other hand, Shield focus on PTE maximization which is less meaningful than PDL maximization.\nContribution: They use a unified model to describe the EMR distribution of all the different coils.\nCommunication Layer Parallel Feedback Communications for Magnetic MIMO Wireless Power Transfer System Motivation: Communications is important: The RX feedback communication and power transfer channel estimation schemes enables the systems for charging in demand, magnetic beamforming for better power transfer efficiency, and so on. Current communication scheme is not efficient. However, most of the existing researches focus on the optimization algorithm for TX currents/voltages adjustment, and adopt simple feedback communication and channel estimation schemes. The existing schemes including omitting the communication and channel estimation stages, simply assuming the existence of communication links and pre-known channel conditions, or some naive method.\nContribution:\nCluster phenomenon: two TXs\u0026rsquo; current values are clearly clustered into four kinds, which are exactly correspondence to the four combined open-short states of the two RXs.\nThey propose a collision-aware parallel communication scheme to identify the open-short state of all the RXs.\nCamel: Context-Aware Magnetic MIMO Wireless Power Transfer with In-band Communication Motivation: There are three different receiver (RX) information collection methods. The RX-independent method does not consider the RX information collection at all. The out-band based methods use additional out-band devices, along with extra energy consumption and spectrum occupation. For the in-band communication methods, some are not used in MIMO scenario. Hua et al.\u0026rsquo;s method need to collect enough combined states before decoding.\nContribution: investigate the signals\u0026rsquo; feature from the view of mutual inductance. Roland: Robust In-band Parallel Communication for Magnetic MIMO Wireless Power Transfer System Motivation: Camel has a premise of ignoring RX-RX coupling when performing RX-level channel decomposition which may not work well under the scenarios where closely coupled RXs exist. RX-RX coupling can exist as relay phenomenon.\nApplication Layer FreAuth: Novel Frequency Feature-Based Device Authentication for Magnetic Wireless Charging Motivation: Device Authentication is important: Threats like freeloading, illegal device attacks need to be mitigated. Therefore, it is essential to identify and authenticate legitimate devices in magnetic WPT systems to prevent illegal access and ensure smooth usage. Current methods are not good: Uploading its identity is vulnerable to forgery as electromagnetic signals are exposed, and the communication protocol is unencrypted. Hardware fingerprint-based device authentication can be divided to two categories. Extracting device-specific characteristics from electromagnetic signals lacks permanence; Using temporal features such as oscillator difference caused clock skew is unstable and requires a long authentication process.\nContribution:\nThey observed that each RX has its own frequency-related features. The proposed method is completely RX-independent, i.e., without requiring communication or cooperation from the receiver. ","date":"October 31, 2024","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/posts/wpt/first-glance/","summary":"\u003ch1 id=\"scheduling-layer\"\u003eScheduling Layer\u003c/h1\u003e\n\u003ch3 id=\"pte-optimization\"\u003ePTE Optimization\u003c/h3\u003e\n\u003ch4 id=\"requirement-driven-magnetic-beamforming-for-mimo-wireless-power-transfer-optimization\"\u003eRequirement-Driven Magnetic Beamforming for MIMO Wireless Power Transfer Optimization\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eMotivation:\u003c/strong\u003e In MIMO MRC-WPT system, we need to consider the scenario when different RXs have different requirements of power.\n\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProblem formulation:\u003c/strong\u003e weighted sum-power maximization\n$WSPMax = max_{{i_n^{tx}}} \\sum_q w_q |i^{rx}_q|^2 r^{rx}_q $\u003c/p\u003e\n\u003cp\u003eSubject to:\n\u003cbr/\u003e\u003c/p\u003e\n\u003cp\u003e$$ \\sum_n \\left|i_n^{tx}\\right|^2 r_n^{tx} + \\sum_q \\left|i_q^{rx}\\right|^2 r_q^{rx} \\leq P_{\\text{max}}, \\ $$\u003c/p\u003e\n\u003cp\u003e$$ \\left|i_n^{tx}\\right|^2 \\leq A_n^{tx}, \\quad \\forall 1 \\leq n \\leq N, \\ $$\u003c/p\u003e","tags":null,"title":"First glance"},{"categories":null,"contents":"Fusion stage Figure 1: Mind map of Deep Multimodal Learning Inference mechanism for multi-sensors fusion Figure 2: Mind map of A comparative review on multi-modal sensors fusion based on deep learning ","date":"October 16, 2024","hero":"/posts/miscellanea/multimodal-fusion/images/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/multimodal-fusion/","summary":"\u003ch2 id=\"fusion-stage\"\u003eFusion stage\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/mindmap-of-DeepMultimodalLearning.png\" \n    alt=\"Mind map of Deep Multimodal Learning\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003eFigure 1:\u003c/strong\u003e Mind map of Deep Multimodal Learning\u003c/center\u003e\n\u003c/div\u003e\n\u003ch2 id=\"inference-mechanism-for-multi-sensors-fusion\"\u003eInference mechanism for multi-sensors fusion\u003c/h2\u003e\n\u003cdiv\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/mindmap-of-InferenceForFusion.png\" alt=\"Mind map of Inference For Fusion\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003eFigure 2:\u003c/strong\u003e Mind map of A comparative review on multi-modal sensors fusion based on deep learning\u003c/center\u003e\n\u003c/div\u003e","tags":null,"title":"Multimodal-fusion research"},{"categories":null,"contents":"个人情况 本科： 重庆大学\n专业： 计算机科学与技术\n排名： 1.3% (rank 3)\n英语： 六级 547\n荣誉： 国家奖学金、重庆市三好学生\n竞赛： 无竞赛\n科研： 有 CV 领域相关经历，面试时有 NIPS 二作在投\n最终去向： 中国科学技术大学-计算机科学与技术学院\n夏令营情况 院校 类型 入营 offer 南京大学-计算机科学与技术学院 硕士 √ 候补 清华大学-深圳国际研究生院(数据与信息研究院人工智能项目) 硕士 × 清华大学-网络研究院 硕士 × 南京大学-人工智能学院 硕士 × 中国人民大学-高瓴人工智能学院 直博 √ 时间冲突，放弃参营 上海交通大学-人工智能学院 直博 √ 不合格（斩杀） 中国科学技术大学-计算机科学与技术学院 硕士 √ 学硕 南京大学-计算机科学与技术学院 线上测试过了，线下机试 0 分。。。\n南京大学-人工智能学院 入了计院，不让入人工智能学院？\n上海交通大学-人工智能学院 没有提前联系好老师，科研积累太少。\n预推免情况 院校 类型 入营 offer 清华大学-软件学院 专硕 √ × 浙江大学-计算机科学与技术学院 硕士 √ 专硕 总结：入营基本都能入，但自身实力差。南大机试报零，面试差导致上交直博不合格。\n建议：\n硕士一般机试需要拿高分，可以通过刷 AcWing 基础和提高课 或者 LeetCode hot 100。\n硕士一般面试需要好好复习数据结构和算法，准备操作系统、计组、计网常见面试题。\n博士一般需要有和导师大方向一致的科研积累，可以提前联系目标院校导师实习。\n","date":"October 10, 2024","hero":"/posts/miscellanea/recommendation-progress/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/recommendation-progress/","summary":"\u003ch2 id=\"个人情况\"\u003e个人情况\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e本科：\u003c/strong\u003e 重庆大学\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e专业：\u003c/strong\u003e 计算机科学与技术\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e排名：\u003c/strong\u003e 1.3% (rank 3)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e英语：\u003c/strong\u003e 六级 547\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e荣誉：\u003c/strong\u003e 国家奖学金、重庆市三好学生\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e竞赛：\u003c/strong\u003e 无竞赛\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e科研：\u003c/strong\u003e 有 CV 领域相关经历，面试时有 NIPS 二作在投\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e最终去向：\u003c/strong\u003e 中国科学技术大学-计算机科学与技术学院\u003c/p\u003e\n\u003ch2 id=\"夏令营情况\"\u003e夏令营情况\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003e院校\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e类型\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e入营\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003eoffer\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e南京大学-计算机科学与技术学院\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e硕士\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e√\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e候补\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e清华大学-深圳国际研究生院(数据与信息研究院人工智能项目)\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e硕士\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e×\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e清华大学-网络研究院\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e硕士\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e×\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e南京大学-人工智能学院\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e硕士\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e×\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e中国人民大学-高瓴人工智能学院\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e直博\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e√\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e时间冲突，放弃参营\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e上海交通大学-人工智能学院\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e直博\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e√\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e不合格（斩杀）\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e中国科学技术大学-计算机科学与技术学院\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e硕士\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e√\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e学硕\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cbr/\u003e\n\u003cp\u003e\u003cstrong\u003e南京大学-计算机科学与技术学院\u003c/strong\u003e\n线上测试过了，线下机试 0 分。。。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e南京大学-人工智能学院\u003c/strong\u003e\n入了计院，不让入人工智能学院？\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e上海交通大学-人工智能学院\u003c/strong\u003e\n没有提前联系好老师，科研积累太少。\u003c/p\u003e\n\u003ch2 id=\"预推免情况\"\u003e预推免情况\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth style=\"text-align: left\"\u003e院校\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e类型\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003e入营\u003c/th\u003e\n          \u003cth style=\"text-align: center\"\u003eoffer\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e清华大学-软件学院\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e专硕\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e√\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e×\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd style=\"text-align: left\"\u003e浙江大学-计算机科学与技术学院\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e硕士\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e√\u003c/td\u003e\n          \u003ctd style=\"text-align: center\"\u003e专硕\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cbr\u003e\n\u003cp\u003e总结：入营基本都能入，但自身实力差。南大机试报零，面试差导致上交直博不合格。\u003c/p\u003e","tags":null,"title":"Recommendation Progress"},{"categories":null,"contents":"怎么和计算机打交道 Figure 1: 早期将穿孔纸带输入计算机 Figure 2: 使用汇编语言编程 Figure 3: 使用高级语言C语言编程 为什么要学 C 语言 基础性强：C 语言是一门底层语言，靠近计算机硬件。学习 C 语言能够帮助你理解计算机是如何工作的，如内存管理、指针操作和硬件交互等。很多高级语言（如 C++、Python）都基于 C 语言。 高效性能：C 语言生成的代码通常比很多高级语言效率更高，特别适合对性能要求高的场景，比如实时系统、游戏引擎等。 。。。 怎么开始学习 C/C++ 语言 首先需要一个写代码的好地方，推荐 VS Code\n安装 VS Code 安装 C/C++ 扩展 安装 MinGW-w64 编译工具，并检查 向编程世界打声招呼 #include \u0026lt;iostream\u0026gt; using namespace std; int main() { cout \u0026lt;\u0026lt; \u0026#34;Hello, World!\u0026#34; \u0026lt;\u0026lt; endl; return 0; } ","date":"September 13, 2024","hero":"/posts/miscellanea/c-lecture/images/image-miscellanea.jpg","permalink":"https://qingbo12.github.io/posts/miscellanea/c-lecture/","summary":"\u003ch3 id=\"怎么和计算机打交道\"\u003e怎么和计算机打交道\u003c/h3\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between;\"\u003e\n    \u003cimg src=\"images/PunchingMachine.jpg\" alt=\"Punching Machine\" style=\"width: 45%\"/\u003e\n    \u003cimg src=\"images/Papertape.jpg\" alt=\"Papertape\" style=\"width: 45%\"/\u003e\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cstrong\u003eFigure 1:\u003c/strong\u003e 早期将穿孔纸带输入计算机\n\u003c/div\u003e\n\u003cbr\u003e\n\u003cdiv style=\"display: flex; justify-content: space-between;\"\u003e\n    \u003cimg src=\"images/asm-01.png\" alt=\"asm\" style=\"width: 45%\"/\u003e\n    \u003cimg src=\"images/riscv.png\" alt=\"riscv\" style=\"width: 45%\"/\u003e\n\u003c/div\u003e\n\u003cdiv style=\"text-align: center;\"\u003e\n    \u003cstrong\u003eFigure 2:\u003c/strong\u003e 使用汇编语言编程\n\u003c/div\u003e\n\u003cbr\u003e\n\u003cdiv\u003e\n    \u003ccenter\u003e\u003cimg src=\"images/c-asm-01.png\" alt=\"c-asm-01\" /\u003e\u003c/center\u003e\n    \u003ccenter\u003e\u003cstrong\u003eFigure 3:\u003c/strong\u003e 使用高级语言C语言编程\u003c/center\u003e\n\u003c/div\u003e\n\u003ch3 id=\"为什么要学-c-语言\"\u003e为什么要学 C 语言\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e基础性强：C 语言是一门底层语言，靠近计算机硬件。学习 C 语言能够帮助你理解计算机是如何工作的，如内存管理、指针操作和硬件交互等。很多高级语言（如 C++、Python）都基于 C 语言。\u003c/li\u003e\n\u003cli\u003e高效性能：C 语言生成的代码通常比很多高级语言效率更高，特别适合对性能要求高的场景，比如实时系统、游戏引擎等。\u003c/li\u003e\n\u003cli\u003e。。。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"怎么开始学习-cc-语言\"\u003e怎么开始学习 C/C++ 语言\u003c/h3\u003e\n\u003cp\u003e首先需要一个写代码的好地方，推荐 VS Code\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e安装 VS Code\u003c/li\u003e\n\u003cli\u003e安装 C/C++ 扩展\u003c/li\u003e\n\u003cli\u003e安装 \u003ca href=\"https://www.msys2.org/\" target=\"_blank\" rel=\"noopener\"\u003eMinGW-w64\u003c/a\u003e 编译工具，并检查\u003c/li\u003e\n\u003cli\u003e向编程世界打声招呼\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-c++\" data-lang=\"c++\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e#include\u003c/span\u003e \u003cspan style=\"color:#75715e\"\u003e\u0026lt;iostream\u0026gt;\u003c/span\u003e\u003cspan style=\"color:#75715e\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003eusing\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003enamespace\u003c/span\u003e std;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eint\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e{\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    cout \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;Hello, World!\u0026#34;\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e endl;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#66d9ef\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#ae81ff\"\u003e0\u003c/span\u003e;\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","tags":null,"title":"C language lecture 1"},{"categories":null,"contents":"Questions What\u0026rsquo;s the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions ? Extended Learning Attention mechanisms with a recurrent network.\nBahdanau, Dzmitry. \u0026ldquo;Neural machine translation by jointly learning to align and translate.\u0026rdquo; arXiv preprint arXiv:1409.0473 (2014).\nUse convolutional neural networks to compute hidden representations in parallel.\nGehring, Jonas, et al. \u0026ldquo;Convolutional sequence to sequence learning.\u0026rdquo; International conference on machine learning. PMLR, 2017.\nWhy is difficult to learn dependencies between distant positions.\nHochreiter, Sepp, et al. \u0026ldquo;Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\u0026rdquo; (2001).\nSelf-attention.\nCheng, Jianpeng. \u0026ldquo;Long short-term memory-networks for machine reading.\u0026rdquo; arXiv preprint arXiv:1601.06733 (2016).\n","date":"September 12, 2024","hero":"/posts/gcd/transformer/cub-dataset-cover.jpg","permalink":"https://qingbo12.github.io/posts/gcd/transformer/","summary":"\u003ch2 id=\"questions\"\u003eQuestions\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eWhat\u0026rsquo;s the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions ?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"extended-learning\"\u003eExtended Learning\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eAttention mechanisms with a recurrent network.\u003c/p\u003e\n\u003cp\u003eBahdanau, Dzmitry. \u0026ldquo;Neural machine translation by jointly learning to align and translate.\u0026rdquo; arXiv preprint arXiv:1409.0473 (2014).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse convolutional neural networks to compute hidden representations in parallel.\u003c/p\u003e\n\u003cp\u003eGehring, Jonas, et al. \u0026ldquo;Convolutional sequence to sequence learning.\u0026rdquo; International conference on machine learning. PMLR, 2017.\u003c/p\u003e","tags":null,"title":"transformer Read"},{"categories":null,"contents":"Questions What\u0026rsquo;s the function of centering $ C $ and sharpening parameter $ \\tau_t $ ? Extended Learning Multi-crop training.\nCaron, Mathilde, et al. \u0026ldquo;Unsupervised learning of visual features by contrasting cluster assignments.\u0026rdquo; Advances in neural information processing systems 33 (2020): 9912-9924.\nUse a noise contrastive estimator (NCE) to compare instances instead of classifying them.\nWu, Zhirong, et al. \u0026ldquo;Unsupervised feature learning via non-parametric instance discrimination.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\nMean Teacher self-distillation.\nTarvainen, Antti, and Harri Valpola. \u0026ldquo;Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.\u0026rdquo; Advances in neural information processing systems 30 (2017).\nknowledge distillation.\nBuciluǎ, Cristian, Rich Caruana, and Alexandru Niculescu-Mizil. \u0026ldquo;Model compression.\u0026rdquo; Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. 2006.\nExtra learnable token in ViT.\nDosovitskiy, Alexey. \u0026ldquo;An image is worth 16x16 words: Transformers for image recognition at scale.\u0026rdquo; arXiv preprint arXiv:2010.11929 (2020).\nDevlin, Jacob. \u0026ldquo;Bert: Pre-training of deep bidirectional transformers for language understanding.\u0026rdquo; arXiv preprint arXiv:1810.04805 (2018).\nAttention mechanism.\nBahdanau, Dzmitry. \u0026ldquo;Neural machine translation by jointly learning to align and translate.\u0026rdquo; arXiv preprint arXiv:1409.0473 (2014).\n","date":"September 6, 2024","hero":"/posts/gcd/dino/cub-dataset-cover.jpg","permalink":"https://qingbo12.github.io/posts/gcd/dino/","summary":"\u003ch2 id=\"questions\"\u003eQuestions\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eWhat\u0026rsquo;s the function of centering $ C $ and sharpening parameter $ \\tau_t $ ?\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"extended-learning\"\u003eExtended Learning\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eMulti-crop training.\u003c/p\u003e\n\u003cp\u003eCaron, Mathilde, et al. \u0026ldquo;Unsupervised learning of visual features by contrasting cluster assignments.\u0026rdquo; Advances in neural information processing systems 33 (2020): 9912-9924.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse a noise contrastive estimator (NCE) to compare instances instead of classifying them.\u003c/p\u003e\n\u003cp\u003eWu, Zhirong, et al. \u0026ldquo;Unsupervised feature learning via non-parametric instance discrimination.\u0026rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\u003c/p\u003e","tags":null,"title":"DINO Read"},{"categories":null,"contents":"Research Task Generalized Category Discovery (GCD) relaxes the assumption of traditional semi-supervised learning by assuming the unlabelled data can also contain different but related categories from the labelled data. The goal of GCD is to learn a model that is able to classify the already-seen categories in the labelled data, and more importantly, jointly discover the new categories in the unlabelled data and make correct classifications.\nLimitations of Previous Methods Previous methods can be categorized into two distinct approaches:\nparametric classifiers.\nHowever, parametric classifiers are prone to overfit to seen categories. This motivates authors to revisit the reason that makes previous parametric classifiers fail to recognise novel classes.\nnon-parametric classifier such as k-means clustering.\nAlbeit obtaining promising results, the non-parametric classifiers suffer from heavy computation costs on large-scale datasets due to quadratic complexity of the clustering algorithm. Besides, unlike a learnable parametric classifier, the non-parametric method loses the ability to jointly optimise the separating hyperplane of all categories in a learnable manner, potentially being sub-optimal.\nFailures of Parametric Classification Comparing Item UNO+ GCD Representation post projector post backbone Decoupled or joint representation learning joint decoupled Prediction bias more less Conclusion:\nPost backbone representation is better.\nJoint representation learning is better.\nReliable pseudo label is count.\nPrediction bias needs to be overcome.\nMethod Representation Learning:\nSupervised contrastive learning on labelled samples, and self-supervised contrastive learning on all samples.\nParametric Classification:\nCross-entropy loss between the pseudo-labels of two views on all samples, and cross-entropy loss between the predictions and ground-truth labels on labelled samples.\nMean-entropy maximisation regulariser for the unsupervised objective to overcome biased predictions.\nExtended Learning Off-the-shelf method to estimate number of categories in $ Y_u $.\nVaze, Sagar, et al. \u0026ldquo;Generalized category discovery.\u0026rdquo; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\nDeep Clustering.\nCaron, Mathilde, et al. \u0026ldquo;Deep clustering for unsupervised learning of visual features.\u0026rdquo; Proceedings of the European conference on computer vision (ECCV). 2018.\nPrototypical classifier.\nSnell, Jake, Kevin Swersky, and Richard Zemel. \u0026ldquo;Prototypical networks for few-shot learning.\u0026rdquo; Advances in neural information processing systems 30 (2017).\nFunction of a projector in self-supervised learning.\nCui, Quan, et al. \u0026ldquo;Discriminability-transferability trade-off: An information-theoretic perspective.\u0026rdquo; European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n","date":"September 2, 2024","hero":"/posts/gcd/simgcd/cub-dataset-cover.jpg","permalink":"https://qingbo12.github.io/posts/gcd/simgcd/","summary":"\u003ch2 id=\"research-task\"\u003eResearch Task\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eGeneralized Category Discovery (GCD)\u003c/strong\u003e relaxes the assumption of traditional semi-supervised learning by assuming the unlabelled data can also contain different but related categories from the labelled data. The goal of GCD is to learn a model that is able to classify the already-seen categories in the labelled data, and more importantly, jointly discover the new categories in the unlabelled data and make correct classifications.\u003c/p\u003e\n\u003ch2 id=\"limitations-of-previous-methods\"\u003eLimitations of Previous Methods\u003c/h2\u003e\n\u003cp\u003ePrevious methods can be categorized into two distinct approaches:\u003c/p\u003e","tags":null,"title":"SimGCD Read"},{"categories":["Basic"],"contents":"Greeting! This is an introduction post. This post tests the followings:\nHero image is in the same directory as the post. This post should be at top of the sidebar. Post author should be the same as specified in author.yaml file. ","date":"June 8, 2020","hero":"/posts/introduction/hero.svg","permalink":"https://qingbo12.github.io/posts/introduction/","summary":"\u003cp\u003eGreeting! This is an introduction post. This post tests the followings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHero image is in the same directory as the post.\u003c/li\u003e\n\u003cli\u003eThis post should be at top of the sidebar.\u003c/li\u003e\n\u003cli\u003ePost author should be the same as specified in \u003ccode\u003eauthor.yaml\u003c/code\u003e file.\u003c/li\u003e\n\u003c/ul\u003e","tags":["Basic","Multi-lingual"],"title":"Introduction"},{"categories":null,"contents":"This is a sample post intended to test the followings:\nA different post author. Table of contents. Markdown content rendering. Math rendering. Emoji rendering. Markdown Syntax Rendering Headings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Inline Markdown In Table italics bold strikethrough code Code Blocks Code block with backticks html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nMath Rendering Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\nEmoji Rendering 🙈 :see_no_evil: 🙉 :hear_no_evil: 🙊 :speak_no_evil:\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"June 8, 2020","hero":"/posts/markdown-sample/hero.svg","permalink":"https://qingbo12.github.io/posts/markdown-sample/","summary":"\u003cp\u003eThis is a sample post intended to test the followings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA different post author.\u003c/li\u003e\n\u003cli\u003eTable of contents.\u003c/li\u003e\n\u003cli\u003eMarkdown content rendering.\u003c/li\u003e\n\u003cli\u003eMath rendering.\u003c/li\u003e\n\u003cli\u003eEmoji rendering.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch1 id=\"markdown-syntax-rendering\"\u003eMarkdown Syntax Rendering\u003c/h1\u003e\n\u003ch2 id=\"headings\"\u003eHeadings\u003c/h2\u003e\n\u003cp\u003eThe following HTML \u003ccode\u003e\u0026lt;h1\u0026gt;\u003c/code\u003e—\u003ccode\u003e\u0026lt;h6\u0026gt;\u003c/code\u003e elements represent six levels of section headings. \u003ccode\u003e\u0026lt;h1\u0026gt;\u003c/code\u003e is the highest section level while \u003ccode\u003e\u0026lt;h6\u0026gt;\u003c/code\u003e is the lowest.\u003c/p\u003e\n\u003ch1 id=\"h1\"\u003eH1\u003c/h1\u003e\n\u003ch2 id=\"h2\"\u003eH2\u003c/h2\u003e\n\u003ch3 id=\"h3\"\u003eH3\u003c/h3\u003e\n\u003ch4 id=\"h4\"\u003eH4\u003c/h4\u003e\n\u003ch5 id=\"h5\"\u003eH5\u003c/h5\u003e\n\u003ch6 id=\"h6\"\u003eH6\u003c/h6\u003e\n\u003ch2 id=\"paragraph\"\u003eParagraph\u003c/h2\u003e\n\u003cp\u003eXerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\u003c/p\u003e","tags":null,"title":"Markdown Samples"},{"categories":["Basic"],"contents":"This sample post tests the followings:\nCategory, sub-category nesting in the sidebar. Hero image and other images are in images folder inside this post directory. Different media rendering like image, tweet, YouTube video, Vimeo video etc. Image Sample Vimeo Video Sample ","date":"June 8, 2020","hero":"/posts/category/sub-category/rich-content/images/forest.jpg","permalink":"https://qingbo12.github.io/posts/category/sub-category/rich-content/","summary":"\u003cp\u003eThis sample post tests the followings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCategory, sub-category nesting in the sidebar.\u003c/li\u003e\n\u003cli\u003eHero image and other images are in \u003ccode\u003eimages\u003c/code\u003e folder inside this post directory.\u003c/li\u003e\n\u003cli\u003eDifferent media rendering like image, tweet, YouTube video, Vimeo video etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"image-sample\"\u003eImage Sample\u003c/h3\u003e\n\u003cimg src=\"/posts/category/sub-category/rich-content/images/forest.jpg\"\n    \n        alt=\"Forest\"\n    \n    \n    \n    \n    \n        class=\"center\"\n    \n\u003e\n\n\u003cdiv style=\"margin-top: rem;\"\u003e\u003c/div\u003e\n\u003ch3 id=\"vimeo-video-sample\"\u003eVimeo Video Sample\u003c/h3\u003e\n\n      \u003cdiv\n          style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n        \u003ciframe\n          src=\"https://player.vimeo.com/video/48912912?dnt=0\"\n            style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allow=\"fullscreen\"\u003e\n        \u003c/iframe\u003e\n      \u003c/div\u003e","tags":["Markdown","Content Organization","Multi-lingual"],"title":"Rich Content"},{"categories":null,"contents":"This is a sample post intended to test the followings:\nDefault hero image. Different shortcodes. Alert The following alerts are available in this theme.\nThis is sample alert with type=\u0026quot;success\u0026quot;. This is sample alert with type=\u0026quot;danger\u0026quot;. This is sample alert with type=\u0026quot;warning\u0026quot;. This is sample alert with type=\u0026quot;info\u0026quot;. This is sample alert with type=\u0026quot;dark\u0026quot;. This is sample alert with type=\u0026quot;primary\u0026quot;. This is sample alert with type=\u0026quot;secondary\u0026quot;. Image A sample image without any attribute. A sample image with height and width attributes. A center aligned image with height and width attributes. A image with float attribute. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. Praesent tellus risus, eleifend vel efficitur ac, venenatis sit amet sem. Ut ut egestas erat. Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. Suspendisse nec ipsum eu erat finibus dictum. Morbi volutpat nulla purus, vel maximus ex molestie id. Nullam posuere est urna, at fringilla eros venenatis quis.\nFusce vulputate dolor augue, ut porta sapien fringilla nec. Vivamus commodo erat felis, a sodales lectus finibus nec. In a pulvinar orci. Maecenas suscipit eget lorem non pretium. Nulla aliquam a augue nec blandit. Curabitur ac urna iaculis, ornare ligula nec, placerat nulla. Maecenas aliquam nisi vitae tempus vulputate.\nSplit This theme support splitting the page into as many columns as you wish.\nTwo column split Left Column Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies.\nRight Column Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat.\nThree column split Left Column Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies.\nMiddle Column Aenean dignissim dictum ex. Donec a nunc vel nibh placerat interdum.\nRight Column Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat.\nVertical Space Give vertical space between two lines.\nThis is line one. This is line two. It should have 4rem vertical space with previous line.\nVideo Video by Rahul Sharma from Pexels.\nMermaid Here, are few example of mermaid shortcode.\nGraph:\ngraph LR; A[Hard edge] --\u003e|Link text| B(Round edge) B --\u003e C{Decision} C --\u003e|One| D[Result one] C --\u003e|Two| E[Result two] Sequence Diagram:\nsequenceDiagram participant Alice participant Bob Alice-\u003e\u003eJohn: Hello John, how are you? loop Healthcheck John-\u003e\u003eJohn: Fight against hypochondria end Note right of John: Rational thoughts prevail! John--\u003e\u003eAlice: Great! John-\u003e\u003eBob: How about you? Bob--\u003e\u003eJohn: Jolly good! Gantt diagram:\ngantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d Class Diagram:\nclassDiagram Class01 \u003c|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --\u003e C2 : Where am i? Class09 --* C3 Class09 --|\u003e Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 \u003c--\u003e C2: Cool label Git Graph:\ngitGraph commit id: \"ZERO\" branch develop commit id:\"A\" checkout main commit id:\"ONE\" checkout develop commit id:\"B\" checkout main commit id:\"TWO\" cherry-pick id:\"A\" commit id:\"THREE\" checkout develop commit id:\"C\" ER Diagram:\nerDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses Gist Embedded PDF Page: / Previous Next ","date":"June 8, 2020","hero":"/posts/shortcodes/boat.jpg","permalink":"https://qingbo12.github.io/posts/shortcodes/","summary":"\u003cp\u003eThis is a sample post intended to test the followings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefault hero image.\u003c/li\u003e\n\u003cli\u003eDifferent shortcodes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"alert\"\u003eAlert\u003c/h2\u003e\n\u003cp\u003eThe following alerts are available in this theme.\u003c/p\u003e\n\n\n\n    \n\n\n\u003cdiv class=\"alert success\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"check-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eThis is sample alert with \u003ccode\u003etype=\u0026quot;success\u0026quot;\u003c/code\u003e.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\n\n\n    \n\n\n\u003cdiv class=\"alert danger\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"alert-octagon\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eThis is sample alert with \u003ccode\u003etype=\u0026quot;danger\u0026quot;\u003c/code\u003e.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\n\n\n    \n\n\n\u003cdiv class=\"alert warning\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"alert-triangle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eThis is sample alert with \u003ccode\u003etype=\u0026quot;warning\u0026quot;\u003c/code\u003e.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\n\n\n    \n\n\n\u003cdiv class=\"alert info\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"info\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eThis is sample alert with \u003ccode\u003etype=\u0026quot;info\u0026quot;\u003c/code\u003e.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"alert dark\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"alert-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eThis is sample alert with \u003ccode\u003etype=\u0026quot;dark\u0026quot;\u003c/code\u003e.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"alert primary\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"alert-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eThis is sample alert with \u003ccode\u003etype=\u0026quot;primary\u0026quot;\u003c/code\u003e.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"alert secondary\"\u003e\n    \u003cspan\u003e\u003ci data-feather=\"alert-circle\"\u003e\u003c/i\u003e\u003c/span\u003e\n    \u003cspan\u003e\u003cstrong\u003eThis is sample alert with \u003ccode\u003etype=\u0026quot;secondary\u0026quot;\u003c/code\u003e.\u003c/strong\u003e\u003c/span\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"image\"\u003eImage\u003c/h2\u003e\n\u003ch4 id=\"a-sample-image-without-any-attribute\"\u003eA sample image without any attribute.\u003c/h4\u003e\n\u003cimg src=\"/posts/shortcodes/boat.jpg\"\n    \n        alt=\"A boat at the sea\"\n    \n    \n    \n    \n    \n\u003e\n\n\u003cdiv style=\"margin-top: 3rem;\"\u003e\u003c/div\u003e\n\u003ch4 id=\"a-sample-image-with-height-and-width-attributes\"\u003eA sample image with \u003ccode\u003eheight\u003c/code\u003e and \u003ccode\u003ewidth\u003c/code\u003e attributes.\u003c/h4\u003e\n\u003cimg src=\"/posts/shortcodes/boat.jpg\"\n    \n        alt=\"A boat at the sea\"\n    \n    \n        width=\"600\"\n    \n    \n        height=\"400\"\n    \n    \n    \n\u003e\n\n\u003cdiv style=\"margin-top: 3rem;\"\u003e\u003c/div\u003e\n\u003ch4 id=\"a-center-aligned-image-with-height-and-width-attributes\"\u003eA center aligned image with \u003ccode\u003eheight\u003c/code\u003e and \u003ccode\u003ewidth\u003c/code\u003e attributes.\u003c/h4\u003e\n\u003cimg src=\"/posts/shortcodes/boat.jpg\"\n    \n        alt=\"A boat at the sea\"\n    \n    \n        width=\"600\"\n    \n    \n        height=\"400\"\n    \n    \n    \n        class=\"center\"\n    \n\u003e\n\n\u003cdiv style=\"margin-top: 3rem;\"\u003e\u003c/div\u003e\n\u003ch4 id=\"a-image-with-float-attribute\"\u003eA image with \u003ccode\u003efloat\u003c/code\u003e attribute.\u003c/h4\u003e\n\u003cimg src=\"/posts/shortcodes/boat.jpg\"\n    \n        alt=\"A boat at the sea\"\n    \n    \n        width=\"500\"\n    \n    \n        height=\"200\"\n    \n    \n        style=\"float: right;\"\n    \n    \n\u003e\n\n\u003cp\u003eLorem ipsum dolor sit amet, consectetur adipiscing elit. Cras egestas lectus sed leo ultricies ultricies. Praesent tellus risus, eleifend vel efficitur ac, venenatis sit amet sem. Ut ut egestas erat. Fusce ut leo turpis. Morbi consectetur sed lacus vitae vehicula. Cras gravida turpis id eleifend volutpat. Suspendisse nec ipsum eu erat finibus dictum. Morbi volutpat nulla purus, vel maximus ex molestie id. Nullam posuere est urna, at fringilla eros venenatis quis.\u003c/p\u003e","tags":null,"title":"Shortcodes Samples"},{"categories":null,"contents":"Go Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/go/_index.bn/","summary":"\u003ch1 id=\"go-notes\"\u003eGo Notes\u003c/h1\u003e","tags":null,"title":"Go এর নোট সমূহ"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/posts/category/sub-category/_index.bn/","summary":"","tags":null,"title":"Sub-Category"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/posts/category/_index.bn/","summary":"","tags":null,"title":"Top Category Sample"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/_index.bn/","summary":"","tags":null,"title":"নোট সমূহ"},{"categories":null,"contents":"","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/posts/_index.bn/","summary":"","tags":null,"title":"পোস্ট সমূহ"},{"categories":null,"contents":"Bash Notes ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://qingbo12.github.io/notes/bash/_index.bn/","summary":"\u003ch1 id=\"bash-notes\"\u003eBash Notes\u003c/h1\u003e","tags":null,"title":"ব্যাশের নোট সমূহ"}]