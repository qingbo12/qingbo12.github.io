---
layout: post
title: transformer Read
date: 2024-09-12 11:23
description: What does transformer tell?
tags: sequence-transduction-model self-attention-mechanisms
categories: paper-reading
tabs: true
featured: true
related_publications: true
toc:
  beginning: true

---

## Questions
1. What's the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions ?



## Extended Learning
1. Attention mechanisms with a recurrent network. {% cite bahdanau2014neural %}
2. Use convolutional neural networks to compute hidden representations in parallel. {% cite gehring2017convolutional %}
3. Why is difficult to learn dependencies between distant positions. {% cite hochreiter2001gradient %}
4. Self-attention. {% cite cheng2016long %}