<!doctype html><html lang=en><head><title>Matrix Base</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.d60808555b3afbcfaf18c23a60e08a98667faa5a60879f6703a355b38eafb832.css integrity="sha256-1ggIVVs6+8+vGMI6YOCKmGZ/qlpgh59nA6NVs46vuDI="><link rel=icon type=image/png href=/images/site/horse-grey_hu6618064002627359663.jpg><meta property="og:url" content="https://qingbo12.github.io/posts/miscellanea/matrix-base/"><meta property="og:site_name" content="Jinpeng's Blog"><meta property="og:title" content="Matrix Base"><meta property="og:description" content="Notes of matrix learning"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-20T16:02:00+08:00"><meta property="article:modified_time" content="2024-11-20T16:02:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Matrix Base"><meta name=twitter:description content="Notes of matrix learning"><meta name=description content="Notes of matrix learning"><script data-goatcounter=https://https://qingbo12.goatcounter.com/count.goatcounter.com/count async src=//gc.zgo.at/count.js></script><script>theme=localStorage.getItem("theme-scheme")||localStorage.getItem("darkmode:color-scheme")||"light",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/><img src=/images/site/horse-grey_hu6618064002627359663.jpg id=logo alt=Logo>
Jinpeng's Blog</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/#home>Home</a></li><li class=nav-item><a class=nav-link href=/#about>About</a></li><li class=nav-item><a class=nav-link href=/#education>Education</a></li><li class=nav-item><a class=nav-link href=/#projects>Projects</a></li><li class=nav-item><a class=nav-link href=/#featured-posts>Featured Posts</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>More</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=/#recent-posts>Recent Posts</a></div></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts>Posts</a></li><li class=nav-item><a class=nav-link id=note-link href=/notes>Notes</a></li><li class=nav-item><a class=nav-link href=https://toha-guides.netlify.app/posts/>Docs</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/horse-grey_hu6618064002627359663.jpg class=d-none id=main-logo alt=Logo>
<img src=/images/site/horse-grey_hu6618064002627359663.jpg class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><a class=list-link href=/posts/building-issues/ title="Building issues">Building issues</a></li><li><a class=list-link href=/posts/introduction/ title=Introduction>Introduction</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/category/> Category</a><ul><li><i data-feather=plus-circle></i><a class=list-link href=/posts/category/sub-category/> Sub-Category</a><ul><li><a class=list-link href=/posts/category/sub-category/rich-content/ title="Rich Content">Rich Content</a></li></ul></li></ul></li><li><a class=list-link href=/posts/markdown-sample/ title="Markdown Sample">Markdown Sample</a></li><li><a class=list-link href=/posts/shortcodes/ title="Shortcodes Sample">Shortcodes Sample</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/gcd/> Generalized Category Discovery</a><ul><li><a class=list-link href=/posts/gcd/dino/ title="DINO Read">DINO Read</a></li><li><a class=list-link href=/posts/gcd/simgcd/ title="SimGCD Read">SimGCD Read</a></li><li><a class=list-link href=/posts/gcd/transformer/ title="transformer Read">transformer Read</a></li></ul></li><li><i data-feather=minus-circle></i><a class="active list-link" href=/posts/miscellanea/> Miscellanea</a><ul class=active><li><a class=list-link href=/posts/miscellanea/c-lecture/ title="C language lecture 1">C language lecture 1</a></li><li><a class=list-link href=/posts/miscellanea/circuit/ title="Circuit Base">Circuit Base</a></li><li><a class="active list-link" href=/posts/miscellanea/matrix-base/ title="Matrix Base">Matrix Base</a></li><li><a class=list-link href=/posts/miscellanea/multimodal-fusion/ title="Multimodal-fusion research">Multimodal-fusion research</a></li><li><a class=list-link href=/posts/miscellanea/optimization/ title='Optimization Methods"'>Optimization Methods"</a></li><li><a class=list-link href=/posts/miscellanea/recommendation-progress/ title="Recommendation Progress">Recommendation Progress</a></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/wpt/> Wireless Power Transfer</a><ul><li><a class=list-link href=/posts/wpt/magmimohotspot/ title="MagMIMOHotspot Read">MagMIMOHotspot Read</a></li><li><a class=list-link href=/posts/wpt/wspmax/ title="WSPMax Read">WSPMax Read</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/miscellanea/matrix-base/images/image-miscellanea.jpg)></div><div class=page-content><div class="author-profile ms-auto align-self-lg-center"><img class=rounded-circle src=/images/author/pandas-part2_hu16066948026037313982.jpg alt="Author Image"><h5 class=author-name>Jinpeng Ma</h5><p class=text-muted>Wednesday, November 20, 2024 | 34 minutes</p></div><div class=title><h1>Matrix Base</h1></div><div class=tags><ul style=padding-left:0></ul></div><div class=post-content id=post-content><h2 id=determinant-of-matrix-product>Determinant of Matrix Product</h2><h3 id=theorem--detab--detadetb->Theorem: $ det(AB) = det(A)det(B) $</h3><p>Let $ A, B $ be a square matrices of order n.</p><p>Let $ det(A) $ be the determinant of $ A $.</p><p>Let $ AB $ be the matrix product of $ A $ and $ B $.</p><p>Then:</p><p>$ det(AB) = det(A)det(B) $</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://proofwiki.org/wiki/Determinant_of_Matrix_Product target=_blank rel=noopener>Determinant of Matrix Product</a></p><p>Consider two cases:</p><ol><li><p>$ A $ is singular.</p></li><li><p>$ A $ is nonsingular.</p></li></ol><p><strong>proof of case1</strong></p><p>Assume $ A $ is singular.</p><p>Then:</p><p>$$ det(A) = 0 $$</p><p><strong>Also if A is singular then so is AB.</strong></p><p>Indeed, if $ AB $ has an inverse $ C $, then:</p><p>$$ ABC = E $$</p><p>whereby $ BC $ is a right inverse of $ A $.</p><p>It follows by Left or Right Inverse of Matrix is Inverse that in that case $ BC $ is the inverse of $ A $. This contradicts the assumption.</p><p>It follows that:</p><p>$$ det(AB)=0 $$</p><p>Thus:</p><p>$$ 0 = 0 \times det(B) $$</p><p>that is:</p><p>$$ det(AB) = 0 = det(A)det(B) $$</p><p><strong>proof of case2</strong></p><p>Assume $ A $ is nonsingular.</p><p>Then $ A $ is a product of elementary row matrices, $E $.</p><p>Let $ A=E_kE_{k−1}⋯E_1 $.</p><p>So:</p><p>$$ det(AB) = det(E_kE_{k−1}⋯E_1B) $$</p><p>It remains to be shown that for any square matrix $ D $ of order n:</p><p>$$ det(ED) = det(E) det(D) $$</p><p>$ det(ED) = -|D| = det(E) det(D) $ for swap 2 rows</p><p>$ det(ED) = k|D| = det(E) det(D) $ for apply k to 1 row</p><p>$ det(ED) = |D| = det(E) det(D) $ for row(i) + k x row(j)</p><p>Then:</p><p>$ det(AB) = det(E_kE_{k−1}⋯E_1B) $</p><p>$ det(AB) = det(E_k) det(E_{k−1}⋯E_1(B)) $</p><p>$ det(AB) = αdet(B) $</p><p>and:</p><p>$ det(A) = det(E_kE_{k−1}⋯E_1I) $</p><p>$ = det(E_kE_{k−1}⋯E_1(I)) $</p><p>$ = αdet(I) $</p><p>Therefore:</p><p>$ det(AB)=det(A)det(B) $</p><h2 id=real-symmetric-matrix>Real Symmetric Matrix</h2><h3 id=theorem-all-real-eigenvalues>Theorem: all real eigenvalues</h3><p>If $ A $ is a (real) $ n \times n $ symmetric matrix, then $ A $ has n real eigenvalues (counted bytheir multiplicities). For each eigenvalue, we can find a real eigenvector associated with it.</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://www.math.wustl.edu/~freiwald/309orthogdiag.pdf target=_blank rel=noopener>Orthogonally Diagonalizable Matrices</a></p><p><img src=/posts/miscellanea/matrix-base/images/realEigenvalues.png alt=proofofrealEigenvalues></p><h3 id=theorem-orthogonally-diagonalizable>Theorem: orthogonally diagonalizable</h3><p>A real symmetric matrix must be orthogonally diagonalizable.</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://www.math.wustl.edu/~freiwald/309orthogdiag.pdf target=_blank rel=noopener>Orthogonally Diagonalizable Matrices</a></p><p>This is a proof by induction, and it uses some simple facts about partitioned matrices and change of coordinates.</p><p>This is obviously true for every $ 1 \times 1 $ matrix $ A $ if $ A = [a] $, then
$ E_1^{-1} (a) E_1 = (a) $</p><p>Assume now that every $ (n - 1) \times (n - 1) $ real symmetric matrix is orthogonally diagonalizable.</p><p>Consider an $ n \times n $ real symmetric matrix $ A $ where $ n \gt 1 $. By the preceding theorem, we can find a real eigenvalue $ \lambda $ of $ A $, together with a real eigenvector $ v_1 $. By normalizing, we can assume
$ v_1 $ is a unit eigenvector. Add vectors to extend
$ (v_1) $ to a basis for
$ R^n $ and then use the Gram Schmidt process to get an orthonormal basis for
$ R^n $:
$ (v_1, v_2, &mldr;, v_n) $.</p><p>Let</p><p>$ T_1 = (v_1, v_2, &mldr;, v_n) $.</p><p>$ T_1 $ is orthogonal.</p><p>Now look that</p><p>$$ T_1^{-1} A T_1 = T_1^{-1} (A v_1, A v_2, &mldr;, A v_n) $$</p><p>$$ T_1^{-1} A T_1 = (T_1^{-1} \lambda_1 v_1, T_1^{-1} A v_2, &mldr;, T_1^{-1} A v_n) $$</p><p>Because $ T_1^{-1} T_1 = E $, we have</p><p>$$ T_1^{-1} (v_1, v_2, &mldr;, v_n) = (\epsilon_1, \epsilon_2, &mldr;, \epsilon_n) $$</p><p>Thus, $ T_1^{-1} v_1 = \epsilon_1 $, So column 1 of
$ T_1^{-1} A T_1 $ is
$ \lambda_1 \epsilon_1 $. Suppose now that</p><p>$$
T_1^{-1} A T_1 =
\begin{pmatrix}
\lambda_1 & \alpha \\
\mathbf{0} & \beta
\end{pmatrix}
$$</p><p>$ T_1^{-1} A T_1 $ is symmetric, because</p><p>$$
(T_1^{-1} A T_1)^{\top} = (T_1^{\top} A T_1)^{\top} =
T_1^{\top} A^{\top} T_1^{\top \top} = T_1^{\top} A T_1 =
T_1^{-1} A T_1
$$</p><p>So $ \alpha = \mathbf{0} $, and
$ \beta $ is a real symmetric matrix. By the induction hypothesis, $ \beta $ is diagonalizable. Thus,
we has $ T_2 $ such that</p><p>$$
T_2^{-1} \beta T_2 = diag\{\lambda_2, \lambda_3, &mldr;, \lambda_n\}
$$</p><p>Let</p><p>$$
T = T_1
\begin{pmatrix}
1 & \mathbf{0} \\
\mathbf{0} & T_2
\end{pmatrix}
$$</p><p>$$
T^{-1} A T =
\begin{pmatrix}
1 & \mathbf{0} \\
\mathbf{0} & T_2
\end{pmatrix}^{-1}
T_1^{-1} A T_1
\begin{pmatrix}
1 & \mathbf{0} \\
\mathbf{0} & T_2
\end{pmatrix}
$$</p><p>$$
T^{-1} A T =
\begin{pmatrix}
1 & \mathbf{0} \\
\mathbf{0} & T_2^{-1}
\end{pmatrix}
\begin{pmatrix}
\lambda_1 & \mathbf{0} \\
\mathbf{0} & \beta
\end{pmatrix}
\begin{pmatrix}
1 & \mathbf{0} \\
\mathbf{0} & T_2
\end{pmatrix}
$$</p><p>$$
T^{-1} A T =
\begin{pmatrix}
\lambda_1 & \mathbf{0} \\
\mathbf{0} & T_2^{-1} \beta T_2
\end{pmatrix}
$$</p><p>$$
T^{-1} A T = diag\{\lambda_1, \lambda_2, &mldr;, \lambda_n\}
$$</p><p>Therefore, a real symmetric matrix must be orthogonally diagonalizable.</p><h3 id=theorem-r-fold-root--a---lambda-e--has-a-rank-of--n---r->Theorem: r-fold root, $ A - \lambda E $ has a rank of $ n - r $</h3><p>If $ A $ is a real symmetric matrix of order n. $ \lambda $ is an r-fold root of its characteristic polynomial</p><p>Then the matrix $ A - \lambda E $ has a rank of $ n - r $. This implies that there are exactly $ r $ linearly independent eigenvectors associated with the eigenvalue $ \lambda $.</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://www.zhihu.com/question/462622563/answer/1918454726 target=_blank rel=noopener>Yiwen&rsquo;s Zhihu Answer</a></p><p>Suppose that $ A&rsquo;s $ r -fold root is $ \lambda_k $.</p><p>Because $ A $ is real symmetric matrix, so we have orthogonal matrix $ P $ such that:</p><p>$$
P^{-1} A P =
\begin{pmatrix}
\lambda_1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & \cdots & 0 \\
0 & \lambda_2 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \blue{\lambda_{k}} & 0 & 0 & \cdots & 0 & \cdots & 0 \\
0 & 0 & 0 & 0 & 0 & \blue{\lambda_{k}} & 0 & \cdots & 0 & \cdots & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \blue{\lambda_{k}} & \cdots & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & \blue{\lambda_{k}} & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & \cdots & \lambda_n
\end{pmatrix}
$$</p><p>We don&rsquo;t care whether $ \lambda_i $ is 0 or not.</p><p>For matrix $ A - \lambda_k E $, we have:</p><p>$$
P^{-1} (A - \lambda_k E) P = P^{-1} A P - \lambda_k P^{-1} E P =
$$</p><p>$$
\varLambda - \lambda_k E =
\begin{pmatrix}
\lambda_1 - \blue{\lambda_{k}} & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & \cdots & 0 \\
0 & \lambda_2 - \blue{\lambda_{k}} & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & \blue{0} & 0 & 0 & \cdots & 0 & \cdots & 0 \\
0 & 0 & 0 & 0 & 0 & \blue{0} & 0 & \cdots & 0 & \cdots & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & \blue{0} & \cdots & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & \blue{0} & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & \cdots & \lambda_n - \blue{\lambda_{k}}
\end{pmatrix}
$$</p><p>$\because \lambda_i \neq \lambda_k \Rightarrow \lambda_i - \lambda_k \neq 0, (i \neq k) $</p><p>$ \therefore rank(A - \lambda_k E) = rank(\varLambda - \lambda_k E) = n - r $</p><h2 id=vector-spaces>Vector spaces</h2><p>$ A \in M_{m,n}(\mathbf{F}) $ as a linear transformation $ x \rightarrow Ax $ from $ F^n $ to $ F^m $.
The domain of this linear transformation is $ F^n $; its range is
$ range A = \{y \in F^m : y =Ax \} $ for some $ x \in F^n $;
its null space is $ nullspace A = {x \in F^n : Ax = 0} $. The range of
$ A $ is a subspace of $ F^m $, and the null space of $ A $ is a
subspace of $ F^n $. The dimension of $ nullspace A $ is denoted by
$ nullity A $; the dimension of $ range A $ is denoted by $ rank A $.</p><blockquote><p>To avoid ambiguities,<br>The range of a linear transformation can be denoted as $ \operatorname{range} A $, $ \operatorname{im} A $.<br>The dimension of the range of a linear transformation can be denoted as $ \operatorname{rank} A $.<br>The null space of a linear transformation can be denoted as $ \operatorname{nullspace} A $, $ \operatorname{ker} A $.<br>The dimension of the null space of a linear transformation can be denoted as $ \operatorname{nullity} A $.</p></blockquote><h3 id=the-dimension-of--operatornamerange-a--is-denoted-by--operatornamerank-a->The dimension of $ \operatorname{range} A $ is denoted by $ \operatorname{rank} A $</h3><p>The proof I provide is based on <a href=https://math.stackexchange.com/a/3540031 target=_blank rel=noopener>rank-of-matrix-equals-dimension-of-range</a></p><p>Suppose that we are given that the $ A \in M_{m, n}(\mathbf{F}) $
has column-rank (and therefore coincident row-rank) $ r $.
That is, the maximal set of linearly independent columns of $ A $
contains $ r $ vectors. It follows that the span of the columns of $ A $,
henceforth the column space of $ A $, is an r-dimensional subspace
of $ R^m $.</p><p>Each column of $ A $ is a vector with $ m $ entries,
so the columns live in $ F^m $. If $ n \gt m $, These columns
must be dependent, so their dimension $ r \leq m $. Hence, the column space of $ A $ is a subspace of
$ F^m $.</p><p>Let $ a_1, \cdots, a_n $ denote the columns of $ A $. Consider an
arbitrary element $ y $ inside the column space of $ A $. By definition,
this mean that there exist coefficients $ x_1, \cdots, x_n $ such that</p><p>$$
y = a_1 x_1 + a_2 x_2 + \cdots + a_n x_n
$$</p><p>Note that we can rewrite the above sum as a product. In particular,
we have</p><p>$$
y =
\begin{pmatrix}
a_1 & a_2 & \cdots & a_n
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix}
= A x
$$</p><p>where $ x = (x_1, x_2, \cdots, x_n)^{\top} \in R^n $. So, any element
$ y $ from the column space of $ A $ is also an element of
the range of $ A $.</p><p>So the dimension of $ range A $ is denoted by $ rank A $.</p><h3 id=rank-nullity-theorem>Rank-nullity theorem</h3><h4 id=matrices>Matrices</h4><p>For $ A \in M_{m, n}(\mathbf{F}) $, we have:</p><p>$$
rank(A) + nullity(A) = n
$$</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://www.math.purdue.edu/files/academic/courses/2010spring/MA26200/4-9.pdf target=_blank rel=noopener>The Rank-Nullity Theorem</a>.</p><p>If $ rank(A) = n $, then by the Invertible Matrix Theorem, the only solution to $ Ax = 0 $ is the trivial solution $ x = 0 $. Hence, in this case, $ nullspace(A) = {0} $, so $ nullity(A) = 0 $ and Equation holds.</p><p>Now suppose $ rank(A) = r \lt n $. In this case, there are $ n − r > 0 $ free variables in the solution to $ Ax = 0 $. Let $ t_{1}, t_{2},&mldr;,t_{n−r} $ denote these free variables (chosen as those variables not attached to a leading one in any row-echelon form of $ A $), and let $ x_1, x_2,&mldr;, x_{n−r} $ denote the solutions obtained by sequentially setting each free variable to 1 and the remaining free variables to zero. Note that $ \{ x_1, x_2,&mldr;, x_{n−r} \} $ is linearly independent. Moreover, every solution to $ Ax = 0 $ is a linear combination of $ x_1, x_2,&mldr;, x_{n−r} $:</p><p>$$
x = t_1x_1 + t_2x_2 +···+ t_{n−r}x_{n−r},
$$</p><p>which shows that $ \{ x_1, x_2,&mldr;, x_{n−r} \} $ spans $nullspace(A) $. Thus, $ \{ x_1, x_2,&mldr;, x_{n−r} \} $ is a basis for nullspace(A), so $ nullity(A) = n − r $ and Equation holds.</p><h4 id=linear-transformations>Linear transformations</h4><p>Let $ T:V \rightarrow W $ be a linear transformation between
two vector spaces where $ T $ &rsquo;s domain $ V $ is finite
dimensional. Then</p><p>$$
rank(T) + nullity(T) = dim V
$$</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem#First_proof target=_blank rel=noopener>Rank–nullity theorem</a>.</p><p>Let $ V, W $ be vector spaces over some field $ F $, and $ T $ defined as in the statement of the theorem with $ \dim V = n $.</p><p>As $ \operatorname{Ker}T \subset V $ is a subspace,
there exists a basis for it. Suppose
$ \dim\operatorname{Ker}T = k $ and let
$ \mathcal{K} := \{v_1, \ldots, v_k\} \subset
\operatorname{Ker}(T) $ be such a basis.</p><p>We may now, by the Steinitz exchange lemma, extend
$ \mathcal{K} $ with $ n-k $ linearly independent vectors
$ w_1, \ldots, w_{n-k} $ to form a full basis of $ V $.</p><p>Let</p><p>$
\mathcal{S} := \{w_1, \ldots, w_{n-k}\} \subset
V \setminus \operatorname{Ker}(T)
$</p><p>such that</p><p>$
\mathcal{B} := \mathcal{K} \cup \mathcal{S} =
\{v_1, \ldots, v_k, w_1, \ldots, w_{n-k}\} \subset V
$</p><p>is a basis for $ V $.
From this, we know that</p><p>$$
\operatorname{Im} T = \operatorname{Span}T(\mathcal{B}) =
\operatorname{Span}\{T(v_1), \ldots, T(v_k),
T(w_1), \ldots, T(w_{n-k})\} =
\operatorname{Span}{T(w_1), \ldots, T(w_{n-k})} =
\operatorname{Span}T(\mathcal{S}).
$$</p><p>We now claim that $ T(\mathcal{S}) $ is a basis for
$ \operatorname{Im} T $. The above equality already
states that $ T(\mathcal{S}) $ is a generating set
for $ \operatorname{Im} T $;
it remains to be shown that it is also linearly independent
to conclude that it is a basis.</p><p>Suppose $ T(\mathcal{S}) $ is not linearly independent,
and let</p><p>$ \sum_{j=1}^{n-k} \alpha _j T(w_j) = 0_W $</p><p>for some $ \alpha _j \in F $.</p><p>Thus, owing to the linearity of $ T $, it follows that</p><p>$$
T \left(\sum_{j=1}^{n-k} \alpha_j w_j \right) = 0_W
\implies \left(\sum_{j=1}^{n-k} \alpha_j w_j \right)
\in \operatorname{Ker} T = \operatorname{Span} \mathcal{K}
\subset V.
$$</p><p>This is a contradiction to $ \mathcal{B} $ being a basis,
unless all $ \alpha _j $ are equal to zero.
This shows that $ T(\mathcal{S}) $ is linearly independent,
and more specifically that it is a basis for
$ \operatorname{Im}T $.</p><p>To summarize, we have $ \mathcal{K} $, a basis for
$ \operatorname{Ker}T $, and $ T(\mathcal{S}) $,
a basis for $ \operatorname{Im}T $.</p><p>Finally we may state that</p><p>$$
\operatorname{Rank}(T) + \operatorname{Nullity}(T) =
\dim \operatorname{Im} T + \dim \operatorname{Ker}T =
|T(\mathcal{S})| + |\mathcal{K}| =
(n-k) + k = n = \dim V .
$$</p><p>This concludes our proof.</p><h4 id=application--dim-cab--dim-cb---dim--operatornamenullspace-a-cap-cb->Application: $ \dim C(AB) = \dim C(B) - \dim ( \operatorname{nullspace} (A) \cap C(B)) $</h4><p>The application I provide is based on <a href=https://math.stackexchange.com/a/1155035 target=_blank rel=noopener>$ \dim C(AB) = \dim C(B) - \dim ( \operatorname{nullspace} (A) \cap C(B)) $</a></p><p>considering the map $ T: C(B) \rightarrow C(AB) $ given by
$ T(y) = Ay $, for all $ y \in C(B) $.</p><p>the kernel of $ T $ is $ \operatorname{nullspace} (A) \cap C(B) $.
Indeed, $ y \in \operatorname{Ker} (T) $ if and only if
$ y \in C(B) $ and $ A y = 0 $, if and only if
$ y \in \operatorname{nullspace} (A) \cap C(B) $.</p><p>Now use the rank-nullity theorem, we have</p><p>$$
\operatorname{Rank}(T) + \operatorname{Nullity}(T) =
\dim C(AB) + \dim (\operatorname{nullspace} (A) \cap C(B)) =
\dim C(B) =
\dim V.
$$</p><p>$$
\dim C(AB) = \dim C(B) - \dim (\operatorname{nullspace} (A) \cap C(B))
$$</p><h2 id=rank>Rank</h2><h3 id=sylvester-inequality>Sylvester inequality</h3><p>If $ A \in M_{m,k} (F) $ and $ B \in M_{k,n} (F) $, then</p><p>$$
(rank A + rank B) - k \leq rank (AB)
$$</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://math.stackexchange.com/a/269622 target=_blank rel=noopener>Prove Sylvester rank inequality</a>.</p><p>If we prove that $ dim ker A + dim ker B \geq dim ker (AB) $.
By using the rank-nullity theorem, we can then deduce
that $ (rank A + rank B) - k \leq rank (AB) $.</p><p>Firstly, we show that $ kerB \subseteq ker(AB) $.</p><p>Any $ x \in kerB $, we have:</p><p>$$ B x = 0 $$</p><p>Now, applying the matrix $ A $ to both sides of this equation:</p><p>$$ A B x = A 0 = 0 \Rightarrow x \in ker(AB) $$</p><p>Thus $ kerB \subseteq ker(AB) $.</p><p>Then we show that $ dim ker A + dim ker B \geq dim ker (AB) $.</p><p>Let $ \beta = \{\alpha_1,…,\alpha_r\} $ be a basis for $ kerB $.
Because $ kerB \subseteq ker(AB) $, so we can extend $ \beta $ to
a basis for $ ker(AB) $. Suppose $ \{\alpha_1, \cdots, \alpha_r,
\alpha_{r+1}, \cdots, \alpha_n \} $ be basis for $ ker(AB) $. Since
$ \alpha_{i + 1}, \cdots, \alpha_n \notin kerB $, we have that
$ B \alpha_i \neq 0 $ for $ i \in \{ r \lt i \lt n + 1 \} $.</p><p>We show that $ \{B \alpha_{r+1}, \cdots, B \alpha_n \} $ is
linear independent. If we can show that, then we would have $ dim ker A \geq n - r $.
(Note: $ dim ker A \neq dim ker (AB) $, Because $ B \alpha_i = 0 $, for $ i \in \{ 1 \leq i \leq r \} $,
if we add $ B \alpha_1 $ to $ \{B \alpha_{r+1}, \cdots, B \alpha_n \} $,
we would have $ \{ B \alpha_1, B \alpha_{r+1}, \cdots, B \alpha_n \} $ is linear dependent.
So we could only say $ dim ker A \geq n - r $.)</p><p>Assume that there exist scalars $ \lambda_1, \cdots, \lambda_n $,
not all zero, such that $ \sum_{i = r + 1}^n \lambda_i B \alpha_i = 0 $.
Since $ B $ is linear, we have $ B \sum_{i = r + 1}^n \lambda_i \alpha_i = 0 $,
so that $ \sum_{i = r + 1}^n \lambda_i \alpha_i $ belongs to the kernel
of $ B $. On other hand, we already know that $ \beta = \{\alpha_1,…,\alpha_r\} $ be a basis for $ kerB $.
Next since the set $ \{\alpha_1, \cdots, \alpha_r, \alpha_{r+1}, \cdots,
\alpha_n \} $ is an independent set, we infer that $ \lambda_i $ must be
zero for all $ i = r + 1, \cdots, n $.</p><p>Now one can see that</p><p>$$
dim ker A + dim ker B \geq n - r + r = n \Rightarrow dim ker A + dim ker B \geq dim ker (AB)
$$</p><p>Using the rank-nullity theorem, we have</p><p>$$
k - rank A + n - rank B \geq n - rank (AB) \Rightarrow (rank A + rank B) - k \leq rank (AB)
$$</p><h3 id=frobenius-inequality>Frobenius inequality</h3><p>Let $ A \in M_{m \times k}(\bf F) $,
$ B \in M_{k \times p}(\bf F) $ and $ C \in M_{p \times n}(\bf F) $,
then $ \operatorname{rank}(AB) + \operatorname{rank}(BC) \leq
\operatorname{rank}(B) + \operatorname{rank}(ABC) $</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://math.stackexchange.com/a/1191064 target=_blank rel=noopener>Frobenius rank inequality</a>.</p><p>We can use the rank-nullity theorem to show that</p><p>$$
\tag{1}
\operatorname{rank}(AB) = \operatorname{rank}(B) -
\dim (\operatorname{im}(B) \cap \operatorname{ker} (A))
$$</p><p>Since $ \operatorname{im}(BC) \subseteq \operatorname{im}(B) $,
we have</p><p>$$
\tag{2}
\operatorname{im}(BC) \cap \operatorname{ker}(A) \subseteq
\operatorname{im}(B) \cap \operatorname{ker}(A)
$$</p><p>Now we want to write $ \operatorname{rank}(ABC) $
in such a way that $ \operatorname{im}(BC) \cap
\operatorname{ker}(A) $ pops up, so we could make use of <em>(2)</em>.
Analogously to <em>(1)</em>:</p><p>$$
\tag{3}
\operatorname{rank} (ABC) = \operatorname{rank} (BC) -
\dim (\operatorname{im} (BC) \cap \operatorname{ker} (A))
$$</p><p>From <em>(1)</em> and <em>(3)</em>, we have</p><p>$$
\operatorname{rank}(AB) + \operatorname{rank}(BC) =
\operatorname{rank}(B) + \operatorname{rank}(ABC) +
\underbrace{
\dim (\operatorname{im} (BC) \cap \operatorname{ker} (A)) -
\dim (\operatorname{im}(B) \cap \operatorname{ker} (A))
}_{\leq 0 \text{ due to (2)}}
$$</p><p>which implies the desired inequality.</p><h3 id=left-or-right-multiplication-by-a-nonsingular-matrix-leaves-rank-unchanged>Left or right multiplication by a nonsingular matrix leaves rank unchanged</h3><p>If $ A \in M_m(\bf F) $ and $ C \in M_n(\bf F) $ are
nonsingular and $ B \in M_{m,n}(\bf F) $, then $ \operatorname{rank} AB =
\operatorname{rank} B = \operatorname{rank} BC =
\operatorname{rank} ABC $; that is,
left or right multiplication by a nonsingular matrix leaves
rank unchanged.</p><p><strong>Proof</strong></p><p>Firstly, we show that $ \operatorname{rank} (B) =
\operatorname{rank} (BC) $.</p><p>The proof is based on <a href=https://math.stackexchange.com/a/3797397 target=_blank rel=noopener>How to prove that $ \operatorname{im}(B) = \operatorname{im}(BA)$?</a>.</p><p>We know that $ C $ is a linear transformation from
$ \bf F^n $ to $ \operatorname{im} (C) $. Since $ C $ is
nonsingular, it follows that $ C $ is onto,
implying $ \operatorname{im} (C) = \bf F^n $.</p><p>Now consider the linear transformation $ BC $.
This composition can be viewed in two stages:</p><ol><li><p>The transformation $ x \rightarrow Cx $, where
$ C: \bf F^n \rightarrow \bf F^n $.</p></li><li><p>The transformation $ Cx \rightarrow BCx $, where $ B:
\bf F^n $ to $ \operatorname{im} (BC) $.</p></li></ol><p>$ Cx $ spans all of $ \bf F^n $, which aligns with the domain
of $ B $. Thus, the transformation $ Cx \rightarrow BCx $
behaves exactly as $ B $, mapping $ \bf F^n $ to
$ \operatorname{im} (B) $.</p><p>Hence, $ \operatorname{im} (BC) = \operatorname{im} (B) $.</p><p>This concludes our proof.</p><p>Then we show that $ \operatorname{rank} (AB) =
\operatorname{rank} (B) $.</p><p>We know that $ A $ is a linear transformation from $ \bf F^m $
to $ \operatorname{im} (A) $. Since $ A $ is nonsingular,
it follows that $ A $ is onto, meaning
$ \operatorname{im} (A) = \bf F^m $. Furthermore, because
$ \dim(\operatorname{im}(A)) = \dim(\operatorname{domain}(A)) = m $,
$ A $ is injective as well.</p><p>Now consider the linear transformation $ AB $.
This composition can be viewed in two stages:</p><ol><li><p>The transformation $ x \rightarrow Bx $, where $ B: \bf F^n
\rightarrow \operatorname{im} (B) $.</p></li><li><p>Then $ Bx \rightarrow ABx $, where $ A:
\operatorname{im} (B) $ to $ \operatorname{im} (AB) $.</p></li></ol><p>Since A is injective, $ \dim (\operatorname{im} (AB)) =
\dim (\operatorname{im} (B)) $.</p><p>This concludes our proof.</p><h3 id=if--a-in-m_mnbf-c--then--operatornamerank-a-a--operatornamerank-a->If $ A \in M_{m,n}(\bf C) $, then $ \operatorname{rank} A^* A = \operatorname{rank} A $</h3><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://math.stackexchange.com/q/349966 target=_blank rel=noopener>Prove $\operatorname{rank}A^TA=\operatorname{rank}A$</a>.</p><p>Let $ x \in \operatorname{nullspace} (A) $.</p><p>$$
A x = 0
$$
$$
\Rightarrow A^* A x = 0
$$
$$
\Rightarrow x \in \operatorname{nullspace} (A^* A)
$$</p><p>Hence, $ \operatorname{nullspace} (A) \subseteq \operatorname{nullspace} (A^* A) $.</p><p>Agian let $ x \in \operatorname{nullspace} (A^* A) $.</p><p>$$
A^* A x = 0
$$
$$
\Rightarrow x^* A^* A x = 0
$$
$$
\Rightarrow (A x)^* A x = 0
$$
$$
\Rightarrow A x = 0
$$
$$
\Rightarrow x \in \operatorname{nullspace} (A)
$$</p><p>Hence, $ \operatorname{nullspace} (A^* A) \subseteq \operatorname{nullspace} (A) $.</p><p>Therefore,
$$
\operatorname{nullspace} (A) = \operatorname{nullspace} (A^* A)
$$
$$
\dim (\operatorname{nullspace} (A)) = \dim (\operatorname{nullspace} (A^* A))
$$
$$
\dim (\operatorname{domain} (A)) - \operatorname{rank} (A) = \dim (\operatorname{domain} (A^* A)) - \operatorname{rank} (A^* A)
$$
$$
\operatorname{rank} (A) = \operatorname{rank} (A^* A)
$$</p><h3 id=cr-factorization>CR Factorization</h3><p>Let $ A \in M_{m,n}(\bf F) $, $ \operatorname{rank} (A) = r $.
Suppose $ C $ contains the first $ r $ independent columns of $ A $.
Suppose $ R $ contains the $ r $ nonzero rows of $ \operatorname{rref} (A) $.
Then $ A = C R $.</p><p>Now suppose the matrix $ B $ contains the first $ r $ independent rows of $ A $.
$ W $ is the $ r $ by $ r $ matrix where $ C $ meets $ B $. Then $ A = C W^{-1} B $.</p><p>The establishment I provided is based on <a href=https://math.mit.edu/~gs/everyone/lucrweb.pdf target=_blank rel=noopener>LU and CR Elimination</a>.</p><p><strong>Establish $ A = C R $</strong></p><p>Here are the steps to establish $ A = C R $. We know that an invertible
elimination matrix $ E $ (a product of simple steps) gives
$ E A = R_0 = \operatorname{rref} (A) $. Then $ A = E^{-1} R_0 $.
Drop the $ m - r $ zero rows of $ R_0 $ and the last $ m - r $ columns
of $ E^{-1} $. This leaves $ A = C \begin{bmatrix} I & F \end{bmatrix} P $,
where the identity matrix in $ R $ allows us to identify $ C $ in the
columns of $ E^{-1} $.</p><p><strong>Establish $ A = C W^{-1} B $</strong></p><p>Suppose the $ r $ independent columns of $ A $ in the first $ r $ columns,
and there are $ r $ independent rows of $ A $ in the first $ r $ rows.</p><p>Let $ A = \begin{bmatrix} W & H \\ J & K \end{bmatrix} $,
$ C = \begin{bmatrix} W \\ J \end{bmatrix} $,
$ B = \begin{bmatrix} W & H \end{bmatrix} $.</p><p>Combinations $ V $ of the rows of $ B $ must produce the dependent rows in
$ \begin{bmatrix} J & K \end{bmatrix} $.
Then $ \begin{bmatrix} J & K \end{bmatrix} = V B = \begin{bmatrix} V W & V H \end{bmatrix} $.
And $ C = \begin{bmatrix} I \\ V \end{bmatrix} W $.</p><p>Combinations $ T $ of the columns of $ A $ must produce the dependent columns in
$ \begin{bmatrix} H \\ K \end{bmatrix} $.
Then $ \begin{bmatrix} H \\ K \end{bmatrix} = C T = \begin{bmatrix} W T \\ J T \end{bmatrix} $.
And $ B = W \begin{bmatrix} I & T \end{bmatrix} $.</p><p>$$
A =
\begin{bmatrix} W & H \\ J & K \end{bmatrix} =
\begin{bmatrix} W & H \\ VW & VH \end{bmatrix} =
\begin{bmatrix} W & WT \\ VW & VWT \end{bmatrix} =
\begin{bmatrix} I \\ V \end{bmatrix} \begin{bmatrix} W \end{bmatrix} \begin{bmatrix} I & T \end{bmatrix} =
C W^{-1} B
$$</p><h3 id=wedderburns-rank-one-reduction-formula>Wedderburn&rsquo;s rank-one reduction formula</h3><p>Let $ A \in M_{m,n}(\bf F) $. If $ x \in \bf F^n $ and $ y \in \bf F^m $, and if</p><p>$$
\omega = y^{\top} A x \neq 0.
$$</p><p>Then</p><p>$$
\operatorname{rank} (A - \omega^{-1} A x y^{\top} A ) =
\operatorname{rank} (A) - 1.
$$</p><p>In general, if $ X \in M_{n,k} (\bf F) $ and $ Y \in M_{m,k} (\bf F) $,
and if</p><p>$$ W = Y^{\top} A X $$</p><p>is nonsingular, then</p><p>$$
\operatorname{rank} (A - A X W^{-1} Y^{\top} A) =
\operatorname{rank} A - k
$$</p><p><strong>Proof</strong></p><p>The proof I provide is based on <a href=https://arxiv.org/pdf/2406.03992 target=_blank rel=noopener>Generalized Wedderburn Rank Reduction</a>.</p><p><strong>Proof of rank-one reduction formula</strong></p><p>For any $ v $ in $ \operatorname{nullspace} (A) $,
$$
A v = 0 \Rightarrow (A - \omega^{-1} A x y^{\top} A) v = 0
$$</p><p>Hence, $ \operatorname{nullspace} (A)
\subseteq \operatorname{nullspace} (A - \omega^{-1} A x y^{\top} A) $.</p><p>Since $ \omega = y^{\top} A x \neq 0 \Rightarrow A x \neq 0 \Rightarrow
x \notin \operatorname{nullspace} (A) $, but</p><p>$$ (A - \omega^{-1} A x y^{\top} A) x = (A - A) = 0
\Rightarrow x \in \operatorname{nullspace} (A - \omega^{-1} A x y^{\top} A) .
$$</p><p>Hence, $ \operatorname{nullity} (A - \omega^{-1} A x y^{\top} A) - 1 \geq \operatorname{nullity} (A) $.
Also, $ \dim (\operatorname{domain} (A)) = \dim (\operatorname{domain} (A - \omega^{-1} A x y^{\top} A)) $.</p><p>Therefore</p><p>$$
\operatorname{rank} (A - \omega^{-1} A x y^{\top} A) \leq
\operatorname{rank} (A) - 1.
$$</p><p>Assume now $ (A - \omega^{-1} A x y^{\top} A) u = 0 $. Let
$ \lambda = \omega^{-1} y^{\top} A u $. then</p><p>$$ A (u - \lambda x) = 0 $$</p><p>hence $ u \in \operatorname{nullspace} (A) + \bf F x $. Therefore</p><p>$$
\operatorname{rank} (A - \omega^{-1} A x y^{\top} A) =
\operatorname{rank} (A) - 1.
$$</p><p>Note that:</p><p>$ A x $: A vector in the column space of $ A $.</p><p>$ y^{\top} A $: A row vector in the row space of $ A $.</p><p>$ (Ax)(y^{\top} A) $: A rank-1 matrix since it&rsquo;s the outer product of
a vector $ (A x) $ and a row vector $ (y^{\top} A) $.</p><p>Thus, the term $ A x y^{\top} A $ represents a modification to $ A $
in the direction of a rank-1 update.</p><p><strong>Proof of general formula</strong></p><p>The proof is similar.</p><p>Since $ \operatorname{rank} (M) = k $, the $ k $ columns of $ X $
are linearly independent, and $ A X \in \bf F^{m \times k} $ is of rank
$ k $, so no linear combination of columns of $ X $ is contained
in the nullspace of $ A $. But we have</p><p>$$ ( A - A X M^{-1} Y^{\top} A ) X = 0 .$$</p><p>We know that $ \operatorname{nullspace} (A) \subseteq \operatorname{nullspace}
(A - A X M^{-1} Y^{\top} A) $, so</p><p>$$
\operatorname{rank} (A - A X M^{-1} Y^{\top} A) =
n - \dim(\operatorname{nullspace} (A - A X M^{-1} Y^{\top} A)) \leq
n - ((n - \operatorname{rank} (A)) + k) =
\operatorname{rank} (A) - k.
$$</p><p>Let $ U \in M_{n \times k} (\bf F) $ be any matrix such that</p><p>$$
\operatorname{rank} (A - A X M^{-1} Y^{\top} A) U = 0.
$$</p><p>Let $ \Lambda = M^{-1} Y^{\top} A U $, then</p><p>$$
A (U - X \Lambda) = 0,
$$</p><p>hence</p><p>$$
U \in \operatorname{nullspace} (A) + X \Lambda.
$$</p><p>This implies that</p><p>$$ \operatorname{nullspace} (A - A X M^{-1} Y^{\top} A)
\subseteq \operatorname{nullspace} (A) + \mathcal{C} (X) \cong
\operatorname{nullspace} (A) \oplus \mathcal{C} (X) ,$$</p><p>and finally,</p><p>$$
\operatorname{rank} (A - A X M^{-1} Y^{\top} A) =
\operatorname{rank} (A) - k.
$$</p><h2 id=the-euclidean-inner-product>The Euclidean inner product</h2><h3 id=coordinates-form-of-inner-product-and--costheta->Coordinates form of inner product and $ cos(\theta) $</h3><p>The inner product $ &lt;F, s> $ is proposed to descrip
the work done by force $ F $ acting along a displacement $ s $.</p><p>$$ W = &lt;F, s> = |F| \sdot |s| \sdot cos(\theta) $$</p><p>From this, the cosine of the angle $ \theta $ between $ F $ and $ s $
can be expressed as:</p><p>$$ cos(\theta) = \frac{&lt;F, s>}{|F| \sdot |s|} $$</p><p>In coordinates, the inner product $ &lt;F, s> $ is defined as:</p><p>$$ &lt;F, s> = s^* F = s_x F_x + s_y F_y + s_z F_z $$</p><p>This decomposition shows that the work done in each direction
corresponds to the component-wise product:
The work in the $ x $ direction is $ s_x F_x $, the work in the $ y $
direction is $ s_y F_y $, and the work in the $ z $ direction is $ s_z F_z $.</p><p>Thus, the coordinate form provides a clear and meaningful breakdown
of the total work into contributions from each axis.</p><h2 id=determinants-again>Determinants again</h2><h3 id=the-cauchy-binet-formula>The Cauchy-Binet formula</h3><p>Let $ A \in M_{m,n} (\bf F) $, $ B \in M_{n,m} (\bf F) $, and $ C = A B $.</p><p>Cauchy-Binet formula expresses determinant $ | C | $ in terms of
the minors of $ A $ and $ B $:</p><p>$$
\tag{1}
\begin{vmatrix} c_{11} & \cdots & c_{1m} \\ \vdots & \ddots & \vdots \\ c_{m1} & \cdots & c_{mm} \end{vmatrix} =
\sum_{1 \leq k_1 &lt; k_2 &lt; \cdots &lt; k_m \leq n}
\begin{vmatrix} a_{1 k_1} & \cdots & a_{1 k_m} \\ \vdots & \ddots & \vdots \\ a_{m k_1} & \cdots & a_{m k_m} \end{vmatrix}
\begin{vmatrix} b_{k_1 1} & \cdots & b_{k_1 m} \\ \vdots & \ddots & \vdots \\ b_{k_m 1} & \cdots & b_{k_m m} \end{vmatrix}
$$</p><p>or in a concise notation,</p><p>$$
\tag{1&rsquo;}
C \begin{pmatrix} 1 & 2 & \cdots & m \\ 1 & 2 & \cdots & m \end{pmatrix} =
\sum_{1 \leq k_1 &lt; k_2 &lt; \cdots &lt; k_m \leq n}
A \begin{pmatrix} 1 & 2 & \cdots & m \\ k_1 & k_2 & \cdots & k_m \end{pmatrix}
B \begin{pmatrix} k_1 & k_2 & \cdots & k_m \\ 1 & 2 & \cdots & m \end{pmatrix}
$$</p><p><strong>Derivation</strong></p><p>The derivation I provide is based on Volume 1 of Gantmacher&rsquo;s classic
<a href=https://www.maths.ed.ac.uk/~v1ranick/papers/gantmacher1.pdf target=_blank rel=noopener>The Theory of Matrices</a>,
in chapter 1 $ \S $ 2.</p><p>The determinant of $ C $ can be represented in the form</p><p>$$
\begin{vmatrix} c_{11} & \cdots & c_{1m} \\ \vdots & \ddots & \vdots \\ c_{m1} & \cdots & c_{mm} \end{vmatrix} =
\begin{vmatrix}
\sum_{\alpha_1 = 1}^n a_{1 \alpha_1} b_{\alpha_1 1} & \cdots & \sum_{\alpha_m = 1}^n a_{1 \alpha_m} b_{\alpha_m m} \\
\vdots & \ddots & \vdots \\
\sum_{\alpha_1 = 1}^n a_{m \alpha_1} b_{\alpha_1 1} & \cdots & \sum_{\alpha_m = 1}^n a_{m \alpha_m} b_{\alpha_m m}
\end{vmatrix}
$$</p><p>Expanding the determinant, we consider the multilinear property of
determinants: the determinant is linear in each row.
Substituting each term into the determinant:</p><p>$$
= \sum_{\alpha_1, \alpha_2, \cdots, \alpha_m = 1}^n
\begin{vmatrix}
a_{1 \alpha_1} b_{\alpha_1 1} & \cdots & a_{1 \alpha_m} b_{\alpha_m m} \\
\vdots & \ddots & \vdots \\
a_{m \alpha_1} b_{\alpha_1 1} & \cdots & a_{m \alpha_m} b_{\alpha_m m}
\end{vmatrix}
$$</p><p>Now, by the property of determinants that &ldquo;multiplying a column by
a number multiplies the determinant by this number,&rdquo; we can factor out
$ b_{\alpha_i i} $ from the $ i $ -th column of the determinant:</p><p>$$
= \sum_{\alpha_1, \alpha_2, \cdots, \alpha_m = 1}^n
\begin{vmatrix}
a_{1 \alpha_1} & \cdots & a_{1 \alpha_m} \\
\vdots & \ddots & \vdots \\
a_{m \alpha_1} & \cdots & a_{m \alpha_m}
\end{vmatrix}
b_{\alpha_1 1} b_{\alpha_2 2} \cdots b_{\alpha_m m}
$$</p><p>$$
\tag{2}
= \sum_{\alpha_1, \alpha_2, \cdots, \alpha_m = 1}^n
A \begin{pmatrix} 1 & 2 & \cdots & m \\ \alpha_1 & \alpha_2 & \cdots & \alpha_m \end{pmatrix}
b_{\alpha_1 1} b_{\alpha_2 2} \cdots b_{\alpha_m m}
$$</p><p>If m > n, the matrices $ A $ and $ B $ do not have minors of order $ m $. In that case
the right-hand sides of <em>(1)</em> and <em>(1&rsquo;)</em> are to be replaced by zero.</p><p>Now let $ m \leq n $. Then in the sum on the right-hand side of <em>(2)</em>
all those summands will be zero in which at least two of the subscripts
$ \alpha_1, \alpha_2, \cdots, \alpha_m $ are equal.
(If two columns of $ A $ are equal, its determinant is zero.)
All the remaining summands of <em>(2)</em> can be split into groups of
$ m! $ terms each by combining into one group those summands that differ
from each other only in the order of the subscripts
$ \alpha_1, \alpha_2, \cdots, \alpha_m $.
Now within one such group the sum of the corresponding terms is</p><p>$$
\sum \varepsilon (\alpha_1, \alpha_2, \cdots, \alpha_m)
A \begin{pmatrix} 1 & 2 & \cdots & m \\ k_1 & k_2 & \cdots & k_m \end{pmatrix}
b_{\alpha_1 1} b_{\alpha_2 2} \cdots b_{\alpha_m m}
$$</p><p>$$
= A \begin{pmatrix} 1 & 2 & \cdots & m \\ k_1 & k_2 & \cdots & k_m \end{pmatrix}
\sum \varepsilon (\alpha_1, \alpha_2, \cdots, \alpha_m)
b_{\alpha_1 1} b_{\alpha_2 2} \cdots b_{\alpha_m m}
$$</p><p>By the Leibniz formula for determinants, the summation over all
permutations can be expressed compactly. Thus, the expression becomes:</p><p>$$
= A \begin{pmatrix} 1 & 2 & \cdots & m \\ k_1 & k_2 & \cdots & k_m \end{pmatrix}
B \begin{pmatrix} k_1 & k_2 & \cdots & k_m \\ 1 & 2 & \cdots & m \end{pmatrix}
$$</p><p>Here, $ k_1 &lt; k_2 &lt; \cdots &lt; k_m $ is the normal order of the subscripts
$ \alpha_1, \alpha_2, \cdots, \alpha_m $ and
$ \varepsilon (\alpha_1, \alpha_2, \cdots, \alpha_m) = (-1)^N $ where
$ N $ is the number of transpositions of the indices needed to put
the permutation $ \alpha_1, \alpha_2, \cdots, \alpha_m $ into normal order.</p><p>Hence from <em>(2)</em> we obtain <em>(1&rsquo;)</em>.</p><p><strong>Generalization</strong></p><p>Let $ A \in M_{m,k} (\bf F) $, $ B \in M_{k,n} (\bf F) $, and $ C = A B $.
Furthermore, let $ 1 \leq r \leq min \{m, k, n\} $, and let
$ \alpha \subseteq \{1, \dots, m\} $ and
$ \beta \subseteq \{1, \dots, n\} $ be index sets,
each of cardinality $ r $. An expression for the $ \alpha $, $ \beta $
minor of $ C $ is</p><p>$$
\operatorname{det} C [\alpha, \beta] = \sum_\gamma
\operatorname{det} A [\alpha, \gamma] \operatorname{det} B [\gamma, \beta]
$$</p><p>in which the sum is taken over all index sets
$ \gamma \subseteq \{1, \dots, k\} $ of cardinality $ r $.</p><p><strong>Derivation</strong></p><p>Given that</p><p>$$ C [\alpha, \beta] = A [\alpha, [k]] B [[k], \beta] $$</p><p>and applying the Cauchy-Binet formula, we arrive at the generalized form above.</p><h3 id=the-sylvester-franke-theorem>The Sylvester-Franke Theorem</h3><p>If $ A \in M_n $ and $ 1 \leq k \leq n $, then
$ \operatorname{det} C_k(A) = (\operatorname{det} A)^e $,
where $ e = {\begin{pmatrix} n - 1 \\ k - 1\end{pmatrix}} $.</p><p><strong>Proof</strong></p><p>The proof I provide is based on Leonard Tornheim,
<a href=https://www.jstor.org/stable/2306811 target=_blank rel=noopener>The Sylvester-Franke Theorem</a>, The American Mathematical Monthly.</p><p>There are three types $ E_1 $, $ E_2 $, $ E_3 $ of elementary
transformations: $ E_1 $ is the multiplication of a row or column
by a constant $ k $; $ E_2 $ is the interchange of two adjacent rows
or columns; and $ E_3 $ is the addition of k times a row or column to
another row or column. If $ B $ is transformed into $ C $ by $ E_i $, then</p><p>$$
\tag{1}
\operatorname{det} C = \mu_i \operatorname{det} B,
$$</p><p>where $ \mu_1 = k $, $ \mu_2 = -1 $, and $ \mu_3 = +1 $.</p><p>The proof of the theorem will be based on the following lemma.</p><p><strong>LEMMA.</strong> Let $ B $ be an arbitrary $ n \times n $ square matrix such that</p><p>$$
\tag{2}
\operatorname{det} C_k(B) = (\operatorname{det} B)^e
$$</p><p>If $ B $ is transformed into $ C $ by an elementary transformation $ E $,
then $ \operatorname{det} C_k(C) = (\operatorname{det} C)^e $.</p><p><strong>Proof of lemma</strong></p><p>We state first that</p><p>$$
\tag{3}
\operatorname{det} C_k(C) = \mu_i^e \operatorname{det} C_k(B).
$$</p><p>We shall prove this in detail only for $ i = 2 $;
the other two cases are easier. Suppose that the $ u $-th and ($ u + 1 $)-th
rows of $ B $ have been interchanged to give a matrix $ C $. The minors
of $ C $ which do not involve the $ u $-th or ($ u + 1 $)-th rows equal
the corresponding minors of $ B $. If both $ u $ and $ u + 1 $ appear in
$ \lambda (p) $, then $ C_{\lambda (p) \lambda (q)} = -B_{\lambda (p) \lambda (q)} $.
The number of rows of $ C_k (C) $ having such minors is $ {\begin{pmatrix} n - 2 \\ k - 2 \end{pmatrix}} $.
Next suppose $ u $ appears in $ \lambda (p) $ but $ u + 1 $ does not.
Let $ \lambda (p&rsquo;) $ be obtained from $ \lambda (p) $ by replacing $ u $ by $ u + 1 $.
Then $ C_{\lambda (p) \lambda (q)} = B_{\lambda (p&rsquo;) \lambda (q)} $ and
$ B_{\lambda (p&rsquo;) \lambda (q)} = C_{\lambda (p) \lambda (q)} $ since
the same elements are used and in the same arrangement, because
the $ u $-th and ($ u + 1 $)-th rows are adjacent. Thus from this cause
$ {\begin{pmatrix} n - 2 \\ k - 1 \end{pmatrix}} $ pairs of rows
have been interchanged in going from $ C_k(B) $ to $ C_k(C) $.
(There are $ 2 \times {\begin{pmatrix} n - 2 \\ k - 1 \end{pmatrix}} $
rows in $ C $ that need to be changed to match $ B $. However, these rows
must be interchanged within $ C $, forming
$ {\begin{pmatrix} n - 2 \\ k - 1 \end{pmatrix}} $ pairs.)
Thus the total number of changes in sign in going from
$ \operatorname{det} C_k(B) $ to $ \operatorname{det} C_k(C) $ is</p><p>$$ {\begin{pmatrix} n - 2 \\ k - 2 \end{pmatrix}} + {\begin{pmatrix} n - 2 \\ k - 1 \end{pmatrix}} = {\begin{pmatrix} n - 1 \\ k - 1 \end{pmatrix}} = e.$$</p><p>From <em>(3)</em>, <em>(1)</em>, and <em>(2)</em> it now follows that</p><p>$$
\operatorname{det} C_k(C) = \mu_i^e \operatorname{det} C_k(B) =
\mu_i^e (\operatorname{det} B)^e = (\mu_i \operatorname{det} B)^e =
(\operatorname{det} C)^e,
$$</p><p>and the proof of the lemma is complete.</p><p><strong>Proof of Sylvester-Franke Theorem</strong></p><p>The proof of the Sylvester-Franke Theorem can now be given as follows. It
is known that there exists an $ n \times n $ matrix</p><p>$$ D = \begin{pmatrix} I_r & 0 \\ 0 & 0 \end{pmatrix} $$</p><p>with $ 0 \leq r \leq n $.</p><p>$ D $ can be ransformed into the given $ n \times n $ matrix $ A $ by
a finite sequence of elementary transformations $ E^{(1)} $, $ E^{(2)} $,
$ \dots $, $ E^{(v)} $.</p><p>If $ r = n $, $ \operatorname{det} D = 1 $ and if $ r &lt; n $,
$ \operatorname{det} D = 0 $. Also if $ r = n $,
$ C_k (D) = I_{\begin{pmatrix} n \\ k \end{pmatrix}} $, and
$ \operatorname{det} C_k (D) = 1 $; otherwise
$ \operatorname{det} C_k (D) = 0 $. Hence the relation
$ \operatorname{det} C_k (D) = (\operatorname{det} D)^e $ holds
in both cases. Then by the lemma, $ \operatorname{det} C_k (E^{(1)} D) =
\operatorname{det} C_k (D_1) = (\operatorname{det} D_1)^e $,
$ \operatorname{det} C_k (E^{(2)} D_1) = \operatorname{det} C_k (D_2) =
(\operatorname{det} D_2)^e $, and so on. If follows by induction
that $ \operatorname{det} C_k (E^{(v)} D_{v - 1}) = \operatorname{det} C_k (A) = (\operatorname{det} A)^e $,
and the proof is complete.</p></div><div class="row ps-3 pe-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/qingbo12/qingbo12.github.io/edit/main/content/posts/Miscellanea/matrix-base/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/miscellanea/circuit/ title="Circuit Base" class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Circuit Base</div></a></div><div class="col-md-6 next-article"><a href=/posts/miscellanea/multimodal-fusion/ title="Multimodal-fusion research" class="btn filled-button"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Multimodal-fusion research</div></a></div></div><hr><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")return;var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="toha-example-site",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the
<a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></div></div><a id=scroll-to-top class=btn type=button data-bs-toggle=tooltip data-bs-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center ps-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#determinant-of-matrix-product>Determinant of Matrix Product</a><ul><li><a href=#theorem--detab--detadetb->Theorem: $ det(AB) = det(A)det(B) $</a></li></ul></li><li><a href=#real-symmetric-matrix>Real Symmetric Matrix</a><ul><li><a href=#theorem-all-real-eigenvalues>Theorem: all real eigenvalues</a></li><li><a href=#theorem-orthogonally-diagonalizable>Theorem: orthogonally diagonalizable</a></li><li><a href=#theorem-r-fold-root--a---lambda-e--has-a-rank-of--n---r->Theorem: r-fold root, $ A - \lambda E $ has a rank of $ n - r $</a></li></ul></li><li><a href=#vector-spaces>Vector spaces</a><ul><li><a href=#the-dimension-of--operatornamerange-a--is-denoted-by--operatornamerank-a->The dimension of $ \operatorname{range} A $ is denoted by $ \operatorname{rank} A $</a></li><li><a href=#rank-nullity-theorem>Rank-nullity theorem</a><ul><li><a href=#matrices>Matrices</a></li><li><a href=#linear-transformations>Linear transformations</a></li><li><a href=#application--dim-cab--dim-cb---dim--operatornamenullspace-a-cap-cb->Application: $ \dim C(AB) = \dim C(B) - \dim ( \operatorname{nullspace} (A) \cap C(B)) $</a></li></ul></li></ul></li><li><a href=#rank>Rank</a><ul><li><a href=#sylvester-inequality>Sylvester inequality</a></li><li><a href=#frobenius-inequality>Frobenius inequality</a></li><li><a href=#left-or-right-multiplication-by-a-nonsingular-matrix-leaves-rank-unchanged>Left or right multiplication by a nonsingular matrix leaves rank unchanged</a></li><li><a href=#if--a-in-m_mnbf-c--then--operatornamerank-a-a--operatornamerank-a->If $ A \in M_{m,n}(\bf C) $, then $ \operatorname{rank} A^* A = \operatorname{rank} A $</a></li><li><a href=#cr-factorization>CR Factorization</a></li><li><a href=#wedderburns-rank-one-reduction-formula>Wedderburn&rsquo;s rank-one reduction formula</a></li></ul></li><li><a href=#the-euclidean-inner-product>The Euclidean inner product</a><ul><li><a href=#coordinates-form-of-inner-product-and--costheta->Coordinates form of inner product and $ cos(\theta) $</a></li></ul></li><li><a href=#determinants-again>Determinants again</a><ul><li><a href=#the-cauchy-binet-formula>The Cauchy-Binet formula</a></li><li><a href=#the-sylvester-franke-theorem>The Sylvester-Franke Theorem</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://qingbo12.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://qingbo12.github.io/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://qingbo12.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://qingbo12.github.io/#featured-posts>Featured Posts</a></li><li class=nav-item><a class=smooth-scroll href=https://qingbo12.github.io/#recent-posts>Recent Posts</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:qingbo12@gmail.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>qingbo12@gmail.com</span></a></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>Liability Notice:</strong> This theme is under MIT license. So, you can use it for non-commercial, commercial, or private uses. You can modify or distribute the theme without requiring any permission from the theme author. However, the theme author does not provide any warranty or takes any liability for any issue with the theme.</p></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2024 Copyright.</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.91bab9af3ea6af3cefda36641d1fdc76d78775d2a99c5cbbfd892093035465cf.js integrity="sha256-kbq5rz6mrzzv2jZkHR/cdteHddKpnFy7/YkgkwNUZc8=" defer></script></body></html>